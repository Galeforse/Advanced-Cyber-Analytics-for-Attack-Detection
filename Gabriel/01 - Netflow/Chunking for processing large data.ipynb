{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89cc59e-27a2-43fa-a5e1-39adf93a291d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attempting to implement chunking\n",
    "\n",
    "Due to the size of certain data, in our case the netflow LANL data, even with each dataset only representing one day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70202a9d-7ab4-4d07-939b-094e1b5af221",
   "metadata": {},
   "source": [
    "First I will import the the packages required in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d3d8fc-a03f-4649-94b5-2161a9c5abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd225a-4c8b-4774-b831-4dee52a21f0b",
   "metadata": {},
   "source": [
    "First we read in the data defining the headers from the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de69ea72-18b9-45a7-862e-4eea5c98a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = (['Time', 'Duration', 'SrcDevice', 'DstDevice', 'Protocol',\n",
    "            'SrcPort', 'DstPort', 'SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f091fb-f961-43ca-9ba2-f74b3fd0c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"G:/Project Data/netflow_day-47/netflow_day-47.csv\",names=headers,chunksize=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd2bc3-d6ef-4260-9211-f349524cfd99",
   "metadata": {},
   "source": [
    "Here we actually read in the data, the main difference being that we specifyin the often unused variable of `pd.read_csv` that is `chunksize`. This chunks the data by the number of lines specified, effectively partitioning the data into several chunks of this size. For instance if we have 10000 lines of data and specify 100 as the chunksize we partition the data into 100 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbc66a3-62a6-4d3c-a874-58e2e61833cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 0:00:00.029444\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "start = dt.now()\n",
    "sh_file = \"G:/Users/Gabriel/Documents/Education/UoB/Project/ATI_Data-20210322T143516Z-001/ATI Data/Summaries/red_team/session_hosts.txt\"\n",
    "rt_sh = list(pd.read_csv(sh_file, header=None)[0])\n",
    "\n",
    "## Change path and file as relevant \n",
    "path = 'G:/Project_Data'\n",
    "df_netflow = pd.read_csv(path + \"/netflow_day-47.bz2\", names=headers,chunksize=1000000)\n",
    "\n",
    "print(\"completed in \"+str(dt.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015be8d-6019-4591-acfe-faa985466bb4",
   "metadata": {},
   "source": [
    "A major thing to notice is that this read in almost instantly, this is as the df_netflow we have read is not a pandas dataframe, instead acting as a pointer towards the data before we can evalute it later. We will look at the type of the variable and see that it is a parsed reader. Again if we specified the chunksize as 1 this would effectively stream the data through when we evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05013e2-b7eb-480f-9472-caba21803e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parsers.TextFileReader"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_netflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f64178-ddef-4906-a59f-1b32bc4238ec",
   "metadata": {},
   "source": [
    "We then evaluate the chunk by evaluating through as mentioned, limiting the data to only those found in the red team data that we read in as sh_file earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8c9e9d5-7527-4bc7-83f7-450710c42a07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 11)\n",
      "(1000000, 7)\n",
      "(1000000, 11)\n",
      "(1000000, 7)\n",
      "(1000000, 11)\n",
      "(1000000, 7)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-1f557b218b65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_netflow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mchunk2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SrcPackets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DstPackets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SrcBytes'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DstBytes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1082\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2034\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2035\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2036\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2037\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2038\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\bz2.py\u001b[0m in \u001b[0;36mread1\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Programs\\Anaconda\\envs\\Project\\lib\\_compression.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for chunk in df_netflow:\n",
    "#    print(chunk.shape)\n",
    "#    chunk2 = chunk.drop(['SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes'],1)\n",
    "#    print(chunk2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "770c69b7-3352-4c8a-804b-7ec84e904e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = []\n",
    "\n",
    "for chunk in df_netflow:\n",
    "    chunk_filter = chunk[chunk['SrcDevice'].isin(rt_sh)]\n",
    "    \n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "df_concat = pd.concat(chunk_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04796c73-8eb0-460d-9c3a-7aec1c08f844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>SrcDevice</th>\n",
       "      <th>DstDevice</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>SrcPort</th>\n",
       "      <th>DstPort</th>\n",
       "      <th>SrcPackets</th>\n",
       "      <th>DstPackets</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>DstBytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3974400</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp953804</td>\n",
       "      <td>Comp037766</td>\n",
       "      <td>6</td>\n",
       "      <td>Port91398</td>\n",
       "      <td>Port55663</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3974400</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp953804</td>\n",
       "      <td>Comp037766</td>\n",
       "      <td>6</td>\n",
       "      <td>Port74411</td>\n",
       "      <td>Port70068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>3974400</td>\n",
       "      <td>1</td>\n",
       "      <td>Comp261075</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port32055</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>3974400</td>\n",
       "      <td>1</td>\n",
       "      <td>Comp953804</td>\n",
       "      <td>Comp217420</td>\n",
       "      <td>6</td>\n",
       "      <td>Port93140</td>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>3974400</td>\n",
       "      <td>1</td>\n",
       "      <td>Comp953804</td>\n",
       "      <td>Comp217420</td>\n",
       "      <td>6</td>\n",
       "      <td>Port70947</td>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>3050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time  Duration   SrcDevice   DstDevice  Protocol    SrcPort  \\\n",
       "154  3974400         0  Comp953804  Comp037766         6  Port91398   \n",
       "155  3974400         0  Comp953804  Comp037766         6  Port74411   \n",
       "777  3974400         1  Comp261075  Comp275646        17  Port32055   \n",
       "951  3974400         1  Comp953804  Comp217420         6  Port93140   \n",
       "952  3974400         1  Comp953804  Comp217420         6  Port70947   \n",
       "\n",
       "       DstPort  SrcPackets  DstPackets  SrcBytes  DstBytes  \n",
       "154  Port55663           0           1         0        46  \n",
       "155  Port70068           0           1         0        46  \n",
       "777         53           2           0       134         0  \n",
       "951        445           0          14         0      2618  \n",
       "952        445           0          16         0      3050  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0abedda5-c07e-40d6-ae05-5209ca3ac513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(743806, 11)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b976c-ff83-40ec-a7fe-8b2046e6b957",
   "metadata": {},
   "source": [
    "Iterates through to find the total length of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da615157-dfd0-4b7f-9929-90e510ff3ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Programs\\Anaconda\\envs\\det\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181936502\n"
     ]
    }
   ],
   "source": [
    "length_total = 0\n",
    "for chunk in df_netflow:\n",
    "    length_total = length_total + (len(chunk))\n",
    "\n",
    "print(length_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05fe0816-7874-429e-ba63-6844c9f3db06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40882725116920193"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(743806/length_total)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f642cb63-9ed4-49d0-8dce-d2467c3a09d6",
   "metadata": {},
   "source": [
    "We see that cutting down to only the red team computers accounts for only .4% of the original, data significantly reducing the memory usage. As we see a bit further down the data we end up with is only about 370mb in comparison to the over 11GB of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f62dc3-ffdf-4643-8b4b-e87ee9e7e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eb93ea4-e8cd-4314-8f4d-85b1f41c97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_list = []\n",
    "\n",
    "def chunk_preproc(chunk):\n",
    "    chunk_filter = chunk[chunk['SrcDevice'].isin(rt_sh)]\n",
    "    chunk_filter = chunk_filter.drop(columns=cols)\n",
    "\n",
    "for chunk in df_netflow:\n",
    "    chunk_filter = chunk_preproc(chunk)    \n",
    "    chunk_list.append(chunk_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c6486-2594-4466-8aad-2ce71b1c6643",
   "metadata": {},
   "source": [
    "The below code contains most of the above in one block that parses the data, dropping the columns that we drop above in the read_csv step instead of when we evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45c3e258-64eb-486b-9a1a-f92407641a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Time  Duration   SrcDevice   DstDevice  Protocol    SrcPort    DstPort\n",
      "154  3974400         0  Comp953804  Comp037766         6  Port91398  Port55663\n",
      "155  3974400         0  Comp953804  Comp037766         6  Port74411  Port70068\n",
      "777  3974400         1  Comp261075  Comp275646        17  Port32055         53\n",
      "951  3974400         1  Comp953804  Comp217420         6  Port93140        445\n",
      "952  3974400         1  Comp953804  Comp217420         6  Port70947        445\n",
      "(6932200, 7)\n",
      "Time taken: 0:09:19.679054\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "sh_file = \"G:/Users/Gabriel/Documents/Education/UoB/Project/ATI_Data-20210322T143516Z-001/ATI Data/Summaries/red_team/session_hosts.txt\"\n",
    "rt_sh = list(pd.read_csv(sh_file, header=None)[0])\n",
    "\n",
    "## Change path and file as relevant \n",
    "path = 'G:/Project_Data'\n",
    "df_netflow = pd.read_csv(path + \"/netflow_day-47.bz2\",usecols=[0,1,2,3,4,5,6],names=headers,chunksize=1000000)\n",
    "\n",
    "chunk_list = []\n",
    "\n",
    "for chunk in df_netflow:\n",
    "    chunk_filter = chunk[chunk['SrcDevice'].isin(rt_sh)]\n",
    "    #chunk_filter = chunk_filter.drop(['SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes'],1)\n",
    "    \n",
    "    chunk_list.append(chunk_filter)\n",
    "    \n",
    "df_concat = pd.concat(chunk_list)\n",
    "\n",
    "print(df_concat.head())\n",
    "print(df_concat.shape)\n",
    "print(\"Processing time taken: \" +str(datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01eb6a-cf1e-4b80-a633-a3407ec5e449",
   "metadata": {},
   "source": [
    "We now export the cutdown data to a new file, of all the data that contains a connection of any of the compromised computers with Command & Control as specified in the documentation of the ATI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af84ea81-771d-48cc-ac3d-db57118c9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "df_concat.to_csv(\"G:/Project_Data/47_C&C.csv.gz\", index=False, compression=\"gzip\")\n",
    "print(\"Export completed in: \" +str(datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff52cd-a5f9-41b6-bb84-52d60ba6f565",
   "metadata": {},
   "source": [
    "The below data then does the same thing days 48 and 49, and if we were to download more of the netflow data could change the computers specified to process and then export suitably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09626c59-ff41-490c-b053-3b201a763bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6932200 entries, 0 to 6932199\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   Time       int64 \n",
      " 1   Duration   int64 \n",
      " 2   SrcDevice  object\n",
      " 3   DstDevice  object\n",
      " 4   Protocol   int64 \n",
      " 5   SrcPort    object\n",
      " 6   DstPort    object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 370.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_concat_test = pd.read_csv(\"G:/Project_Data/47_C&C.csv.gz\")\n",
    "df_concat_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee15d073-7a17-4ba5-a768-c5e1beef1a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Time  Duration   SrcDevice   DstDevice  Protocol    SrcPort    DstPort\n",
      "401  4060800         1  Comp289117  Comp576031        17  Port29551        514\n",
      "496  4060800         9  Comp249659  Comp963933         6  Port74692  Port28904\n",
      "838  4060801         0  Comp953804  Comp876725         6  Port45643        139\n",
      "839  4060801         0  Comp953804  Comp876725         6  Port74532        135\n",
      "840  4060801         0  Comp953804  Comp876725         6  Port57466         22\n",
      "(7324228, 7)\n",
      "Processing time for computer 48 taken: 0:08:32.659294\n",
      "Export completed in: 0:00:59.853735\n",
      "         Time  Duration   SrcDevice   DstDevice  Protocol    SrcPort DstPort\n",
      "1732  4147201         3  Comp992775  Comp611862         6  Port65569      80\n",
      "2069  4147202         2  Comp289117  Comp576031        17  Port49469     514\n",
      "2446  4147203         1  Comp992775  Comp275646        17  Port83264      53\n",
      "2447  4147203         1  Comp992775  Comp275646        17  Port90474      53\n",
      "2448  4147203         1  Comp992775  Comp275646        17  Port66508      53\n",
      "(7352812, 7)\n",
      "Processing time for computer 49 taken: 0:09:18.895023\n",
      "Export completed in: 0:00:59.606057\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "sh_file = \"G:/Users/Gabriel/Documents/Education/UoB/Project/ATI_Data-20210322T143516Z-001/ATI Data/Summaries/red_team/session_hosts.txt\"\n",
    "rt_sh = list(pd.read_csv(sh_file, header=None)[0])\n",
    "\n",
    "comps=[48,49]\n",
    "\n",
    "for i in comps:\n",
    "    ## Change path and file as relevant\n",
    "    path = 'G:/Project_Data'\n",
    "    df_netflow = pd.read_csv(path + \"/netflow_day \"+str(i)+\".bz2\",usecols=[0,1,2,3,4,5,6],names=headers,chunksize=1000000)\n",
    "\n",
    "    chunk_list = []\n",
    "\n",
    "    for chunk in df_netflow:\n",
    "        chunk_filter = chunk[chunk['SrcDevice'].isin(rt_sh)]\n",
    "        #chunk_filter = chunk_filter.drop(['SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes'],1)\n",
    "    \n",
    "        chunk_list.append(chunk_filter)\n",
    "    \n",
    "    df_concat = pd.concat(chunk_list)\n",
    "\n",
    "    print(df_concat.head())\n",
    "    print(df_concat.shape)\n",
    "    print(\"Processing time for computer \"+str(i)+\" taken: \" +str(datetime.now()-start))\n",
    "\n",
    "    start = datetime.now()\n",
    "    df_concat.to_csv(\"G:/Project_Data/\"+str(i)+\"_C&C.csv.gz\", index=False, compression=\"gzip\")\n",
    "    print(\"Export completed in: \" +str(datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e34b2-6ec8-45c7-bd8b-08268d0fd32e",
   "metadata": {},
   "source": [
    "This was still a better way than reading in the total data as when I tried to read in the whole dataset as it floored out my RAM despite having 16GB. Either way we decided to abandon the netflow idea, but this could be useful for some slightly larger data at some later point in the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
