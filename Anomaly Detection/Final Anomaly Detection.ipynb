{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e84a26-1a84-4c12-bcb4-081f774f1e6e",
   "metadata": {},
   "source": [
    "# Final Anomaly Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891d2d3-acab-4904-989f-d9d9adf1e12b",
   "metadata": {},
   "source": [
    "### Original Data and Other Necessary Data Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea18a9f-9dfa-4bcb-b732-6a0a51eda0e3",
   "metadata": {},
   "source": [
    "Here are all the libraries and modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94ede6d-bd5f-4a1d-9f9a-2c8cc70c70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCP_APR import CP_APR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import bz2\n",
    "import random\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38072c7-9f37-4785-971a-1490156ea1d7",
   "metadata": {},
   "source": [
    "We set seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdd04db-1c55-4820-a10e-330ec05b1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa73404-41eb-42c2-ad69-8119d29ebeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\corri\\OneDrive\\Documents\\GitHub\\Advanced-Cyber-Analytics-for-Attack-Detection\\Anomaly Detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f6a49-a70f-4483-a953-7b5363df41c5",
   "metadata": {},
   "source": [
    "Here we import the original data - these are the data summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844457f4-7fa5-49d3-b2f8-41b05958624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read entire data set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corri\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Attempting to read entire data set.')\n",
    "    authentication_data = pd.read_csv('../Data/Authentication data.gz', compression='gzip', index_col = 0)\n",
    "    process_data = pd.read_csv('../Data/Process data.gz', compression='gzip', index_col = 0)\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Unable to read entire data set, reading from original files.')\n",
    "    rootdir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls'\n",
    "    unzippeddir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls/Unzipped'\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == '.gz':\n",
    "                filedir = rootdir + '/' + file\n",
    "                with gzip.open(filedir) as f:\n",
    "                    df = pd.read_csv(filedir, header=None)\n",
    "                    frames.append(df)\n",
    "                if 'authentications' in str(file):\n",
    "                    count = count + len(df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    authentication_data = df[:count]\n",
    "    authentication_data.columns = ['UserName', 'SrcDevice','DstDevice', 'Authent Type', 'Failure', 'DailyCount']\n",
    "\n",
    "    process_data = df[count:]\n",
    "    process_data = process_data[[0,1,2,3,4]]\n",
    "    process_data.columns = ['UserName', 'Device', 'ProcessName', 'ParentProcessName', 'DailyCount']\n",
    "\n",
    "    authentication_data.to_csv('../Data/Authentication data.gz', header=True, compression='gzip')\n",
    "    process_data.to_csv('../Data/Process data.gz', header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282df99d-80ac-40bb-834f-fe16e601327a",
   "metadata": {},
   "source": [
    "The list of known red team users, which is assumed to be comprehensive, is required later on for analysis of techniques. We'll also obtain a list of non red-team users and a sample of them to employ when testing the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085b03a3-3c26-4008-804d-61a0fa8da190",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_usernames = list(pd.read_csv('../Data/AuthUserNames.txt', header=None)[0])\n",
    "non_rt_users = [un for un in authentication_data['UserName'].unique() if un not in rt_usernames]\n",
    "non_rt_sample = random.sample(non_rt_users,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca82707-08f1-49e9-81fb-459d0b193bf4",
   "metadata": {},
   "source": [
    "For data creation later we require a list of authentication types - this can be created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c7fec8-a6d4-424a-9c29-852f750890ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = list(authentication_data['Authent Type'].unique())\n",
    "AT_dict = { i : a_t[i] for i in range(0, len(a_t) ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6798-760c-487d-80f6-47db52a5be7f",
   "metadata": {},
   "source": [
    "Finally, we require a list of the seperation points in our data frame. Each day can be seperated out by indexing and we find the indices here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e314dae9-4ed4-4e7b-9713-62e4eda1e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_index_list = authentication_data.index.tolist()\n",
    "auth_start_days = [i for i, e in enumerate(auth_index_list) if e == 0]\n",
    "auth_start_days.append(len(authentication_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916819f-7163-41e2-821a-144a7ea699ad",
   "metadata": {},
   "source": [
    "### Data Creation for Anomaly Detction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a752c-dc1a-46d8-91b2-6008e1d5fe20",
   "metadata": {},
   "source": [
    "#### Main Data Creation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7215a8f-d60e-4cf8-9518-d134f5aaf648",
   "metadata": {},
   "source": [
    "This first function is used to split a data frame into equal chunks. Since we need to split each day into 8/24 hours we use this function to split into equal time periods - this may not be perfectly representitive of the actual hour split but should be a good estimate since we don't have the original time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d7814b-edfc-4898-a421-9b49c4d0fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df,n):\n",
    "    chunks = list()\n",
    "    chunk_size = int(np.round(df.shape[0]/n))\n",
    "    num_chunks = n\n",
    "    for i in range(num_chunks):\n",
    "        if i != num_chunks-1:\n",
    "            chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "        else:\n",
    "            chunks.append(df[i*chunk_size:])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5271dc77-8972-4ac3-bc9f-e3767ebc9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_type_un_df(user,n):\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    n = n\n",
    "    auth_type_dict = {}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                data = chunks[j]\n",
    "                auth_type_data = data[data['UserName'] == user].groupby('Authent Type')['DailyCount'].sum()\n",
    "                auth_type_dict[i*n + j] = auth_type_df.index.to_series().map(auth_type_data.to_dict())\n",
    "    \n",
    "    auth_type_df = pd.DataFrame(data=auth_type_dict,index = list(authentication_data['Authent Type'].unique()))\n",
    "    auth_type_df = auth_type_df.transpose()\n",
    "    auth_type_df = auth_type_df.fillna(0)\n",
    "    \n",
    "    return auth_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280345f-3f38-48c5-8e74-38f093b90a7f",
   "metadata": {},
   "source": [
    "#### Authentication Type Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681862c-69a2-4eeb-8249-8b8781ead5fe",
   "metadata": {},
   "source": [
    "This function creates the authentication type data frames . It takes as input a username and a split by number (8/24) and returns a data frame of the user's authentiation events split by type over 90 days, split by 8/24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54b0a22d-b17b-4732-9512-c89c1889f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_type_un_df(user,n):\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    n = n\n",
    "    auth_type_dict = {}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                data = chunks[j]\n",
    "                auth_type_data = data[data['UserName'] == user].groupby('Authent Type')['DailyCount'].sum()\n",
    "                auth_type_dict[i*n + j] = auth_type_df.index.to_series().map(auth_type_data.to_dict())\n",
    "    \n",
    "    auth_type_df = pd.DataFrame(data=auth_type_dict,index = list(authentication_data['Authent Type'].unique()))\n",
    "    auth_type_df = auth_type_df.transpose()\n",
    "    auth_type_df = auth_type_df.fillna(0)\n",
    "    \n",
    "    return auth_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400f548-da88-4fec-82c2-7a7944dd2953",
   "metadata": {},
   "source": [
    "#### Various Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d3e56-4743-4e62-8f82-65930ff810d3",
   "metadata": {},
   "source": [
    "This function creates a data frame of other features. It includes counts from various possible features that we could've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02f0fb4-53ec-4d0f-be1e-a8c23345006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06e104-8801-42f7-98fa-af0ede49d31e",
   "metadata": {},
   "source": [
    "#### Adjacency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c19d56-5cbf-4ccd-ba80-cff5c8d9da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    data_frame_list_uase = pickle.load(open('C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/Data Frame List UASE.p','rb'))\n",
    "    index_sparse = pickle.load(open('Index UASE.p','rb'))\n",
    "    columns_sparse = pickle.load(open('Columns UASE.p','rb'))\n",
    "    \n",
    "except:\n",
    "    clear_output()\n",
    "    print('Creating Data Frames.')\n",
    "    data_frame_list_uase = []\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    for i in tqdm(range(len(auth_start_days)-1)):\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(index = list(authentication_data['DstDevice'].unique()))\n",
    "\n",
    "        chunk = authentication_data[auth_start_days[i]:auth_start_days[i+1]]\n",
    "        data_un ={}\n",
    "        for user in authentication_data['UserName'].unique():\n",
    "            dstdevice_data = chunk[chunk['UserName'] == user].groupby('DstDevice').size()\n",
    "            data_un[user] = data_frame_ind.index.to_series().map(dstdevice_data.to_dict())\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(data=data_un,index = list(authentication_data['DstDevice'].unique()))\n",
    "        data_frame_ind = data_frame_ind.notnull().astype('int')\n",
    "        data_frame_ind = data_frame_ind.fillna(0)\n",
    "        A = np.array(data_frame_ind)\n",
    "        sA = sparse.csr_matrix(A)\n",
    "        data_frame_list_uase.append(sA)    \n",
    "\n",
    "    index_sparse = data_frame_ind.index\n",
    "    columns_sparse = data_frame_ind.columns\n",
    "    pickle.dump(data_frame_list_uase, open('Data Frame List UASE.p', 'wb'))\n",
    "    pickle.dump(index_sparse, open('Index UASE.p', 'wb'))\n",
    "    pickle.dump(columns_sparse, open('Columns UASE.p', 'wb'))\n",
    "    print(datetime.datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6cc7e-3c68-4c4c-83fd-3c8fcf594c33",
   "metadata": {},
   "source": [
    "#### Supervised Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84bc1b-f5be-4af7-ba6a-a457c0d52bbf",
   "metadata": {},
   "source": [
    "We turn our anomaly detection exercise into a supervised problem here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3dd7c83-9e8f-4bd5-a25f-e1ba5633a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_supervised(user,n,anom):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "            \n",
    "            hour = j\n",
    "            data = chunks[j]\n",
    "            \n",
    "            if 57 <= i+1 <= 82 and anom==True:\n",
    "                anombool = 1\n",
    "            else:\n",
    "                anombool = 0\n",
    "\n",
    "            authents = len(data[data['UserName'] == user])\n",
    "            failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "            srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "            dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "            uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "            feat_dict[i*n + j] = [user,srcunique,dstunique,authents,failures,anombool]\n",
    "            #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['UserName','SrcUnique','DstUnique','Authentications','Failures','Anomaly'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eeb4a-c65d-4423-a855-0305ada36ef9",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1fb5b-ccee-4d79-aa77-3b004328dfc3",
   "metadata": {},
   "source": [
    "#### Isolation Forest and Local Outlier Factor Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2185f73b-3e0e-4c39-a3ba-f0259fdf91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_iso_lof(data,plot=False,c='auto'):\n",
    "    \n",
    "    # scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data))\n",
    "    \n",
    "    # isolation forest predictions\n",
    "    if_model = IsolationForest(contamination=c)\n",
    "    if_predictions = if_model.fit_predict(data)\n",
    "    \n",
    "    # local outlier factor predictions\n",
    "    lof = LocalOutlierFactor(n_neighbors=2)\n",
    "    lof_predictions = lof.fit_predict(data)\n",
    "    \n",
    "    if plot == True:\n",
    "        \n",
    "        # PCA reduction for plotting\n",
    "        pca = PCA(n_components=2)\n",
    "        auth_types_pca = pd.DataFrame(pca.fit_transform(data))\n",
    "        \n",
    "        # finding anomaly locations\n",
    "        a_if = auth_types_pca.loc[if_predictions == -1]\n",
    "        a_lof = auth_types_pca.loc[lof_predictions == -1]\n",
    "        \n",
    "        anomalies = auth_types_pca.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,6))\n",
    "        ax.plot(auth_types_pca[0], auth_types_pca[1], color='black', label='Normal')\n",
    "        ax.scatter(anomalies[0], anomalies[1], color='red', label='Anomaly')\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Number of Events\")\n",
    "        ax.text(0,auth_types_pca[1].max()-0.1,('Number of combined anomalies found: {}. \\n Number of LOF anomalies found: {}. \\n Number of IF anomalies found: {}.'.format(len(anomalies), len(a_lof), len(a_if))))\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        a_if = data.loc[if_predictions == -1]\n",
    "        a_lof = data.loc[lof_predictions == -1]\n",
    "        \n",
    "        anomalies = data.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "    \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "773644b5-a13f-4d05-a6e6-29fff4772ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rt_if_lof_anomalies = []\n",
    "\n",
    "# for un in rt_usernames:\n",
    "#     data = auth_type_un_df(un,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6893321-45f3-4b61-b9ae-085fa0d1c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = auth_type_un_df(rt_usernames[0],14)\n",
    "#scaled_iso_lof(data,plot=False,c='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2db86-65b2-4f57-b0f3-25ccd9e2289f",
   "metadata": {},
   "source": [
    "#### Auto Encoder AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f70408-13bd-4c2c-ae5c-c79c20da21a8",
   "metadata": {},
   "source": [
    "The first function allows us to work back from the anomalies produced by the auto-encoder to obtain the original anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90d945bf-a657-45b7-832b-7ab9366e60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f4068-5392-4c02-b089-1b08564744bc",
   "metadata": {},
   "source": [
    "Here we define our best auto encoder anomaly detection function from the 6 we built - built from https://csce.ucmss.com/cr/books/2019/LFS/CSREA2019/ICA2282.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3976c1b1-4a5b-4923-ba98-78905dc15817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > np.array(history.history[\"val_loss\"]).min())\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15749135-8d4c-4f00-97ff-20d744fb0f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_5 = pickle.load(open('Anomalies AE VL.p','rb'))\n",
    "    val_loss_5 = pickle.load(open('Validation Loss VL.p','rb'))\n",
    "    anomaly_bool_5 = []\n",
    "    anomalies_ae_5 = anomalies_ae_5.drop_duplicates()\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_5.append((len(anomalies_ae_5[anomalies_ae_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_5 = []\n",
    "    anomaly_bool_5 = []\n",
    "    val_loss_5 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_5.append(f)\n",
    "            anomaly_bool_5.append((len(f),rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_5.append((0,rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_5 = pd.concat(frames_5)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_5, open('Anomalies AE VL.p','wb'))\n",
    "    pickle.dump(val_loss_5, open('Validation Loss VL.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53501c-2c0b-4040-9f35-46e8b809d033",
   "metadata": {},
   "source": [
    "This analyses the results from the auto-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13435d5e-781f-45a6-9e3d-0ae024544a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ae_eval(anomaly_bools,anomalies,val_losses,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    final_losses = []\n",
    "    for i in range(len(val_losses)):\n",
    "        final_losses.append(val_losses[i][0][len(val_losses[i][0])-1]) \n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(data = final_losses)\n",
    "    plt.show()\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a352f96e-b038-418f-b733-fa2c6b5ab133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 of the red team usernames were identified to have anomalies out of 99 red team usernames. The folloiwng usernames were found to have no anomalies\n",
      "--------------------------------------------------------------------------------------------\n",
      "User409683, User133028, User089536, User859748, User630088, User457932, User758916, User829941, User900703, User497510, User222822\n",
      "14.75% of the red team data was identified as anomalous.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authent Type\n",
      "Batch                    189\n",
      "CachedInteractive         77\n",
      "InteractiveLogon         588\n",
      "NetworkLogon            7995\n",
      "RemoteInteractive        212\n",
      "ScreensaverDismissed     200\n",
      "ScreensaverInvoked       204\n",
      "Service                  463\n",
      "TGS                     8540\n",
      "TGT                     2548\n",
      "WorkstationLock          581\n",
      "WorkstationUnlock        797\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "simple_ae_eval(anomaly_bool_5,anomalies_ae_5,val_loss_5,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f52a250f-334a-4608-ab7e-69ffdcc5ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c2854d23e041afbb98d5b411fdcec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00029: early stopping\n",
      "No anomalies found.\n",
      "1:51:21.363239\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    non_rt_sample = random.sample(non_rt_users,bound)\n",
    "    anomalies_ae_n_5 = pickle.load(open('Non RT Anomalies.p','rb'))\n",
    "    val_loss_n_5 = pickle.load(open('Non RT VL.p','rb'))\n",
    "    anomaly_bool_N_5 = []\n",
    "    anomalies_ae_n_5 = anomalies_ae_n_5.drop_duplicates()\n",
    "\n",
    "    for un in non_rt_sample:\n",
    "        try:\n",
    "            anomaly_bool_N_5.append((len(anomalies_ae_n_5[anomalies_ae_n_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_n_5 = []\n",
    "    anomaly_bool_N_5 = []\n",
    "    val_loss_n_5 = []\n",
    "    bound = 400\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i,un in enumerate(non_rt_sample):\n",
    "        f,b,val = ae_anomaly_finder(un,24)\n",
    "        if b == 1:\n",
    "            frames_n_5.append(f)\n",
    "            anomaly_bool_N_5.append((len(f),un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "        else:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "            pass\n",
    "    anomalies_ae_n_5 = pd.concat(frames_n_5)\n",
    "    pickle.dump(anomalies_ae_n_5,open('Non RT Anomalies.p','wb'))\n",
    "    pickle.dump(val_loss_n_5,open('Non RT VL.p','wb'))\n",
    "    pickle.dump(non_rt_sample,open('Random Sample of Non Red Team Usernames.p','wb'))\n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce73162b-932e-447b-ac51-d6e7b383ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336 of the normal usernames were identified to have anomalies out of 400 normal usernames. The folloiwng usernames were found to have no anomalies\n",
      "--------------------------------------------------------------------------------------------\n",
      "User558903, User439048, Comp623389$, User939034, User565970, User262941, User703488, User159815, User845839, Comp014951$, User896647, Comp499682$, Comp775436$, User125119, User602494, User901705, User799647, User894599, Comp216772$, User936556, User882175, User946590, User428181, User799659, User486339, Comp317443$, Comp261013$, User573468, User387455, User764890, Comp008365$, Comp629361$, User674811, User467601, User616026, Comp398685$, User325958, User670114, Comp628167$, Comp393779$, User546230, User501826, Comp416325$, User854203, User101046, User338382, Comp989146$, Comp380170$, Comp027416$, User276491, Comp758007$, Comp216216$, Comp492299$, User496596, User443647, Comp760116$, Comp277773$, User362650, User860058, Comp887177$, Comp497770$, User656736, User889694, Comp467014$\n",
      "24.87% of the normal data was identified as anomalous.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQhElEQVR4nO3dUWhc153H8d9PM3hJQkrpRNs2drwOxBAMSUoZ3EKWhmApjAux96UloVTapq0xNHH7UNg89aUv+7Av6+DWVktAgpaQQAMq68qW8rIsaViPSzZO2qYI1yWyshtlWlposk1H+u/DjM1YO/acqS3fuaffDwjr3HvO6C+Kfjk9c+ccR4QAAPkaK7oAAMDWIugBIHMEPQBkjqAHgMwR9ACQuWrRBfRzxx13xK5du4ouAwBK4+zZs+9GxHi/eyMZ9Lt27VKz2Sy6DAAoDdu/udo9lm4AIHMEPQBkjqAHgMwR9ACQuaSgt92w/abtZdtP97n/Bduvdb9etv1Az70Lts/ZftU277CitFqtlo4cOaJWq1V0KcBQBga97YqkY5L2S9oj6XHbezZ1+7WkhyLifknfljSz6f7DEfGJiKjfgJqBQszOzurcuXOam5sruhRgKCkz+r2SliPifER8IOk5SQd7O0TEyxHxu27zFUk7bmyZQLFarZYWFhYUEVpYWGBWj1JJCfrtkt7qaa90r13NlyX9pKcdkk7bPmv70NUG2T5ku2m7uba2llAWcPPMzs5qY2NDkrS+vs6sHqWSEvTuc63vJva2H1Yn6P+p5/KDEfFJdZZ+vmb7M/3GRsRMRNQjoj4+3vfDXUBhlpaW1G63JUntdluLi4sFVwSkSwn6FUl39bR3SFrd3Mn2/ZK+L+lgRFz+/7URsdr99x1JL6qzFASUysTEhKrVzgfJq9WqJicnC64ISJcS9Gck7bZ9t+1tkh6TNN/bwfZOST+S9MWI+FXP9dts337pe0mPSHr9RhUP3CzT09MaG+v8uVQqFU1NTRVcEZBuYNBHRFvSk5JOSfqFpOcj4g3bh20f7nb7lqSapO9seozyo5L+w/Z/SfpPSf8WEQs3/LcAtlitVlOj0ZBtNRoN1Wq1oksCkiVtahYRJyWd3HTteM/3X5H0lT7jzkt6YPN1oIymp6d14cIFZvMonZHcvRIYRbVaTUePHi26DGBobIEAAJkj6AEgcwQ9AGSOoAeAzBH0AJA5gh4AMkfQA0DmCHoAyBxBDwCZI+gBIHMEPQBkjqAHgMwR9ACQOYIeADJH0ANA5gh6AMgcQQ8AmSPoASBzBD0AZI6gB4DMEfQAkDmCHgAyR9ADQOYIegDIHEEPAJkj6IFErVZLR44cUavVKroUYCgEPZBodnZW586d09zcXNGlAEMh6IEErVZLCwsLiggtLCwwq0epEPRAgtnZWW1sbEiS1tfXmdWjVAh6IMHS0pLa7bYkqd1ua3FxseCKgHRJQW+7YftN28u2n+5z/wu2X+t+vWz7gdSxQBlMTEyoWq1KkqrVqiYnJwuuCEg3MOhtVyQdk7Rf0h5Jj9ves6nbryU9FBH3S/q2pJkhxgIjb3p6WmNjnT+XSqWiqampgisC0qXM6PdKWo6I8xHxgaTnJB3s7RARL0fE77rNVyTtSB0LlEGtVlOj0ZBtNRoN1Wq1oksCkqUE/XZJb/W0V7rXrubLkn4y7Fjbh2w3bTfX1tYSygJurunpad13333M5lE61YQ+7nMt+na0H1Yn6P9+2LERMaPukk+9Xu/bByhSrVbT0aNHiy4DGFpK0K9IuqunvUPS6uZOtu+X9H1J+yOiNcxYAMDWSVm6OSNpt+27bW+T9Jik+d4OtndK+pGkL0bEr4YZCwDYWgNn9BHRtv2kpFOSKpKejYg3bB/u3j8u6VuSapK+Y1uS2hFRv9rYLfpdAAB9OGL0lsPr9Xo0m82iywCA0rB9NiLq/e7xyVgAyBxBDwCZI+gBIHMEPQBkjqAHgMwR9EAijhJEWRH0QCKOEkRZEfRAAo4SRJkR9EACjhJEmRH0QAKOEkSZEfRAAo4SRJkR9EACjhJEmRH0QAKOEkSZpRw8AkCdWf2FCxeYzaN0CHogEUcJoqxYugGAzBH0AJA5gh4AMkfQA0DmCHogEbtXoqwIeiARu1eirAh6IAG7V6LMCHogAbtXoswIeiABu1eizAh6IAG7V6LMCHogAbtXoswIeiABu1eizNjUDEjE7pUoK4IeSMTulSgrlm4AIHNJQW+7YftN28u2n+5z/17bP7X9J9vf3HTvgu1ztl+13bxRhQMA0gxcurFdkXRM0qSkFUlnbM9HxM97uv1W0hFJ/3CVl3k4It69zloBAH+BlBn9XknLEXE+Ij6Q9Jykg70dIuKdiDgj6c9bUCMA4DqkBP12SW/1tFe611KFpNO2z9o+NExxAIDrl/LUjftciyF+xoMRsWr7byUt2v5lRPz7//shnf8IHJKknTt3DvHyAIBrSZnRr0i6q6e9Q9Jq6g+IiNXuv+9IelGdpaB+/WYioh4R9fHx8dSXBwAMkBL0ZyTttn237W2SHpM0n/Litm+zfful7yU9Iun1v7RYAMDwBi7dRETb9pOSTkmqSHo2It6wfbh7/7jtj0lqSvqQpA3b35C0R9Idkl60feln/TAiFrbkNwEA9JX0ydiIOCnp5KZrx3u+/291lnQ2+4OkB66nQADA9eGTsQCQOYIeADJH0ANA5gh6AMgcQQ8AmSPoASBzBD0AZI6gB4DMEfQAkDmCHgAyR9ADQOYIegDIHEEPAJkj6AEgcwQ9AGSOoAeAzBH0AJA5gh4AMkfQA0DmCHoAyBxBDwCZI+gBIHMEPQBkjqAHgMwR9ACQOYIeADJH0ANA5gh6AMgcQQ8AmSPoASBzBD0AZC4p6G03bL9pe9n2033u32v7p7b/ZPubw4wFAGytgUFvuyLpmKT9kvZIetz2nk3dfivpiKR/+QvGAgC2UMqMfq+k5Yg4HxEfSHpO0sHeDhHxTkSckfTnYccCALZWStBvl/RWT3uley1F8ljbh2w3bTfX1tYSXx4AMEhK0LvPtUh8/eSxETETEfWIqI+Pjye+PABgkJSgX5F0V097h6TVxNe/nrEAgBsgJejPSNpt+27b2yQ9Jmk+8fWvZywA4AaoDuoQEW3bT0o6Jaki6dmIeMP24e7947Y/Jqkp6UOSNmx/Q9KeiPhDv7Fb9LsAAPpwROpy+81Tr9ej2WwWXQYAlIbtsxFR73ePT8YCQOYIegDIHEEPAJkj6AEgcwQ9AGSOoAeAzBH0AJA5gh5I1Gq1dOTIEbVaraJLAYZC0AOJZmdnde7cOc3NzRVdCjAUgh5I0Gq1tLCwoIjQwsICs3qUCkEPJJidndXGxoYkaX19nVk9SoWgBxIsLS2p3W5LktrtthYXFwuuCEhH0AMJJiYmVK12NnutVquanJwsuCIgHUEPJJientbYWOfPpVKpaGpqquCKgHQEPZCgVqup0WjIthqNhmq1WtElAckGHjwCoGN6eloXLlxgNo/SIeiBRLVaTUePHi26DGBoLN0AQOYIegDIHEEPAJkj6AEgcwQ9AGSOoAeAzBH0AJA5gh4AMkfQA0DmCHoAyBxBDwCZI+iBRBwOjrIi6IFEJ06c0GuvvaaZmZmiSwGGQtADCVqtlpaWliRJi4uLzOpRKklBb7th+03by7af7nPfto92779m+5M99y7YPmf7VdvNG1k8cLOcOHHi8uHgGxsbzOpRKgOD3nZF0jFJ+yXtkfS47T2buu2XtLv7dUjSdzfdfzgiPhER9esvGbj5XnrppSval2b3QBmkzOj3SlqOiPMR8YGk5yQd3NTnoKS56HhF0odtf/wG1woUxvY128AoSwn67ZLe6mmvdK+l9glJp22ftX3oaj/E9iHbTdvNtbW1hLKAm2ffvn3XbAOjLCXo+01dYog+D0bEJ9VZ3vma7c/0+yERMRMR9Yioj4+PJ5QF3Dyf+9znrtkGRllK0K9IuqunvUPSamqfiLj07zuSXlRnKQgolRdeeOGabWCUpQT9GUm7bd9te5ukxyTNb+ozL2mq+/TNpyX9PiLetn2b7dslyfZtkh6R9PoNrB+4KTa/Gbu5DYyy6qAOEdG2/aSkU5Iqkp6NiDdsH+7ePy7ppKTPSlqW9J6kL3WHf1TSi903rqqSfhgRCzf8twC2WERcsw2MsoFBL0kRcVKdMO+9drzn+5D0tT7jzkt64DprBAq3b98+nT59+nJ7YmKiwGqA4fDJWCDB5z//+SvavBmLMiHogQTz81e+LfXjH/+4oEqA4RH0QILNn4RdXFwsqBJgeAQ9kGBiYuLyp2Fta3JysuCKgHQEPZDgwIEDl5+0iQg9+uijBVcEpCPogQTz8/NXzOhZo0eZEPRAgqWlpStm9KzRo0wIeiDBxMSEqtXOx06q1Spr9CgVgh5IMD09fXnpZmxsTFNTUwVXBKQj6IEEtVpN27d3dt6+8847VavVCq4ISEfQAwlarZZWVzubtq6urnJmLEqFoAcSzM7OXnFm7NzcXMEVAekIeiDB0tKS2u22JKndbvPUDUqFoAcS8NQNyoygBxJMT09rbKzz51KpVHjqBqVC0AMJarWaGo2GbKvRaPDUDUqFoAcSHThwQLfeeiv73KB0CHog0fPPP68//vGPHAyO0iHogQStVuvyUYKnTp3iOXqUCkEPJDhx4sQV7ZmZmYIqAYZH0AMJNj8333tQODDqCHogwaUtiq/WBkYZQQ8AmSPoASBzBD0AZK5adAEYbc8884yWl5eLLmMkff3rXy+6hELdc889euqpp4ouAwmY0QNA5pjR45qYsXV873vf0w9+8IPL7ampKT3xxBMFVgSkY0YPJPjqV796RZuQR5kwo++DdWn0U6lUtL6+rvHx8b/69XlcadTfryDo+1heXtarr/9C67d+pOhSMELGxv5GGpPe/t+K3j7/P0WXgxFRee+3RZcwUFLQ225I+ldJFUnfj4h/3nTf3fuflfSepH+MiJ+ljB1V67d+RO/f+9miywAw4m755cmiSxhoYNDbrkg6JmlS0oqkM7bnI+LnPd32S9rd/fqUpO9K+lTi2JFz8eJFVd77fSn+BwRQrMp7LV282C66jGtKmdHvlbQcEeclyfZzkg5K6g3rg5LmorMByCu2P2z745J2JYwdTettVd5jK1ptrEvs64J+bGmsUnQVxVsf7ZCX0oJ+u6S3etor6szaB/XZnjhWkmT7kKRDkrRz586EsrbOQw89xJuxXRcvXtT7779fdBkYQbfccou2b99edBkj4Z577im6hGtKCXr3ubZ5ine1PiljOxcjZiTNSFK9Xi90CjnK754DwLBSgn5F0l097R2SVhP7bEsYCwDYQikfmDojabftu21vk/SYpPlNfeYlTbnj05J+HxFvJ44FAGyhgTP6iGjbflLSKXUekXw2It6wfbh7/7ikk+o8WrmszuOVX7rW2C35TQAAfXkUT8qp1+vRbDaLLgMASsP22Yio97vHXjcAkDmCHgAyR9ADQOYIegDI3Ei+GWt7TdJviq4D6OMOSe8WXQTQx99FxHi/GyMZ9MCost282pMNwKhi6QYAMkfQA0DmCHpgODNFFwAMizV6AMgcM3oAyBxBDwCZI+iBBLYbtt+0vWz76aLrAYbBGj0wQPeQ+1+p55B7SY+P+iH3wCXM6IHB9qp7yH1EfCDp0iH3QCkQ9MBg/Q6551RslAZBDwyWfMg9MIoIemCwFXHIPUqMoAcG45B7lNrAw8GBv3Ycco+y4/FKAMgcSzcAkDmCHgAyR9ADQOYIegDIHEEPAJkj6AEgcwQ9AGTu/wBrLTVMw6P3hwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authent Type\n",
      "CachedInteractive          96\n",
      "InteractiveLogon          729\n",
      "NetworkLogon            14012\n",
      "RemoteInteractive         142\n",
      "ScreensaverDismissed      478\n",
      "ScreensaverInvoked        467\n",
      "Service                    21\n",
      "TGS                     23357\n",
      "TGT                      6150\n",
      "WorkstationLock          1208\n",
      "WorkstationUnlock        1559\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "simple_ae_eval(anomaly_bool_N_5,anomalies_ae_n_5,val_loss_n_5,non_rt_sample,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac58db-748b-44c3-aa3a-8298c6e06b38",
   "metadata": {},
   "source": [
    "#### Poisson AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0c0ff-3602-4d7f-b3fe-d6e75b87ab00",
   "metadata": {},
   "source": [
    "Here we run a Poisson anomaly detection algorithm.\n",
    "\n",
    "Based on a gamma prior for a poisson distribution, we can estimate $ \\lambda $ by $ \\frac{(\\sum{x_{i}} + \\alpha)}{(n + \\beta)}$. This is because under a $\\Gamma(\\alpha,\\beta)$ prior for $\\lambda$ we have that the posterior is $$ \\pi(\\lambda|x) \\propto \\lambda^{\\sum{x_{i}}+\\alpha-1} e^{-(n+\\beta)\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e393d7-720f-4180-a896-1da54490fff1",
   "metadata": {},
   "source": [
    "This allows us to find anomalies based on the output of our poisson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82f5b874-bdd1-4942-bcd9-287408c45857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cad03-d1ba-4e55-830d-c285d1fa8f4e",
   "metadata": {},
   "source": [
    "Here we define the Poisson anomaly detection model. We define the model with both the possibility to combine and not combine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aad350a-20c5-4a11-a3c6-7f73a5175939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_ae_detection(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in tqdm(enumerate(usernames)):\n",
    "\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for col in pois_df.columns:\n",
    "            dt = pois_df.iloc[:57*n][col]\n",
    "            bayes_mean.append((alpha+sum(dt))/(beta+len(dt)))\n",
    "            bayes_var.append((alpha+sum(dt))/(beta+len(dt))**2)\n",
    "\n",
    "        probabilities = stats.poisson.pmf(pois_df.iloc[57*n:82*n],bayes_mean)\n",
    "        \n",
    "        if comb == True:\n",
    "            for i in range(len(probabilities)):\n",
    "\n",
    "                prob = probabilities[i]\n",
    "\n",
    "                f_probs.append(stats.combine_pvalues(prob))\n",
    "            for i in range(len(f_probs)):\n",
    "\n",
    "                pv = f_probs[i][1]\n",
    "\n",
    "                if pv <= 0.05:\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "        elif comb == False:\n",
    "            for i in range(len(probabilities)):\n",
    "                \n",
    "                prob = probabilities[i]\n",
    "                \n",
    "                if any(prob <= 0.05):\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "    events = pd.concat(events_frames)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4454a45-6bb1-401d-b67c-30fd484b3c3c",
   "metadata": {},
   "source": [
    "Here we define our analysis for the poisson anomaly detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9edbd74-018e-4675-b5a9-a9783921f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_poiss_eval(anomaly_bools,anomalies,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c3311-6b49-4481-86ab-209398101381",
   "metadata": {},
   "source": [
    "This block of code runs and analyses the models predictions on the red team data when combining results using Fisher's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c0dc4b3-b421-45ee-9b5d-f74fc192b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[AC:\\Users\\corri\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:7661: RuntimeWarning: divide by zero encountered in log\n",
      "  statistic = -2 * np.sum(np.log(pvalues))\n",
      "\n",
      "\n",
      "1it [00:04,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:08,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:13,  4.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:17,  4.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:21,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:26,  4.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:31,  4.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:36,  4.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:40,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:45,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:49,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "12it [00:54,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:59,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "14it [01:03,  4.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "15it [01:07,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "16it [01:12,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "17it [01:16,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "18it [01:21,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "19it [01:26,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "20it [01:31,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "21it [01:35,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "22it [01:39,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "23it [01:44,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "24it [01:49,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "25it [01:53,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "26it [01:58,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "27it [02:02,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "28it [02:07,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "29it [02:12,  4.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "30it [02:16,  4.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "31it [02:21,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "32it [02:25,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "33it [02:30,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "34it [02:34,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "35it [02:39,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "36it [02:43,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "37it [02:49,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "38it [02:53,  4.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "39it [02:58,  4.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "40it [03:02,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "41it [03:07,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "42it [03:12,  4.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "43it [03:16,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "44it [03:20,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "45it [03:25,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "46it [03:30,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "47it [03:34,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "48it [03:39,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "49it [03:43,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "50it [03:47,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "51it [03:52,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "52it [03:56,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "53it [04:01,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "54it [04:05,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "55it [04:09,  4.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "56it [04:14,  4.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "57it [04:18,  4.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "58it [04:23,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "59it [04:27,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "60it [04:32,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "61it [04:37,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "62it [04:42,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "63it [04:46,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "64it [04:50,  4.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "65it [04:55,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "66it [04:59,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "67it [05:04,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "68it [05:09,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "69it [05:13,  4.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "70it [05:17,  4.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "71it [05:22,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "72it [05:27,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "73it [05:31,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "74it [05:35,  4.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "75it [05:40,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "76it [05:44,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "77it [05:48,  4.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "78it [05:54,  4.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "79it [05:58,  4.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "80it [06:03,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "81it [06:07,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "82it [06:12,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "83it [06:16,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "84it [06:20,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "85it [06:25,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "86it [06:29,  4.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "87it [06:33,  4.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "88it [06:38,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "89it [06:43,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "90it [06:47,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "91it [06:52,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "92it [06:56,  4.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "93it [07:00,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "94it [07:05,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "95it [07:10,  4.53s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:13.824025\n",
      "90 of the red team usernames were identified to have anomalies out of 99 red team usernames. The folloiwng usernames were found to have no anomalies\n",
      "--------------------------------------------------------------------------------------------\n",
      "User133028, User702833, User859748, User457932, User829941, User405399, User900703, User497510, User222822\n",
      "14.71% of the red team data was identified as anomalous.\n",
      "Authent Type\n",
      "Batch                    199\n",
      "CachedInteractive        101\n",
      "InteractiveLogon         647\n",
      "NetworkLogon            8717\n",
      "RemoteInteractive        232\n",
      "ScreensaverDismissed     199\n",
      "ScreensaverInvoked       196\n",
      "Service                  453\n",
      "TGS                     7817\n",
      "TGT                     2416\n",
      "WorkstationLock          538\n",
      "WorkstationUnlock        815\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms = poiss_ae_detection(list(set(rt_usernames)),24,True)\n",
    "\n",
    "poisson_anoms = poisson_anoms.drop_duplicates()\n",
    "poiss_anom_bool = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms['UserName']):\n",
    "        poiss_anom_bool.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)\n",
    "simple_poiss_eval(poiss_anom_bool,poisson_anoms,list(set(rt_usernames)),'red team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b323e-e62b-4121-a270-9441d39ed292",
   "metadata": {},
   "source": [
    "This block of code runs and analyses the poisson model on the red team data when not combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "186e6008-7413-4c61-86dd-860bf94fd6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:04,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:09,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:14,  4.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:19,  5.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:23,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:28,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:33,  4.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:37,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:42,  4.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:47,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:51,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "12it [00:56,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "13it [01:00,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "14it [01:05,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "15it [01:10,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "16it [01:14,  4.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "17it [01:19,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "18it [01:24,  4.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "19it [01:29,  4.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "20it [01:33,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "21it [01:38,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "22it [01:43,  4.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "23it [01:48,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "24it [01:52,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "25it [01:57,  4.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "26it [02:02,  4.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "27it [02:07,  4.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "28it [02:12,  4.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "29it [02:16,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "30it [02:21,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "31it [02:25,  4.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "32it [02:30,  4.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "33it [02:35,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "34it [02:40,  4.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "35it [02:45,  4.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "36it [02:50,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "37it [02:54,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "38it [02:59,  4.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "39it [03:04,  4.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "40it [03:09,  4.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "41it [03:14,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "42it [03:18,  4.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "43it [03:24,  4.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "44it [03:29,  4.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "45it [03:33,  4.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "46it [03:37,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "47it [03:42,  4.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "48it [03:47,  4.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "49it [03:52,  4.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "50it [03:56,  4.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "51it [04:02,  4.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "52it [04:07,  4.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "53it [04:11,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "54it [04:16,  4.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "55it [04:20,  4.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "56it [04:25,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "57it [04:29,  4.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "58it [04:34,  4.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "59it [04:39,  4.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "60it [04:44,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "61it [04:49,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "62it [04:53,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "63it [04:57,  4.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "64it [05:02,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "65it [05:06,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "66it [05:11,  4.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "67it [05:16,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "68it [05:21,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "69it [05:25,  4.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "70it [05:30,  4.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "71it [05:34,  4.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "72it [05:39,  4.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "73it [05:43,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "74it [05:48,  4.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "75it [05:52,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "76it [05:57,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "77it [06:02,  4.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "78it [06:07,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "79it [06:11,  4.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "80it [06:17,  4.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "81it [06:21,  4.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "82it [06:26,  4.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "83it [06:30,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "84it [06:34,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "85it [06:39,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "86it [06:43,  4.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "87it [06:48,  4.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "88it [06:52,  4.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "89it [06:57,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "90it [07:02,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "91it [07:07,  4.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "92it [07:11,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "93it [07:16,  4.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "94it [07:20,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "95it [07:24,  4.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "96it [07:28,  4.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "97it [07:32,  4.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "98it [07:37,  4.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "99it [07:42,  4.67s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:47.529822\n",
      "98 of the red team usernames were identified to have anomalies out of 99 red team usernames. The folloiwng usernames were found to have no anomalies\n",
      "--------------------------------------------------------------------------------------------\n",
      "User405399\n",
      "16.96% of the red team data was identified as anomalous.\n",
      "Authent Type\n",
      "Batch                     200\n",
      "CachedInteractive         133\n",
      "InteractiveLogon          733\n",
      "NetworkLogon            10545\n",
      "RemoteInteractive         264\n",
      "ScreensaverDismissed      210\n",
      "ScreensaverInvoked        207\n",
      "Service                   454\n",
      "TGS                      9000\n",
      "TGT                      2560\n",
      "WorkstationLock           569\n",
      "WorkstationUnlock         873\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_nc = poiss_ae_detection(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_nc = poisson_anoms_nc.drop_duplicates()\n",
    "poiss_anom_bool_nc = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_nc['UserName']):\n",
    "        poiss_anom_bool_nc.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_nc.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)\n",
    "simple_poiss_eval(poiss_anom_bool_nc,poisson_anoms_nc,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130fb296-cef4-4ad3-8acf-c84057596784",
   "metadata": {},
   "source": [
    "This block of code runs and analyses the poisson model for the non red team username sample when not combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d963555f-95ec-47ca-9881-77be8a323954",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-39-09c9ffaaad9a>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-39-09c9ffaaad9a>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    simple_poiss_eval(poiss_anom_bool_n,poisson_anoms_n,non_rt_sample,'norma\u001b[0m\n\u001b[1;37m                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "poisson_anoms_n = poiss_ae_detection(non_rt_sample,24,False)\n",
    "\n",
    "poisson_anoms_n = poisson_anoms_n.drop_duplicates()\n",
    "poiss_anom_bool_n = []\n",
    "\n",
    "for un in non_rt_sample:\n",
    "    if un in list(poisson_anoms_n['UserName']):\n",
    "        poiss_anom_bool_n.append((1,un))\n",
    "    else:  \n",
    "        poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)\n",
    "simple_poiss_eval(poiss_anom_bool_n,poisson_anoms_n,non_rt_sample,'norma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30626643-df10-4da2-bc1f-9e89627682c4",
   "metadata": {},
   "source": [
    "#### Inhomogenous Poisson AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abb51d-151f-44a8-8ce5-a38acdb4c53e",
   "metadata": {},
   "source": [
    "Here we define our 'inhomohenous' poisson model. For each day we calculate an independent poisson $ \\lambda $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250aaac-19d7-4bfc-8714-e35edeedc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhom_poiss_ae_detection_2(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for i in range(n):\n",
    "            bayes_mean_hr = []\n",
    "            bayes_var_hr = []\n",
    "            for col in pois_df.columns:\n",
    "                dt = list(pd.concat([pois_df.iloc[:57*n],pois_df.iloc[83*n:]])[col])\n",
    "                dt_hspl = dt[i::n]\n",
    "                bayes_mean_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl)))\n",
    "                bayes_var_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl))**2)\n",
    "\n",
    "            bayes_mean.append(bayes_mean_hr)\n",
    "            bayes_var.append(bayes_var_hr)\n",
    "        \n",
    "        find_prob = []\n",
    "        for i in range(n):\n",
    "            find_prob.append(pd.DataFrame(stats.poisson.pmf(pd.concat([pois_df.iloc[:57*n], pois_df.iloc[83*n:]]).iloc[i::n],bayes_mean[i])).min())\n",
    "                \n",
    "        probabilities = []\n",
    "        \n",
    "        for i in range(n):\n",
    "\n",
    "            probabilities.append(stats.poisson.pmf(pois_df.iloc[57*n:82*n].iloc[i::n],bayes_mean[i]))\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            probs = probabilities[i]\n",
    "\n",
    "            for j in range(len(probs)):\n",
    "\n",
    "                hr_prob = probs[j]\n",
    "                \n",
    "                thresh = find_prob[i]\n",
    "                \n",
    "                if [item1 for item1,item2 in zip(hr_prob,thresh) if item1 <= item2] != []:\n",
    "\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+j*n+i))\n",
    "\n",
    "    events = pd.concat(events_frames)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ad0c1-dc42-4c2e-93c8-789deeb5c35c",
   "metadata": {},
   "source": [
    "Here we run our inhomogenous poisson model on the red team data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5665af4-5c4a-42b1-9826-d63bae07034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_inhom = inhom_poiss_ae_detection_2(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_inhom = poisson_anoms_inhom.drop_duplicates()\n",
    "poiss_anom_bool_inhom = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_inhom['UserName']):\n",
    "        poiss_anom_bool_inhom.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_inhom.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)\n",
    "simple_poiss_eval(poiss_anom_bool_inhom,poisson_anoms_inhom,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622c965-8fed-4642-bd9f-a0dbf915012a",
   "metadata": {},
   "source": [
    "Here we run our inhomogenous model on the non-red team data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04fc3c-2091-42cb-ab7d-bb788b2f2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poiss_ae_detection_2(non_rt_sample,24,False)\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poisson_anoms_n.drop_duplicates()\n",
    "inhom_poiss_anom_bool_n = []\n",
    "\n",
    "for un in non_rt_sample:\n",
    "    if un in list(inhom_poisson_anoms_n['UserName']):\n",
    "        inhom_poiss_anom_bool_n.append((1,un))\n",
    "    else:\n",
    "        inhom_poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)\n",
    "simple_poiss_eval(inhom_poiss_anom_bool_n,inhom_poisson_anoms_n,non_rt_sample,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff18d0-74d2-4e15-b7eb-8ef2109f2b0a",
   "metadata": {},
   "source": [
    "#### Supervised Neural Network AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d7fdb-c43e-4f8d-a19e-9bbf7a68edb9",
   "metadata": {},
   "source": [
    "Here we create our data for our supervised neural network data and turn the task of unsupervised anomaly detection into supervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c9cd7-d2a6-48bf-adaf-a8811b391cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_sample = list(pd.Series(rt_usernames).unique()) + non_rt_sample\n",
    "data = []\n",
    "for user in tqdm(username_sample):\n",
    "    if user in rt_usernames:\n",
    "        anom = 1\n",
    "    else:\n",
    "        anom = 0\n",
    "    df = feature_generation_supervised(user,24,anom)\n",
    "    data.append(df)\n",
    "nn_data = pd.concat(data)\n",
    "nn_data.to_csv('nn_supervised data.gz', compression='gzip')\n",
    "X_train, X_test, y_train, y_test = train_test_split(nn_data[['UserName','SrcUnique','DstUnique','Authentications','Failures']], nn_data['Anomaly'], test_size=0.1, random_state=42)\n",
    "X_train_un_list = list(X_train['UserName'])\n",
    "X_test_un_list = list(X_test['UserName'])\n",
    "X_train = X_train.drop('UserName',axis=1)\n",
    "X_test = X_test.drop('UserName',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dec00-6a4f-46ed-8a6b-e95c52650ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_anomaly_finder(x_train,x_test,y_train,y_test,train_user,test_user,n,BATCH_SIZE=256, EPOCHS=1000):\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train)\n",
    "    x_train = pipeline.transform(x_train)\n",
    "    x_test = pipeline.transform(x_test)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the nn\n",
    "    neural_network =Sequential([\n",
    "\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    neural_network.compile(optimizer=\"adam\", \n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = neural_network.fit(\n",
    "        x_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = neural_network.predict(x_test)\n",
    "    Y_pred = x_test_pred.argmax(axis=1)\n",
    "    test_loss, test_acc = neural_network.evaluate(X_test, y_test)\n",
    "    print(test_acc)\n",
    "    \n",
    "    anomalies = x_test[np.where(x_test_pred == 1)]\n",
    "    #anomaly_idx = anomalies[0]\n",
    "    \n",
    "    #frame = []\n",
    "    #for i in range(len(anomaly_idx)):\n",
    "    #    user = test_user[anomaly_idx[i]]\n",
    "    #    anomaly = nn_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "    #    if len(anomaly) == 0:\n",
    "    #        pass\n",
    "    #    else:\n",
    "    #        frame.append(anomaly)\n",
    "    #    \n",
    "    #if len(frame) != 0:\n",
    "    #    anomaly_df = pd.concat(frame)\n",
    "    #    return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    #else:\n",
    "    #    print('No anomalies found.')\n",
    "    #    return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2580a2d-f652-4e2c-96d9-211e5f4c4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nn_anomaly_finder(X_train,X_test,y_train,y_test,X_train_un_list,X_test_un_list,24,BATCH_SIZE=256, EPOCHS=1000)\n",
    "pd.DataFrame(Y_pred).groupby(0).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca10f08-9f94-415e-8e12-e046452454a1",
   "metadata": {},
   "source": [
    "#### ASE AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc1c9e-fe9b-4a55-82a2-18e8cbca60f1",
   "metadata": {},
   "source": [
    "Here we implement the ASE function based on - https://arxiv.org/pdf/2008.10055.pdf, https://jscholarship.library.jhu.edu/bitstream/handle/1774.2/37075/SUSSMAN-DISSERTATION-2014.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf73cf-225a-4d78-b7c4-db9f4a7648a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASE_anom_finder(window_length):\n",
    "\n",
    "    vertex_norm_tracker = []\n",
    "    graph_norm_tracker = []\n",
    "\n",
    "    graph_mam_tracker = []\n",
    "    graph_range_tracker = []\n",
    "\n",
    "    vertex_mam_tracker = []\n",
    "    vertex_range_tracker = []\n",
    "\n",
    "    for t in tqdm(range(len(data_frame_list_uase)-1)):\n",
    "        u1, s1, v1 = scipy.sparse.linalg.svds(data_frame_list_uase[t],k=7)\n",
    "        u2, s2, v2 = scipy.sparse.linalg.svds(data_frame_list_uase[t+1],k=7)\n",
    "        Y1 = pd.DataFrame((v1.transpose()*np.abs(s1)**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "        Y2 = pd.DataFrame((v2.transpose()*np.abs(s2)**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "\n",
    "        nrm = scipy.linalg.norm(Y2-Y1,ord=2)\n",
    "        graph_norm_tracker.append(nrm)\n",
    "\n",
    "        vertex_norms = []\n",
    "        for v in list(columns_sparse):\n",
    "            nrm = scipy.linalg.norm(Y2[v]-Y1[v],ord=2)\n",
    "            vertex_norms.append(nrm)\n",
    "\n",
    "        vertex_norm_tracker.append(vertex_norms)\n",
    "\n",
    "        if t >= window_length:\n",
    "            graph_mam = np.sum(graph_norm_tracker[(t-window_length+1):t-1])/(window_length-1)\n",
    "            graph_range = np.sum(np.linalg.norm(np.array(graph_norm_tracker[t-window_length+2:t-1])-np.array(graph_norm_tracker[t-window_length+1:t-2])))/(1.128*(window_length-2))\n",
    "            graph_mam_tracker.append(graph_mam)\n",
    "            graph_range_tracker.append(graph_range)\n",
    "\n",
    "            vertex_mam_tracker_day = []\n",
    "            vertex_range_tracker_day = []\n",
    "\n",
    "            for j in range(len(list(columns_sparse))):\n",
    "                vertex_mam = np.sum([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_mam_tracker_day.append(vertex_mam)\n",
    "                vertex_range = np.std([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_range_tracker_day.append(vertex_range)\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker_day)\n",
    "            vertex_range_tracker.append(vertex_range_tracker_day)\n",
    "\n",
    "    day_anomalies = []\n",
    "\n",
    "    for i,v in enumerate(list(columns_sparse)):\n",
    "        y_v = np.array([vertex_norm_tracker[k][2] for k in range(len(vertex_norm_tracker))])\n",
    "        ucl_v = np.array([vertex_mam_tracker[k][2] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][0] for k in range(len(vertex_range_tracker))])\n",
    "        anomalies = [j+window_length for j,y in enumerate(y_v[window_length:]) if y > ucl_v[j]]\n",
    "        day_anomalies.append((v,anomalies))\n",
    "        \n",
    "    return vertex_norm_tracker, graph_norm_tracker, graph_mam_tracker, graph_range_tracker, vertex_mam_tracker, vertex_range_tracker, day_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622043a0-6d59-41ac-a2a1-2d1a1c793313",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_norm_tracker_7, graph_norm_tracker_7, graph_mam_tracker_7, graph_range_tracker_7, vertex_mam_tracker_7, vertex_range_tracker_7, day_anomalies_7 = ASE_anom_finder(7)\n",
    "vertex_norm_tracker_14, graph_norm_tracker_14, graph_mam_tracker_14, graph_range_tracker_14, vertex_mam_tracker_14, vertex_range_tracker_14, day_anomalies_14 = ASE_anom_finder(14)\n",
    "vertex_norm_tracker_3, graph_norm_tracker_3, graph_mam_tracker_3, graph_range_tracker_3, vertex_mam_tracker_3, vertex_range_tracker_3, day_anomalies_3 = ASE_anom_finder(3)\n",
    "vertex_norm_tracker_2, graph_norm_tracker_2, graph_mam_tracker_2, graph_range_tracker_2, vertex_mam_tracker_2, vertex_range_tracker_2, day_anomalies_2 = ASE_anom_finder(2)\n",
    "vertex_norm_tracker_5, graph_norm_tracker_5, graph_mam_tracker_5, graph_range_tracker_5, vertex_mam_tracker_5, vertex_range_tracker_5, day_anomalies_5 = ASE_anom_finder(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca680204-d6bb-4289-92cc-478b182bc9fb",
   "metadata": {},
   "source": [
    "We also produce plots of the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d0a74-7a0a-40dc-ad7c-68d1b3b655b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_creation(window_length,vertex_mam_tracker,vertex_range_tracker,vertex_norm_tracker,u):\n",
    "    \n",
    "    plt.figure(figsize = (16,8))  \n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),[vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])\n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),np.array([vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][0] for k in range(len(vertex_range_tracker))]),'--')\n",
    "    plt.plot([vertex_norm_tracker[k][u] for k in range(len(vertex_norm_tracker))],'r.')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('$||{X_{i}^{(t+1)}-X_{i}^{(t)}}||$')\n",
    "    plt.title('Window Length: {}'.format(window_length))\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86de32a-7ba0-4880-a85a-b0e2c4a67906",
   "metadata": {},
   "source": [
    "The below code finds a red team user and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e707d7d-555e-483f-b740-f1bf6e650b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_user_idx = []\n",
    "for i,un in enumerate(columns_sparse):\n",
    "    b = un in list(rt_usernames)\n",
    "    if b == True:\n",
    "        rt_user_idx.append(i)\n",
    "\n",
    "plot_creation(14,vertex_mam_tracker_14,vertex_range_tracker_14,vertex_norm_tracker_14,idx[0])\n",
    "plot_creation(7,vertex_mam_tracker_7,vertex_range_tracker_7,vertex_norm_tracker_7,idx[0])\n",
    "plot_creation(5,vertex_mam_tracker_5,vertex_range_tracker_5,vertex_norm_tracker_5,idx[0])\n",
    "plot_creation(3,vertex_mam_tracker_3,vertex_range_tracker_3,vertex_norm_tracker_3,idx[0])\n",
    "plot_creation(2,vertex_mam_tracker_2,vertex_range_tracker_2,vertex_norm_tracker_2,idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba00baf-54d2-4ea9-8af4-812b9bf20acc",
   "metadata": {},
   "source": [
    "### Combining Anomaly Detection Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1fb20-2a7c-4632-b1cb-d4235476a5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ae00e1-37da-4dba-a7c9-9386c6d8bdf8",
   "metadata": {},
   "source": [
    "### Final Anomaly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c247f-de1e-4a7a-a04c-1953d6d8c268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
