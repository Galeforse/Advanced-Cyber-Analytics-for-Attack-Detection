{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e84a26-1a84-4c12-bcb4-081f774f1e6e",
   "metadata": {},
   "source": [
    "# Final Anomaly Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891d2d3-acab-4904-989f-d9d9adf1e12b",
   "metadata": {},
   "source": [
    "### Original Data and Other Necessary Data Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea18a9f-9dfa-4bcb-b732-6a0a51eda0e3",
   "metadata": {},
   "source": [
    "Here are all the libraries and modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94ede6d-bd5f-4a1d-9f9a-2c8cc70c70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCP_APR import CP_APR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import bz2\n",
    "import random\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38072c7-9f37-4785-971a-1490156ea1d7",
   "metadata": {},
   "source": [
    "We set seeds to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdd04db-1c55-4820-a10e-330ec05b1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f6a49-a70f-4483-a953-7b5363df41c5",
   "metadata": {},
   "source": [
    "Here we import the original data - these are the data summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844457f4-7fa5-49d3-b2f8-41b05958624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read entire data set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corri\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Attempting to read entire data set.')\n",
    "    authentication_data = pd.read_csv('../Data/Authentication data.gz', compression='gzip', index_col = 0)\n",
    "    process_data = pd.read_csv('../Data/Process data.gz', compression='gzip', index_col = 0)\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Unable to read entire data set, reading from original files.')\n",
    "    rootdir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls'\n",
    "    unzippeddir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls/Unzipped'\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == '.gz':\n",
    "                filedir = rootdir + '/' + file\n",
    "                with gzip.open(filedir) as f:\n",
    "                    df = pd.read_csv(filedir, header=None)\n",
    "                    frames.append(df)\n",
    "                if 'authentications' in str(file):\n",
    "                    count = count + len(df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    authentication_data = df[:count]\n",
    "    authentication_data.columns = ['UserName', 'SrcDevice','DstDevice', 'Authent Type', 'Failure', 'DailyCount']\n",
    "\n",
    "    process_data = df[count:]\n",
    "    process_data = process_data[[0,1,2,3,4]]\n",
    "    process_data.columns = ['UserName', 'Device', 'ProcessName', 'ParentProcessName', 'DailyCount']\n",
    "\n",
    "    authentication_data.to_csv('../Data/Authentication data.gz', header=True, compression='gzip')\n",
    "    process_data.to_csv('../Data/Process data.gz', header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282df99d-80ac-40bb-834f-fe16e601327a",
   "metadata": {},
   "source": [
    "The list of known red team users, which is assumed to be comprehensive, is required later on for analysis of techniques. We'll also obtain a list of non red-team users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085b03a3-3c26-4008-804d-61a0fa8da190",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_usernames = list(pd.read_csv('../Data/AuthUserNames.txt', header=None)[0])\n",
    "non_rt_users = [un for un in authentication_data['UserName'].unique() if un not in rt_usernames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca82707-08f1-49e9-81fb-459d0b193bf4",
   "metadata": {},
   "source": [
    "For data creation later we require a list of authentication types - this can be created here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c7fec8-a6d4-424a-9c29-852f750890ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = list(authentication_data['Authent Type'].unique())\n",
    "AT_dict = { i : a_t[i] for i in range(0, len(a_t) ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f6798-760c-487d-80f6-47db52a5be7f",
   "metadata": {},
   "source": [
    "Finally, we require a list of the seperation points in our data frame. Each day can be seperated out by indexing and we find the indices here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e314dae9-4ed4-4e7b-9713-62e4eda1e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_index_list = authentication_data.index.tolist()\n",
    "auth_start_days = [i for i, e in enumerate(auth_index_list) if e == 0]\n",
    "auth_start_days.append(len(authentication_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916819f-7163-41e2-821a-144a7ea699ad",
   "metadata": {},
   "source": [
    "### Data Creation for Anomaly Detction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280345f-3f38-48c5-8e74-38f093b90a7f",
   "metadata": {},
   "source": [
    "#### Authentication Type Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400f548-da88-4fec-82c2-7a7944dd2953",
   "metadata": {},
   "source": [
    "#### Various Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06e104-8801-42f7-98fa-af0ede49d31e",
   "metadata": {},
   "source": [
    "#### Adjacency Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6cc7e-3c68-4c4c-83fd-3c8fcf594c33",
   "metadata": {},
   "source": [
    "#### Supervised Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eeb4a-c65d-4423-a855-0305ada36ef9",
   "metadata": {},
   "source": [
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d832-5dae-40cc-b669-63128638792f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba00baf-54d2-4ea9-8af4-812b9bf20acc",
   "metadata": {},
   "source": [
    "### Combining Anomaly Detection Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1fb20-2a7c-4632-b1cb-d4235476a5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ae00e1-37da-4dba-a7c9-9386c6d8bdf8",
   "metadata": {},
   "source": [
    "### Final Anomaly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c247f-de1e-4a7a-a04c-1953d6d8c268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
