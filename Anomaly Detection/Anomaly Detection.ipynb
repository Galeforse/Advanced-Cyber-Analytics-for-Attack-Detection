{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8416ea61-e02e-4905-bf6b-8d96807140e2",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8039-eee4-484e-838e-3f8ec42ca268",
   "metadata": {},
   "source": [
    "In this notebook we finally perform our anomaly detection. We perform this in 6 steps:\n",
    "1. Create a data frame for each individual username showing authentication types for 8/24 hours of each day\n",
    "2. Use EDA, Isolation Forest's, Local Outlier Factor and other models to find 'normal' days or 'normal' usernames to allow us to train th CP_APR model\n",
    "3. Train the CP_APR model with the data we've identified in step 2\n",
    "4. Run the trained CP_APR model on the other data to identify anomalies in the 'test' data\n",
    "5. Use a function to return the anomalous entry from the original data frame based on the output of the CP_APR function\n",
    "6. Create a new data frame of anomalies\n",
    "\n",
    "Finally, we may verify this process through other means such as HTM studio for a subset or other anomaly detection techniques. We may also use the original red team authentication data to determine whether the events given there were picked up by the CP_APR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7621-65db-4fee-8767-ff6fced6f973",
   "metadata": {},
   "source": [
    "First we import our libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324bf39e-193d-443f-b8aa-f7d38b0ce455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCP_APR import CP_APR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import bz2\n",
    "import random\n",
    "random.seed(1134)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b403bc-8bf1-4fb1-ada6-bedb07590004",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb2728-63b8-46bb-bfca-d8199e11374c",
   "metadata": {},
   "source": [
    "Now we import the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e567a1-389c-4125-a73d-8f100841418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read entire data set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corri\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Attempting to read entire data set.')\n",
    "    authentication_data = pd.read_csv('../Data/Authentication data.gz', compression='gzip', index_col = 0)\n",
    "    process_data = pd.read_csv('../Data/Process data.gz', compression='gzip', index_col = 0)\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Unable to read entire data set, reading from original files.')\n",
    "    rootdir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls'\n",
    "    unzippeddir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls/Unzipped'\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == '.gz':\n",
    "                filedir = rootdir + '/' + file\n",
    "                with gzip.open(filedir) as f:\n",
    "                    df = pd.read_csv(filedir, header=None)\n",
    "                    frames.append(df)\n",
    "                if 'authentications' in str(file):\n",
    "                    count = count + len(df)\n",
    "\n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    authentication_data = df[:count]\n",
    "    authentication_data.columns = ['UserName', 'SrcDevice','DstDevice', 'Authent Type', 'Failure', 'DailyCount']\n",
    "\n",
    "    process_data = df[count:]\n",
    "    process_data = process_data[[0,1,2,3,4]]\n",
    "    process_data.columns = ['UserName', 'Device', 'ProcessName', 'ParentProcessName', 'DailyCount']\n",
    "\n",
    "    authentication_data.to_csv('../Data/Authentication data.gz', header=True, compression='gzip')\n",
    "    process_data.to_csv('../Data/Process data.gz', header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b2ae0-becb-4d45-b680-eb5867a34fe0",
   "metadata": {},
   "source": [
    "### Other required data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2c11b-bca3-4c47-8de9-4134a94e2cf5",
   "metadata": {},
   "source": [
    "#### Possible Username Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3966cd5-bf94-4301-ad8d-79b81c856bea",
   "metadata": {},
   "source": [
    "We need a list of usernames we'll consider for training/testing. Currently at the beginning of all this we will consider all usernames for both training and testing and reduce this as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97df6b7a-8fc0-4814-98bb-05227271c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = list(authentication_data['UserName'].unique())\n",
    "test_users = list(authentication_data['UserName'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed820446-d320-4774-bc7b-8ebb0b025420",
   "metadata": {},
   "source": [
    "#### Authentication Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc959f95-cfef-4504-8e99-12ba9e80930c",
   "metadata": {},
   "source": [
    "We'll need a dictionary of authentication types for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd6b3b6-607b-452a-9627-7911413bee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = list(authentication_data['Authent Type'].unique())\n",
    "AT_dict = { i : a_t[i] for i in range(0, len(a_t) ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb28cc-fe6c-45e5-8e2f-d32a450dad54",
   "metadata": {},
   "source": [
    "#### Authentication Day Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc7edb-5462-4328-aadb-9ff165d08a4a",
   "metadata": {},
   "source": [
    "The below code defines the indices where each day begins in the authentiation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f244ce39-dffe-43f3-bb9e-30d2883e29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_index_list = authentication_data.index.tolist()\n",
    "auth_start_days = [i for i, e in enumerate(auth_index_list) if e == 0]\n",
    "auth_start_days.append(len(authentication_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c7cea-a0d2-4a09-a9a3-e4a4a1d04f26",
   "metadata": {},
   "source": [
    "### Step 1: DataFrame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68659e-83da-4900-8f06-658cb8729480",
   "metadata": {},
   "source": [
    "This first function is used to split a data frame into equal chunks. Since we need to split each day into 8/24 hours we use this function to split into equal time periods - this may not be perfectly representitive of the actual hour split but should be a good estimate since we don't have the original time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cd7347-8165-4a1a-8d48-cfef11033d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df,n): \n",
    "    chunks = list()\n",
    "    chunk_size = int(np.round(df.shape[0]/n))\n",
    "    num_chunks = n\n",
    "    for i in range(num_chunks):\n",
    "        if i != num_chunks-1:\n",
    "            chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "        else:\n",
    "            chunks.append(df[i*chunk_size:])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e87e56-af50-43cf-90d8-94a4047db507",
   "metadata": {},
   "source": [
    "This function creates the required data frames. It takes as input a username and a split by number (8/24) and returns a data frame of the user's authentiation events split by type over 90 days, split by 8/24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1edb198f-6cbe-4c06-874d-86394c9218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_type_un_df(user,n):\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    n = n\n",
    "    auth_type_dict = {}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                data = chunks[j]\n",
    "                auth_type_data = data[data['UserName'] == user].groupby('Authent Type').size()\n",
    "                auth_type_dict[i*n + j] = auth_type_df.index.to_series().map(auth_type_data.to_dict())\n",
    "    \n",
    "    auth_type_df = pd.DataFrame(data=auth_type_dict,index = list(authentication_data['Authent Type'].unique()))\n",
    "    auth_type_df = auth_type_df.transpose()\n",
    "    auth_type_df = auth_type_df.fillna(0)\n",
    "    \n",
    "    return auth_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9f5ca-df35-4972-a2c6-ca78b6e29283",
   "metadata": {},
   "source": [
    "This function creates the inputs for our CP_APR model. We pass a list of usernames to the function and it returns the set of co-ordinate tuples (i,j,e) where we have non-zero entries in our data matrices, along with the corresponding values for that matrix. i is the row of the matrix i.e. time, j is the column i.e. authentication type and e is the username number. We can instead pass a single username which would return this for just one user but this is optimised to run for all users when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4ac08a-4102-4d22-92ca-2e8bff478603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_df(usernamelist,n):\n",
    "    \n",
    "    coords = []\n",
    "    vals_list = []\n",
    "    \n",
    "    for e,user in enumerate(usernamelist):\n",
    "        df = auth_type_un_df(user,n)\n",
    "    \n",
    "        s = sparse.coo_matrix(df)\n",
    "        co = [[s.row[i],s.col[i],e] for i in range(len(s.row))]\n",
    "        vals = s.data\n",
    "        \n",
    "        coords.append(co)\n",
    "        vals_list.append(vals)\n",
    "    \n",
    "    coords = np.array([item for sublist in coords for item in sublist])\n",
    "    vals_list = np.array([item for sublist in vals_list for item in sublist])\n",
    "    \n",
    "    return vals_list, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bed4f4-dc9c-4106-87c9-9dcf3fa129a1",
   "metadata": {},
   "source": [
    "### Step 2: Determining Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26571997-92d2-4bde-b7dc-79446683803f",
   "metadata": {},
   "source": [
    "Here we determine the training data we'll use in our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ab1d-922e-438f-8bfd-2bfc013b59f8",
   "metadata": {},
   "source": [
    "### Step 3: Train the CP_APR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beacb46-1e6f-4d71-8cc4-b47bbdf11fc7",
   "metadata": {},
   "source": [
    "Here we define our CP_APR model. We then train it on the data we have determined to be 'normal' above to teach the model what is likely to be normal activity in the authentication sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5522919e-1210-47f7-af89-8d2fe19d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_apr = CP_APR(n_iters=10, random_state=42, verbose=200, method='numpy', return_type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51b7f32-7771-483b-890e-2153b8ca0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#factors = cp_apr.fit(coords=train_coords, values=train_vals)\n",
    "#factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb2eb9-af37-46fa-a4a2-11cffc038eb9",
   "metadata": {},
   "source": [
    "### Step 4: Apply the CP_APR model to the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf3137-049f-4aed-bba3-675b8ff7d34d",
   "metadata": {},
   "source": [
    "Here we apply the model to the data we want to find anomalies in. This data will then be used to find the final set of anomalies to pass into the final stage of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df36cb1d-9ba4-4d4d-8bce-dc8311807add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_values = cp_apr.predict_scores(coords=test_coords, values=test_vals)\n",
    "#p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c6823-c17a-4e25-9112-ce6f6da2c890",
   "metadata": {},
   "source": [
    "### Step 5: Obtain the data frame of anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cf321-c5f0-4a62-b80f-d43df88c7848",
   "metadata": {},
   "source": [
    "Here we use the p-values found above to retrieve the final set of anomalies from the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b583dd-abf8-4f3c-9fb7-7ce3908715d4",
   "metadata": {},
   "source": [
    "This function returns a single anomaly based on the test coordinates array we obtain i.e. the actual data we look for anomalies in, the entry value i.e. the position of the anomaly in the array output by our CP_APR model and n, the number of hours we split the data frame by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cad662e-8ad8-4a37-846f-a710f867651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orig_finder(test_coords, entry_val, n):\n",
    "    \n",
    "    # gets the co-ordinates of the entry where we have the erro\n",
    "    orig_co = test_coords[entry_val]\n",
    "    \n",
    "    # gets the authentication type\n",
    "    authent = AT_dict[orig_co[1]]\n",
    "    \n",
    "    # gets the username of the individual who the anomaly occured with\n",
    "    username = test_users[orig_co[2]]\n",
    "    \n",
    "    # gets the day the anomaly occured (n is the number of hours we split the data frame into)\n",
    "    day = int(orig_co[0]/n)\n",
    "    \n",
    "    # gets the hour the anomaly occured in\n",
    "    hour = orig_co[0] - n * day\n",
    "    \n",
    "    # gets the n hour chunks for that day\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[day]:auth_start_days[day+1]],n)\n",
    "    \n",
    "    # gets the hour\n",
    "    data = chunks[hour]\n",
    "    \n",
    "    # finds the anomaly\n",
    "    anom = data[(data['UserName'] == username) & (data['Authent Type'] == authent)]\n",
    "    \n",
    "    return anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af207da-84f4-45d6-a09e-0d6241f386f7",
   "metadata": {},
   "source": [
    "The p-values array defined below will be the output of the CP_APR function. We then set a threshold for anomaly scores to determine what we will class as an anomaly. Using the np.where function we will find all instances where we are below the threshold and return a data frame of the anomalies that we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f567bd-70ab-4905-b8d1-06e355af5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames = []\n",
    "#threshold = 0.05\n",
    "\n",
    "#for i in range(len(np.where(p_values < threshold)[0])):\n",
    "#    entry = np.where(p_values < threshold)[0][i]\n",
    "#    anom = orig_finder(test_coords, entry_val, 24)\n",
    "#    frames.append(anom)\n",
    "    \n",
    "#anomalies = pd.concat(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
