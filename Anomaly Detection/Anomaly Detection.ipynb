{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8416ea61-e02e-4905-bf6b-8d96807140e2",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8039-eee4-484e-838e-3f8ec42ca268",
   "metadata": {},
   "source": [
    "In this notebook we finally perform our anomaly detection. We perform this in 6 steps:\n",
    "1. Create a data frame for each individual username showing authentication types for 8/24 hours of each day\n",
    "2. Use EDA, Isolation Forest's, Local Outlier Factor and other models to find 'normal' days or 'normal' usernames to allow us to train th CP_APR model\n",
    "3. Train the CP_APR model with the data we've identified in step 2\n",
    "4. Run the trained CP_APR model on the other data to identify anomalies in the 'test' data\n",
    "5. Use a function to return the anomalous entry from the original data frame based on the output of the CP_APR function\n",
    "6. Create a new data frame of anomalies\n",
    "\n",
    "Finally, we may verify this process through other means such as HTM studio for a subset or other anomaly detection techniques. We may also use the original red team authentication data to determine whether the events given there were picked up by the CP_APR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7621-65db-4fee-8767-ff6fced6f973",
   "metadata": {},
   "source": [
    "First we import our libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324bf39e-193d-443f-b8aa-f7d38b0ce455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCP_APR import CP_APR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import bz2\n",
    "import random\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bac645-9484-4140-8cd1-fddaad31862d",
   "metadata": {},
   "source": [
    "We want to create reproducibility for our neural networks and doing the following permits this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1ab95c-a310-4eca-aecb-217a3145c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b403bc-8bf1-4fb1-ada6-bedb07590004",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb2728-63b8-46bb-bfca-d8199e11374c",
   "metadata": {},
   "source": [
    "Now we import the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e567a1-389c-4125-a73d-8f100841418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to read entire data set, reading from original files.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ea9caac9adfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempting to read entire data set.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mauthentication_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/Authentication data.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/Process data.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gzip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ea9caac9adfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mfiledir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrootdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                     \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'authentications'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Attempting to read entire data set.')\n",
    "    authentication_data = pd.read_csv('../Data/Authentication data.gz', compression='gzip', index_col = 0)\n",
    "    process_data = pd.read_csv('../Data/Process data.gz', compression='gzip', index_col = 0)\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Unable to read entire data set, reading from original files.')\n",
    "    rootdir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls'\n",
    "    unzippeddir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls/Unzipped'\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == '.gz':\n",
    "                filedir = rootdir + '/' + file\n",
    "                with gzip.open(filedir) as f:\n",
    "                    df = pd.read_csv(filedir, header=None)\n",
    "                    frames.append(df)\n",
    "                if 'authentications' in str(file):\n",
    "                    count = count + len(df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    authentication_data = df[:count]\n",
    "    authentication_data.columns = ['UserName', 'SrcDevice','DstDevice', 'Authent Type', 'Failure', 'DailyCount']\n",
    "\n",
    "    process_data = df[count:]\n",
    "    process_data = process_data[[0,1,2,3,4]]\n",
    "    process_data.columns = ['UserName', 'Device', 'ProcessName', 'ParentProcessName', 'DailyCount']\n",
    "\n",
    "    authentication_data.to_csv('../Data/Authentication data.gz', header=True, compression='gzip')\n",
    "    process_data.to_csv('../Data/Process data.gz', header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd6d06-a9fa-4db5-99c2-426ca02a0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication_data[authentication_data['UserName'] == 'User035855']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbb5e1-95c0-42de-81db-224622d790d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication_data['Authent Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b2ae0-becb-4d45-b680-eb5867a34fe0",
   "metadata": {},
   "source": [
    "### Other required data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2c11b-bca3-4c47-8de9-4134a94e2cf5",
   "metadata": {},
   "source": [
    "#### Possible Username Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3966cd5-bf94-4301-ad8d-79b81c856bea",
   "metadata": {},
   "source": [
    "We need a list of usernames we'll consider for training/testing. Currently at the beginning of all this we will consider all usernames for both training and testing and reduce this as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df6b7a-8fc0-4814-98bb-05227271c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = list(authentication_data['UserName'].unique())\n",
    "test_users = list(authentication_data['UserName'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac116b2-e39f-4e4c-bb81-dd503c0c3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e2983-bb66-4416-8535-5799ba253b0e",
   "metadata": {},
   "source": [
    "#### Authentication Red Team Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593a077-e344-4498-b626-c27d6499c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_usernames = list(pd.read_csv('../Data/AuthUserNames.txt', header=None)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed820446-d320-4774-bc7b-8ebb0b025420",
   "metadata": {},
   "source": [
    "#### Authentication Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc959f95-cfef-4504-8e99-12ba9e80930c",
   "metadata": {},
   "source": [
    "We'll need a dictionary of authentication types for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6b3b6-607b-452a-9627-7911413bee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = list(authentication_data['Authent Type'].unique())\n",
    "AT_dict = { i : a_t[i] for i in range(0, len(a_t) ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb28cc-fe6c-45e5-8e2f-d32a450dad54",
   "metadata": {},
   "source": [
    "#### Authentication Day Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc7edb-5462-4328-aadb-9ff165d08a4a",
   "metadata": {},
   "source": [
    "The below code defines the indices where each day begins in the authentiation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244ce39-dffe-43f3-bb9e-30d2883e29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_index_list = authentication_data.index.tolist()\n",
    "auth_start_days = [i for i, e in enumerate(auth_index_list) if e == 0]\n",
    "auth_start_days.append(len(authentication_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c7cea-a0d2-4a09-a9a3-e4a4a1d04f26",
   "metadata": {},
   "source": [
    "### Step 1: DataFrame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68659e-83da-4900-8f06-658cb8729480",
   "metadata": {},
   "source": [
    "This first function is used to split a data frame into equal chunks. Since we need to split each day into 8/24 hours we use this function to split into equal time periods - this may not be perfectly representitive of the actual hour split but should be a good estimate since we don't have the original time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd7347-8165-4a1a-8d48-cfef11033d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df,n):\n",
    "    chunks = list()\n",
    "    chunk_size = int(np.round(df.shape[0]/n))\n",
    "    num_chunks = n\n",
    "    for i in range(num_chunks):\n",
    "        if i != num_chunks-1:\n",
    "            chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "        else:\n",
    "            chunks.append(df[i*chunk_size:])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e87e56-af50-43cf-90d8-94a4047db507",
   "metadata": {},
   "source": [
    "This function creates the required data frames. It takes as input a username and a split by number (8/24) and returns a data frame of the user's authentiation events split by type over 90 days, split by 8/24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb198f-6cbe-4c06-874d-86394c9218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_type_un_df(user,n):\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    n = n\n",
    "    auth_type_dict = {}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                data = chunks[j]\n",
    "                auth_type_data = data[data['UserName'] == user].groupby('Authent Type')['DailyCount'].sum()\n",
    "                auth_type_dict[i*n + j] = auth_type_df.index.to_series().map(auth_type_data.to_dict())\n",
    "    \n",
    "    auth_type_df = pd.DataFrame(data=auth_type_dict,index = list(authentication_data['Authent Type'].unique()))\n",
    "    auth_type_df = auth_type_df.transpose()\n",
    "    auth_type_df = auth_type_df.fillna(0)\n",
    "    \n",
    "    return auth_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9f5ca-df35-4972-a2c6-ca78b6e29283",
   "metadata": {},
   "source": [
    "This function creates the inputs for our CP_APR model. We pass a list of usernames to the function and it returns the set of co-ordinate tuples (i,j,e) where we have non-zero entries in our data matrices, along with the corresponding values for that matrix. i is the row of the matrix i.e. time, j is the column i.e. authentication type and e is the username number. We can instead pass a single username which would return this for just one user but this is optimised to run for all users when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ac08a-4102-4d22-92ca-2e8bff478603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sparse_df(usernamelist,n):\n",
    "    \n",
    "#     coords = []\n",
    "#     vals_list = []\n",
    "    \n",
    "#     for e,user in enumerate(usernamelist):\n",
    "#         df = auth_type_un_df(user,n)\n",
    "    \n",
    "#         s = sparse.coo_matrix(df)\n",
    "#         co = [[s.row[i],s.col[i],e] for i in range(len(s.row))]\n",
    "#         vals = s.data\n",
    "        \n",
    "#         coords.append(co)\n",
    "#         vals_list.append(vals)\n",
    "    \n",
    "#     coords = np.array([item for sublist in coords for item in sublist])\n",
    "#     vals_list = np.array([item for sublist in vals_list for item in sublist])\n",
    "    \n",
    "#     return vals_list, coords\n",
    "\n",
    "# the function above does this for a list of usernames - doesn't work atm so needs fixing but the below does what we want for a single username\n",
    "def sparse_df(username,n):\n",
    "    \n",
    "    df = auth_type_un_df(username,n)\n",
    "\n",
    "    s = sparse.coo_matrix(df)\n",
    "    co = [[s.row[i],s.col[i],1] for i in range(len(s.row))]\n",
    "    vals = s.data\n",
    "    \n",
    "    return vals, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb064a5-0c91-421f-87aa-aff2ae8142e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stime = datetime.datetime.now()\n",
    "\n",
    "# n=10\n",
    "# for i in range(n):\n",
    "#     vals,co = sparse_df(train_users[i],24)\n",
    "\n",
    "# etime = datetime.datetime.now()\n",
    "\n",
    "# print('Time taken for {} iterations: {}.'.format(n,etime-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265acb9-9286-41bc-9454-c5a8eb557129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stime = datetime.datetime.now()\n",
    "\n",
    "# n=10\n",
    "# for i in range(n):\n",
    "#     vals,co = sparse_df(train_users[i],24)\n",
    "\n",
    "# etime = datetime.datetime.now()\n",
    "\n",
    "# print('Time taken for {} iterations: {}.'.format(n,etime-stime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10ecba-3a94-4048-af4a-18f6c7ce59d8",
   "metadata": {},
   "source": [
    "So we take roughly 4 seconds to compute a single username - doing this for 28,815 usernames would take $28,815 * \\frac{4}{86400} = 1.3 $ days. Lets rewrite this with parallelisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0a1b7-ba9e-4f8a-82ab-29f40bfdbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i SparseDataFrameCreation.py 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d26457-1512-4dc9-a015-7112e9ca61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i SparseDataFrameCreation.py 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb8779-e061-4608-a82f-e523b04a7537",
   "metadata": {},
   "source": [
    "We see speed improvements for large numbers of usernames but not for small ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfc727-2a73-4090-b22c-884434e3f696",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb7cb5-eb60-491f-be44-ee341632ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of a full matrix of the data we're considering called on a single username over 24 hours\n",
    "#auth_type_un_df(train_users[0],24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfd98b-0772-4f95-9e5b-7e816c1451d7",
   "metadata": {},
   "source": [
    "From the above we see that we obtain a data set of each day split into 24 hours. Each column represents an authentication type and non-zero entries represent an authentication event in that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e30917-564c-4a65-8a0f-de953d22e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of the sparse matrix representation we'll pass to the model - creates a big list so is commented but feel free to uncomment\n",
    "# sparse_df(train_users[1000],24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d342d13-c398-4787-85c8-d31a4dd99f52",
   "metadata": {},
   "source": [
    "The first list is the non-zero values in the matrix and the second list is the list of co-ordinates where those non-zero values occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bed4f4-dc9c-4106-87c9-9dcf3fa129a1",
   "metadata": {},
   "source": [
    "### Step 2: Determining Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66026c-b492-4cdf-a434-73066e6f4ea8",
   "metadata": {},
   "source": [
    "Days 1-56, and 83-90 are normal activity and therefore are training data whereas days 57-82 contain red team data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b585c-0e92-4f3c-8b6c-dcfbb252d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_iso_lof(data,plot=False,c='auto'):\n",
    "    \n",
    "    # scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data))\n",
    "    \n",
    "    # isolation forest predictions\n",
    "    if_model = IsolationForest(contamination=c)\n",
    "    if_predictions = if_model.fit_predict(data)\n",
    "    \n",
    "    # local outlier factor predictions\n",
    "    lof = LocalOutlierFactor(n_neighbors=2)\n",
    "    lof_predictions = lof.fit_predict(data)\n",
    "    \n",
    "    if plot == True:\n",
    "        \n",
    "        # PCA reduction for plotting\n",
    "        pca = PCA(n_components=2)\n",
    "        auth_types_pca = pd.DataFrame(pca.fit_transform(data))\n",
    "        \n",
    "        # finding anomaly locations\n",
    "        a_if = auth_types_pcapca.loc[if_predictions == -1]\n",
    "        a_lof = auth_types_pca.loc[lof_predictions == -1]\n",
    "        \n",
    "        anomalies = auth_types_pca.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,6))\n",
    "        ax.plot(auth_types_pca[0], auth_types_pca[1], color='black', label='Normal')\n",
    "        ax.scatter(anomalies[0], anomalies[1], color='red', label='Anomaly')\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Number of Events\")\n",
    "        ax.text(0,auth_types_pca[1].max()-0.1,('Number of combined anomalies found: {}. \\n Number of LOF anomalies found: {}. \\n Number of IF anomalies found: {}.'.format(len(anomalies), len(a_lof), len(a_if))))\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        a_if = data.loc[if_predictions == -1]\n",
    "        a_lof = data.loc[lof_predictions == -1]\n",
    "\n",
    "        anomalies = data.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "    \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bfece-4400-488d-9e05-c3e3c14754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i ParallelisedTrainingData.py 22815"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ab1d-922e-438f-8bfd-2bfc013b59f8",
   "metadata": {},
   "source": [
    "### Step 3: Train the CP_APR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beacb46-1e6f-4d71-8cc4-b47bbdf11fc7",
   "metadata": {},
   "source": [
    "Here we define our CP_APR model. We then train it on the data we have determined to be 'normal' above to teach the model what is likely to be normal activity in the authentication sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522919e-1210-47f7-af89-8d2fe19d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_apr = CP_APR(n_iters=10, random_state=42, verbose=200, method='numpy', return_type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b7f32-7771-483b-890e-2153b8ca0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#factors = cp_apr.fit(coords=train_coords, values=train_vals)\n",
    "#factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb2eb9-af37-46fa-a4a2-11cffc038eb9",
   "metadata": {},
   "source": [
    "### Step 4: Apply the CP_APR model to the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf3137-049f-4aed-bba3-675b8ff7d34d",
   "metadata": {},
   "source": [
    "Here we apply the model to the data we want to find anomalies in. This data will then be used to find the final set of anomalies to pass into the final stage of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36cb1d-9ba4-4d4d-8bce-dc8311807add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_values = cp_apr.predict_scores(coords=test_coords, values=test_vals)\n",
    "#p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c6823-c17a-4e25-9112-ce6f6da2c890",
   "metadata": {},
   "source": [
    "### Step 5: Obtain the data frame of anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cf321-c5f0-4a62-b80f-d43df88c7848",
   "metadata": {},
   "source": [
    "Here we use the p-values found above to retrieve the final set of anomalies from the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b583dd-abf8-4f3c-9fb7-7ce3908715d4",
   "metadata": {},
   "source": [
    "This function returns a single anomaly based on the test coordinates array we obtain i.e. the actual data we look for anomalies in, the entry value i.e. the position of the anomaly in the array output by our CP_APR model and n, the number of hours we split the data frame by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad662e-8ad8-4a37-846f-a710f867651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orig_finder(test_coords, entry_val, n):\n",
    "    \n",
    "    # gets the co-ordinates of the entry where we have the erro\n",
    "    orig_co = test_coords[entry_val]\n",
    "    \n",
    "    # gets the authentication type\n",
    "    authent = AT_dict[orig_co[1]]\n",
    "    \n",
    "    # gets the username of the individual who the anomaly occured with\n",
    "    username = test_users[orig_co[2]]\n",
    "    \n",
    "    # gets the day the anomaly occured (n is the number of hours we split the data frame into)\n",
    "    day = int(orig_co[0]/n)\n",
    "    \n",
    "    # gets the hour the anomaly occured in\n",
    "    hour = orig_co[0] - n * day\n",
    "    \n",
    "    # gets the n hour chunks for that day\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[day]:auth_start_days[day+1]],n)\n",
    "    \n",
    "    # gets the hour\n",
    "    data = chunks[hour]\n",
    "    \n",
    "    # finds the anomaly\n",
    "    anom = data[(data['UserName'] == username) & (data['Authent Type'] == authent)]\n",
    "    \n",
    "    return anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af207da-84f4-45d6-a09e-0d6241f386f7",
   "metadata": {},
   "source": [
    "The p-values array defined below will be the output of the CP_APR function. We then set a threshold for anomaly scores to determine what we will class as an anomaly. Using the np.where function we will find all instances where we are below the threshold and return a data frame of the anomalies that we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f567bd-70ab-4905-b8d1-06e355af5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames = []\n",
    "#threshold = 0.05\n",
    "\n",
    "#for i in range(len(np.where(p_values < threshold)[0])):\n",
    "#    entry = np.where(p_values < threshold)[0][i]\n",
    "#    anom = orig_finder(test_coords, entry_val, 24)\n",
    "#    frames.append(anom)\n",
    "    \n",
    "#anomalies = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ec7bc-9f7f-4351-9f0b-4c393e16ccec",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda737e-fe26-4e57-979a-7032b1efb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the first entry of the first day, where the Authentication type is TGS, for the 1000th user in the list of test_users\n",
    "orig_finder([[0,0,1000]],0,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292f89e-ca04-45a9-968b-2a3db3e4fdc0",
   "metadata": {},
   "source": [
    "Dan's Notes:\n",
    "- Possibly hard to work with a single machine since it may change role\n",
    "- Consider with computers, both source and destination computers\n",
    "- Natural extension is to throw more information at the prediction - features et\n",
    "- Plot p-values distribution (assume uniform) q-q plot (should see a gap and then you can set a threshold on them)\n",
    "- Counts over time, days of the week structure\n",
    "- Dismiss Saturdays/Sundays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a314449-486e-42f4-818d-c1de1d2e88d6",
   "metadata": {},
   "source": [
    "### Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd4f4b-9ddd-49ee-91e7-649d619e9a52",
   "metadata": {},
   "source": [
    "#### Convolutional Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8b124-2e06-4928-8e20-a9f781d140e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cae_feature_generation(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65e0f5-4d85-49d6-9dc8-7b6a0e45b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication type data frames - didn't seem to work particularly well - sparse data structures arent particularly successful in neural networks\n",
    "#df = auth_type_un_df(rt_usernames[0],24)\n",
    "#df = df.drop('ScreensaverDismissed',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01c814-9097-4cec-afc0-a37bd3c1a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cae_feature_generation(rt_usernames[0],24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b2402-8283-4fc0-8653-8df5027bb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 4\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df[:24*57])\n",
    "x_test = create_sequences(df[24*57:])\n",
    "x_train = np.pad(x_train,[(0,0),(0,0),(0,1)], constant_values=0)\n",
    "x_test = np.pad(x_test,[(0,0),(0,0),(0,1)], constant_values=0)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8965089-e81e-4dac-a49c-474c3318c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_l = layers.Input(shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\",  activation=\"relu\")(input_l)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=7, padding=\"same\",  activation=\"relu\")(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\")(encoded)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "convo_autoencoder = Model(input_l, decoded)\n",
    "convo_autoencoder.compile(metrics=['accuracy'], optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43484fe0-f386-4d65-9b9b-2ce0d42f732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8adc6-5561-489c-b3b1-65388ee7a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = convo_autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "        TqdmCallback(verbose=1)\n",
    "    ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c50a9-3dcd-44cf-8ec2-420b2a3fbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd1271-d8c3-4cef-b6b0-ac430bf1665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = convo_autoencoder.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred.reshape(x_train_pred.shape[0],x_train_pred.shape[1],x_train_pred.shape[2]) - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3d448-bf25-4c97-974c-e3e7804beaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(train_mae_loss,99)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e38511-372d-4a57-9b4b-7036861ebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred = convo_autoencoder.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred.reshape(x_test_pred.shape[0],x_test_pred.shape[1],x_test_pred.shape[2]) - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87203328-51c9-41f6-9e01-c51a2e02b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff986e19-2804-4681-bcee-a63cf8aae146",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd686e6e-a49d-4310-87ed-8223681ffbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_data_indices = []\n",
    "for idx in range(TIME_STEPS - 1, len(df) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[idx - TIME_STEPS + 1 : idx]):\n",
    "        anomalous_data_indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778efcfb-ec6c-4ff8-b4aa-d3fcd58f759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies = df.iloc[pd.Series(anomalous_data_indices).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92264a57-49ca-47d3-bcd1-de5e9c9c66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdb50-fa77-4b85-abba-6610399edbc3",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edb78a-724b-4dd3-94ea-b8716c087a88",
   "metadata": {},
   "source": [
    "Here we produce an auto encoder. This is semi-supervised learning since the NN is attempting to approximate the input data and then using mean squared error to determine the error in the prediction. High error indicates an anomalous entry since we couldn't predict this well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31ca41-87dd-4c00-9044-6d7075239cae",
   "metadata": {},
   "source": [
    "The first set of cells is a proof of concept allowing us to visualise the neural network before we define a full anomaly detecction function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2e4b8-9cac-43b1-adf8-46d60eef914b",
   "metadata": {},
   "source": [
    "We first get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e524b6-d2e8-4b9f-8fa5-eb68099d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296c65a-aef2-459f-a04c-43c126d25dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_generation(rt_usernames[0],24)\n",
    "x_train_ae = np.array(df[0:57*24])\n",
    "x_test_ae = np.array(df[57*24:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5bd74-c90e-4120-a46c-df7bfde9d8ef",
   "metadata": {},
   "source": [
    "We define a pipeline to rescale and normalize our data. This means the data will be optimised for our neural network since large values will be scaled down and small values will be scaled up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a672438-6d18-4ac5-8176-e795b7dfa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309ea99-d8df-46e0-97e5-0346fd935c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_ae)\n",
    "x_train_ae = pipeline.transform(x_train_ae)\n",
    "x_test_ae = pipeline.transform(x_test_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2fe4f-e245-4b80-876e-d24e9bb1074b",
   "metadata": {},
   "source": [
    "We define our autoencoder. The architecture is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d682af-33af-421d-86dd-6edbe5ad03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_ae.shape[1]\n",
    "BATCH_SIZE = 120\n",
    "EPOCHS = 50\n",
    "\n",
    "autoencoder =Sequential([\n",
    "\n",
    "    # deconstruct / encode\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "    # reconstruction / decode\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer=\"adam\", \n",
    "                    loss=\"mse\",\n",
    "                    metrics=[\"acc\"])\n",
    "\n",
    "\n",
    "autoencoder.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e164584-1fac-4c63-b224-1587cbd01025",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    x_train_ae, x_train_ae,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=cb   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c32a94-aef6-4e4e-8388-86c7969f8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e70033-f21e-45a4-9c8a-a686f28ea3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = autoencoder.predict(x_train_ae)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train_ae), axis=1)\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "train_mae_loss = mmscaler.fit_transform(train_mae_loss.reshape(-1,1))\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc614ea6-28dc-4da7-adc0-7e9f5a0788ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred = autoencoder.predict(x_test_ae)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ac353-ea13-47b0-807b-995fcaed41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler()\n",
    "test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e79e9-d7c2-4203-a983-5399dbfcc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_mae_loss > 0.95\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cfb14-1faa-4af3-a2c1-273b8e87a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies = df.iloc[pd.Series(np.where(anomalies)[0]).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cff01-a745-44ee-9f37-69cc68b92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029f5f1-bab1-4d95-918a-c110474a7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d38bda-9cc3-4bb6-851c-47a8cf4daf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_orig_finder(rt_usernames[0],24,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95537a5b-f55b-471b-9256-9eeb7c7fc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', StandardScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2653e-4e1c-49df-abab-adcaebf6fde4",
   "metadata": {},
   "source": [
    "This cell is just a quick test, as well as determining timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c5e1f-273e-4be7-b39d-a302f0eac371",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "anomalies = ae_anomaly_finder(rt_usernames[0],24)\n",
    "e_time=datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548abf7-72f3-443e-8d2d-b59a939b981f",
   "metadata": {},
   "source": [
    "In this cell we run the model over all the red team usernames to identify the anomalies in the defined period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0ba80-bb13-491b-b0cc-75fd568c30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae = pickle.load(open('Anomalies AE.p','rb'))\n",
    "    val_loss = pickle.load(open('Validation Loss.p','rb'))\n",
    "    anomaly_bool = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool.append((len(anomalies_ae[anomalies_ae['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames = []\n",
    "    anomaly_bool = []\n",
    "    val_loss = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames.append(f)\n",
    "            anomaly_bool.append((len(f),rt_usernames[i]))\n",
    "            val_loss.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool.append((0,rt_usernames[i]))\n",
    "            val_loss.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae = pd.concat(frames)\n",
    "    \n",
    "    pickle.dump(anomalies_ae, open('Anomalies AE.p','wb'))\n",
    "    pickle.dump(val_loss, open('Validation Loss.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013eb7b-cca2-4a05-bfe5-0ab9d9aafbab",
   "metadata": {},
   "source": [
    "Here we analyse the output of our auto encoder and review the amount of anomalies identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23106732-16db-4bf9-92ba-66fff31d6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom = []\n",
    "for i in range(len(anomaly_bool)):\n",
    "    if anomaly_bool[i][0] == 0:    \n",
    "        non_anom.append(anomaly_bool[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool]) > 0)[0]),len(anomaly_bool)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0f188-54cf-4a32-89d6-5d1dd269a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rt_users = [un for un in authentication_data['UserName'].unique() if un not in rt_usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a857e-84db-4963-8a0f-235698664c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_n = []\n",
    "anomaly_bool_N = []\n",
    "val_loss_n = []\n",
    "bound = 20\n",
    "rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "s_time = datetime.datetime.now()\n",
    "\n",
    "for i,un in enumerate(rand_non_rt):\n",
    "    clear_output(wait=True)\n",
    "    print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "    f,b,val = ae_anomaly_finder(un,24)\n",
    "    if b == 1:\n",
    "        frames_n.append(f)\n",
    "        anomaly_bool_N.append((len(f),un))\n",
    "        val_loss_n.append(val)\n",
    "    else:\n",
    "        anomaly_bool_N.append((0,un))\n",
    "        val_loss_n.append(val)\n",
    "        pass\n",
    "anomalies_ae_n = pd.concat(frames_n)\n",
    "\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddd5f5-1372-465d-8ccf-3a977df89072",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_n = []\n",
    "for i in range(len(anomaly_bool_N)):\n",
    "    if anomaly_bool_N[i][0] == 0:    \n",
    "        non_anom_n.append(anomaly_bool_N[i][1])\n",
    "        \n",
    "print('{} of the non red team usernames analysed were identified to have anomalies out of {} non red team usernames analysed. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_N]) > 0)[0]),len(anomaly_bool_N)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1805581-d900-4b5a-9766-ce21b17ca047",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea2131-5971-40ce-9dd0-efed635d6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae_n)/len(authentication_data[authentication_data['UserName'].isin(rand_non_rt)])\n",
    "print('{:.2f}% of the \"normal\" data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeb233-9800-4d5f-8670-8c2a2fbcc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f59bb-44e9-4075-a45e-4f510ec32dba",
   "metadata": {},
   "source": [
    "Analysing validation losses across both red team and non-red team usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15fa17-2f2d-466a-856f-9b7b6be3c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = []\n",
    "for i in range(len(val_loss)):\n",
    "    final_loss.append(val_loss[i][0][len(val_loss[i][0])-1])\n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(final_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd220a9-d00b-40fa-9349-b64b96bfd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_n = []\n",
    "for i in range(len(val_loss_n)):\n",
    "    final_loss_n.append(val_loss_n[i][len(val_loss_n[i])-1])\n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(final_loss_n)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d5c06-d9ed-4b23-9212-399c72d514e5",
   "metadata": {},
   "source": [
    "#### AE2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c185f3-665d-4c4e-bbcf-94d5d791ceb8",
   "metadata": {},
   "source": [
    "We add more features - giving both timing and day entries to the auto encoder to determine if this has an impact on the anomalies detected. We also change our scaling to MinMaxScaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d22c7-baf3-4ee4-9691-1348746b9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_2(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                #feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                feat_dict[i*n + j] = [hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82ef30-ac14-451c-a908-422eecc7f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_2(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_2(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23719dcb-aeb2-4a3c-bc1d-c218e7fbabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_2 = pickle.load(open('Anomalies AE Time.p','rb'))\n",
    "    val_loss_2 = pickle.load(open('Validation Loss Time.p','rb'))\n",
    "    anomaly_bool_2 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_2.append((len(anomalies_ae_2[anomalies_ae_2['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_2.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_2 = []\n",
    "    anomaly_bool_2 = []\n",
    "    val_loss_2 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_2(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_2.append(f)\n",
    "            anomaly_bool_2.append((len(f),rt_usernames[i]))\n",
    "            val_loss_2.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_2.append((0,rt_usernames[i]))\n",
    "            val_loss_2.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_2 = pd.concat(frames_2)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_2, open('Anomalies AE Time.p','wb'))\n",
    "    pickle.dump(val_loss_2, open('Validation Loss Time.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a935f-075f-4d0b-9506-e1dbd923be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_2 = []\n",
    "for i in range(len(anomaly_bool_2)):\n",
    "    if anomaly_bool_2[i][0] == 0:    \n",
    "        non_anom_2.append(anomaly_bool_2[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_2]) > 0)[0]),len(anomaly_bool_2)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493806bd-38ce-4cb6-9bf1-c75c728069ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_2 = 100*len(anomalies_ae_2)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e8776-8df2-48c6-bfd2-f6bea176ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_2 = []\n",
    "for i in range(len(val_loss_2)):\n",
    "    final_loss_2.append(val_loss_2[i][0][len(val_loss_2[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61a928-e609-4f8b-a55c-512a256e2a4a",
   "metadata": {},
   "source": [
    "So this reduced the number of usernames identified as anamolous but did also reduce the error - I think this is likely due to the min max scaling so I'll implement this without the additional features and see what impact it has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57846e-c3ae-4655-8099-64e950bf1a66",
   "metadata": {},
   "source": [
    "#### AE3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c173102-497c-41f7-aef5-466aed8b20e7",
   "metadata": {},
   "source": [
    "This autoencoder runs under the MinMaxScaling to determine the impact of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc48033-3437-4bf1-96a7-1bbbd6b3cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_3(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff8d51-763d-4ec9-ab9d-8bc4167d0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_3 = pickle.load(open('Anomalies AE MMS.p','rb'))\n",
    "    val_loss_3 = pickle.load(open('Validation Loss MMS.p','rb'))\n",
    "    anomaly_bool_3 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_3.append((len(anomalies_ae_3[anomalies_ae_3['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_3.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_3 = []\n",
    "    anomaly_bool_3 = []\n",
    "    val_loss_3 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_3(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_3.append(f)\n",
    "            anomaly_bool_3.append((len(f),rt_usernames[i]))\n",
    "            val_loss_3.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_3.append((0,rt_usernames[i]))\n",
    "            val_loss_3.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_3 = pd.concat(frames_3)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_3, open('Anomalies AE MMS.p','wb'))\n",
    "    pickle.dump(val_loss_3, open('Validation Loss MMS.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67cde3-cfd6-421b-928d-1e3e1fc8d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_3 = []\n",
    "for i in range(len(anomaly_bool_3)):\n",
    "    if anomaly_bool_3[i][0] == 0:    \n",
    "        non_anom_3.append(anomaly_bool_3[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_3]) > 0)[0]),len(anomaly_bool_3)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962591c7-c80d-46ae-ae45-1cb9ed0361c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_3 = 100*len(anomalies_ae_3)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b63fb-3752-4fff-a219-f20ec4049ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_3 = []\n",
    "for i in range(len(val_loss_3)):\n",
    "    final_loss_3.append(val_loss_3[i][0][len(val_loss_3[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7e3bb-7e93-4da6-97a3-ae71f1e2bfc5",
   "metadata": {},
   "source": [
    " A similar amount of data is identified as anomalous and the validation error is much lower while still avoiding overfitting so we'll stick with this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d49576-6a2c-4f3c-961f-683509757679",
   "metadata": {},
   "source": [
    "#### Analysis of 3rd DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de1849-306c-4330-9582-21ba4b91e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f336563-c9c5-4f7b-bf44-34c3be19a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_3.groupby('Authent Type').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f921dd-5703-42d0-a8b5-c07f3aadb7b9",
   "metadata": {},
   "source": [
    "Some of these authentications are unlikely to be anomalies - for example, ScreensaverInvoked is most likely not a malicious anomaly since the attacker wouldn't gain anything from this, whereas NetworkLogons may be malicious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb18f6c-c97a-4bd5-9d19-281d2dd0c684",
   "metadata": {},
   "source": [
    "#### AE4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb732f-d152-4896-98c8-c0254ac66e33",
   "metadata": {},
   "source": [
    "We'll work with an autoencoder where we also pass the number of network logons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838094f4-76d9-4868-b8f5-d17c41f9609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0840a-69aa-4601-ba26-f8ef19fb259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_CAT(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                \n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    auth_df = auth_type_un_df(user,n)\n",
    "    df = pd.concat([df,auth_df])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f9bc6-8ddd-43b7-a723-7315a5ffd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_4(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_CAT(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "    \n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb60713-3fd1-4fed-85d6-ee2a3cd593e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_4 = pickle.load(open('Anomalies AE CAT.p','rb'))\n",
    "    val_loss_4 = pickle.load(open('Validation Loss CAT.p','rb'))\n",
    "    anomaly_bool_4 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_4.append((len(anomalies_ae_4[anomalies_ae_4['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_4.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_4 = []\n",
    "    anomaly_bool_4 = []\n",
    "    val_loss_4 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_4(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_4.append(f)\n",
    "            anomaly_bool_4.append((len(f),rt_usernames[i]))\n",
    "            val_loss_4.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_4.append((0,rt_usernames[i]))\n",
    "            val_loss_4.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_4 = pd.concat(frames_4)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_4, open('Anomalies AE CAT.p','wb'))\n",
    "    pickle.dump(val_loss_4, open('Validation Loss CAT.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06e93e-0e3a-479b-a525-b9e4adc96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_4 = []\n",
    "for i in range(len(anomaly_bool_4)):\n",
    "    if anomaly_bool_4[i][0] == 0:    \n",
    "        non_anom_4.append(anomaly_bool_4[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_4]) > 0)[0]),len(anomaly_bool_4)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110b5ae-cf8f-444a-ba45-c7a3b3a4dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_4 = 100*len(anomalies_ae_4)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a4e95-132b-481a-90f3-e01e13193052",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_4 = []\n",
    "for i in range(len(val_loss_4)):\n",
    "    final_loss_4.append(val_loss_4[i][0][len(val_loss_4[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b087ac-613f-4fe3-bfec-40495fcf25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785df5f-43e3-4e05-bd77-2c152784d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(anomalies_ae_3,anomalies_ae_4,how='inner',on=list(anomalies_ae_4.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bafc2-a7c6-418e-a9a8-a0ba5d725605",
   "metadata": {},
   "source": [
    "So we find all (we miss 5) anomalies from the original auto encoder when including authentication type as a feature as well as finding an extra 22. The extra 22 are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458687e-e90c-4030-880d-23eb48412015",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([anomalies_ae_4, anomalies_ae_3, anomalies_ae_3]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c30d55-9ab8-4d14-8e6e-7511ff4a5f15",
   "metadata": {},
   "source": [
    "Reviewing these shows that they are 'interesting' authentiations - they are mostly authentication types that are of interest and are likely to be compromisable such as a network logon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b6fdc-b22d-45fe-8c0c-b3c073843ab0",
   "metadata": {},
   "source": [
    "We'll test this algorithm to see how many anomalies it picks up in a random subset of non-red team usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8521e5-8c9c-453a-9f90-91e866a94949",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_n_4 = []\n",
    "anomaly_bool_N_4 = []\n",
    "val_loss_n_4 = []\n",
    "bound = 20\n",
    "rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "s_time = datetime.datetime.now()\n",
    "\n",
    "for i,un in enumerate(rand_non_rt):\n",
    "    clear_output(wait=True)\n",
    "    print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "    f,b,val = ae_anomaly_finder_4(un,24)\n",
    "    if b == 1:\n",
    "        frames_n_4.append(f)\n",
    "        anomaly_bool_N_4.append((len(f),un))\n",
    "        val_loss_n_4.append(val)\n",
    "    else:\n",
    "        anomaly_bool_N_4.append((0,un))\n",
    "        val_loss_n_4.append(val)\n",
    "        pass\n",
    "anomalies_ae_n_4 = pd.concat(frames_n_4)\n",
    "\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04612f1-7d50-431d-9b48-c542e02b4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae_n_4)/len(authentication_data[authentication_data['UserName'].isin(rand_non_rt)])\n",
    "print('{:.2f}% of the \"normal\" data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722694c-64a1-492f-b7b0-158f1a38d33f",
   "metadata": {},
   "source": [
    "Worryingly, we have a large amount of anomalies found in the 'normal' data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be209556-b6f7-4761-9d07-0afc5c0cb234",
   "metadata": {},
   "source": [
    "#### AE5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae4aa4-c5e0-4f86-92ce-5845b09b44c0",
   "metadata": {},
   "source": [
    "I'm not 100% sure but I think minmax scaling the output data is having adverse affects on our results - the third auto encoder seemed to do the 'best' so we'll use that as a basis but we'll remove scaling of the 'anomaly scores' and instead use the validation loss as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead163e-796b-48c3-844c-26bdc1aa0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_5(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > np.array(history.history[\"val_loss\"]).min())\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e05ddf-935f-49b2-980e-5ac7cc5ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_5 = pickle.load(open('Anomalies AE VL.p','rb'))\n",
    "    val_loss_5 = pickle.load(open('Validation Loss VL.p','rb'))\n",
    "    anomaly_bool_5 = []\n",
    "    anomalies_ae_5 = anomalies_ae_5.drop_duplicates()\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_5.append((len(anomalies_ae_5[anomalies_ae_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_5 = []\n",
    "    anomaly_bool_5 = []\n",
    "    val_loss_5 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_5(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_5.append(f)\n",
    "            anomaly_bool_5.append((len(f),rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_5.append((0,rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_5 = pd.concat(frames_5)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_5, open('Anomalies AE VL.p','wb'))\n",
    "    pickle.dump(val_loss_5, open('Validation Loss VL.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7452fdf-098e-408b-8fa6-999204c7ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ae_eval(anomaly_bools,anomalies,val_losses,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    perc_2 = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(anom)])\n",
    "    print('{:.2f}% of the anomalous user name data was identified as anomalous.'.format(perc_2,type_un))\n",
    "    \n",
    "    final_losses = []\n",
    "    for i in range(len(val_losses)):\n",
    "        final_losses.append(val_losses[i][0][len(val_losses[i][0])-1]) \n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(data = final_losses)\n",
    "    plt.show()\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788fc59-9443-40ca-8a72-0d17dc824a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_5,anomalies_ae_5,val_loss_5,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588045b-55d4-4e78-8f40-cc2fe22f46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    rand_non_rt = random.sample(non_rt_users,bound)\n",
    "    anomalies_ae_n_5 = pickle.load(open('Non RT Anomalies.p','rb'))\n",
    "    val_loss_n_5 = pickle.load(open('Non RT VL.p','rb'))\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "    anomaly_bool_N_5 = []\n",
    "    anomalies_ae_n_5 = anomalies_ae_n_5.drop_duplicates()\n",
    "\n",
    "    for un in rand_non_rt:\n",
    "        try:\n",
    "            anomaly_bool_N_5.append((len(anomalies_ae_n_5[anomalies_ae_n_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_n_5 = []\n",
    "    anomaly_bool_N_5 = []\n",
    "    val_loss_n_5 = []\n",
    "    bound = 200\n",
    "    rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i,un in enumerate(rand_non_rt):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "        f,b,val = ae_anomaly_finder_5(un,24)\n",
    "        if b == 1:\n",
    "            frames_n_5.append(f)\n",
    "            anomaly_bool_N_5.append((len(f),un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "        else:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "            pass\n",
    "    anomalies_ae_n_5 = pd.concat(frames_n_5)\n",
    "    pickle.dump(anomalies_ae_n_5,open('Non RT Anomalies.p','wb'))\n",
    "    pickle.dump(val_loss_n_5,open('Non RT VL.p','wb'))\n",
    "    pickle.dump(rand_non_rt,open('Random Sample of Non Red Team Usernames.p','wb'))\n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a59b9-58ce-4910-979b-ca0a044b0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_N_5,anomalies_ae_n_5,val_loss_n_5,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bfb9b-db68-4460-b963-7f50fb7b47e6",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacae89-9c13-463b-bcab-48415e59b570",
   "metadata": {},
   "source": [
    "We'll use data from the decision tree that Alex created to retry this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ac04b-e992-4db4-ae1c-263b6b4c6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = pd.read_csv('../Data/AuthScores.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536e770-756c-44ea-b6cc-681907757a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_w_scores = authentication_data\n",
    "auth_w_scores['Score'] = dt_scores['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0df96-1212-4874-a711-789d3912b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_score(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(auth_w_scores[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                score_sum = np.sum(data[data['UserName'] == user]['Score'])\n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures,score_sum]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures','score_sum'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7bd59-e8e7-4939-b5c5-aeff332c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_6(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_score(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > np.array(history.history[\"val_loss\"]).min())\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedf47d-ee0e-4495-81a0-3c131e750947",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_6 = pickle.load(open('Anomalies AE score.p','rb'))\n",
    "    val_loss_6 = pickle.load(open('Validation Loss score.p','rb'))\n",
    "    anomaly_bool_6 = []\n",
    "    anomalies_ae_6 = anomalies_ae_6.drop_duplicates()\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_6.append((len(anomalies_ae_6[anomalies_ae_6['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_6.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_6 = []\n",
    "    anomaly_bool_6 = []\n",
    "    val_loss_6 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_6(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_6.append(f)\n",
    "            anomaly_bool_6.append((len(f),rt_usernames[i]))\n",
    "            val_loss_6.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_6.append((0,rt_usernames[i]))\n",
    "            val_loss_6.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_6 = pd.concat(frames_6)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_6, open('Anomalies AE score.p','wb'))\n",
    "    pickle.dump(val_loss_6, open('Validation Loss score.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240185e-ba9c-4f2b-a100-d4dbe7346354",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_6,anomalies_ae_6,val_loss_6,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ffe00-9417-4ac0-83e9-d037eed203e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_n_6 = pickle.load(open('Non RT Anomalies score.p','rb'))\n",
    "    val_loss_n_6 = pickle.load(open('Non RT score.p','rb'))\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "    anomaly_bool_N_6 = []\n",
    "    anomalies_ae_n_6 = anomalies_ae_n_6.drop_duplicates()\n",
    "\n",
    "    for un in rand_non_rt:\n",
    "        try:\n",
    "            anomaly_bool_N_6.append((len(anomalies_ae_n_5[anomalies_ae_n_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_N_6.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_n_6 = []\n",
    "    anomaly_bool_N_6 = []\n",
    "    val_loss_n_6 = []\n",
    "    bound = 200\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i,un in enumerate(rand_non_rt):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "        f,b,val = ae_anomaly_finder_6(un,24)\n",
    "        if b == 1:\n",
    "            frames_n_6.append(f)\n",
    "            anomaly_bool_N_6.append((len(f),un))\n",
    "            val_loss_n_6.append((val,un))\n",
    "        else:\n",
    "            anomaly_bool_N_6.append((0,un))\n",
    "            val_loss_n_6.append((val,un))\n",
    "            pass\n",
    "    anomalies_ae_n_6 = pd.concat(frames_n_6)\n",
    "    pickle.dump(anomalies_ae_n_6,open('Non RT Anomalies score.p','wb'))\n",
    "    pickle.dump(val_loss_n_6,open('Non RT score.p','wb'))\n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3695c8-b968-4de9-b38c-ad386c29b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_N_6,anomalies_ae_n_6,val_loss_n_6,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7ecb8-0621-440c-b94f-e9a580ba052a",
   "metadata": {},
   "source": [
    "#### Bayes Poisson Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c1ed6-f7da-4b7d-bd37-d41f2f99d033",
   "metadata": {},
   "source": [
    "Based on a gamma prior for a poisson distribution, we can estimate $ \\lambda $ by $ \\frac{(\\sum{x_{i}} + \\alpha)}{(n + \\beta)}$. This is because under a $\\Gamma(\\alpha,\\beta)$ prior for $\\lambda$ we have that the posterior is $$ \\pi(\\lambda|x) \\propto \\lambda^{\\sum{x_{i}}+\\alpha-1} e^{-(n+\\beta)\\lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87b3bf-c577-4fd7-865a-0a6b42713347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(auth_w_scores[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203234-dcd3-4680-8ada-793b2d40ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_ae_detection(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for col in pois_df.columns:\n",
    "            dt = pois_df.iloc[:57*n][col]\n",
    "            bayes_mean.append((alpha+sum(dt))/(beta+len(dt)))\n",
    "            bayes_var.append((alpha+sum(dt))/(beta+len(dt))**2)\n",
    "\n",
    "        probabilities = stats.poisson.pmf(pois_df.iloc[57*n:82*n],bayes_mean)\n",
    "        \n",
    "        if comb == True:\n",
    "            for i in range(len(probabilities)):\n",
    "\n",
    "                prob = probabilities[i]\n",
    "\n",
    "                f_probs.append(stats.combine_pvalues(prob))\n",
    "            for i in range(len(f_probs)):\n",
    "\n",
    "                pv = f_probs[i][1]\n",
    "\n",
    "                if pv <= 0.05:\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "        elif comb == False:\n",
    "            for i in range(len(probabilities)):\n",
    "                \n",
    "                prob = probabilities[i]\n",
    "                \n",
    "                if any(prob <= 0.05):\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "    events = pd.concat(events_frames)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a770d1f-e3e5-4788-9ea4-46f06aa2c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_poiss_eval(anomaly_bools,anomalies,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e28ad3-b804-466a-aeb7-3a8f442f5d02",
   "metadata": {},
   "source": [
    "The commented code below was testing methods of combining probabilities based on: https://www.sciencedirect.com/science/article/pii/S0169207013001635 and https://link.springer.com/article/10.1007%2Fs11004-012-9396-3#citeas. Ultimately none of them were fruitful and only fishers method of combining produces meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5b103-a206-4bfd-91a3-0e1b882ff3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "\n",
    "#     prob = probabilities[i]\n",
    "\n",
    "#     f_probs.append(stats.combine_pvalues(prob))\n",
    "\n",
    "# print([prob for prob in f_probs if prob[1] <= 0.1])\n",
    "\n",
    "# geom_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     geom_probs.append(stats.mstats.gmean(prob))\n",
    "    \n",
    "# print([prob for prob in geom_probs if prob <= 0.1])\n",
    "\n",
    "# elop_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     numer = np.prod(prob**(1/len(prob)))\n",
    "#     denom = np.prod(prob**(1/len(prob))) + np.prod((1-prob)**(1/len(prob)))\n",
    "#     logit_probs.append(numer/denom)\n",
    "# print([prob for prob in elop_probs if prob <= 0.1])\n",
    "\n",
    "# beta_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     x = np.sum(prob*(1/len(prob)))\n",
    "#     beta_probs.append(stats.beta.cdf(x,a=1,b=1))\n",
    "    \n",
    "# print([prob for prob in beta_probs if prob <= 0.1])\n",
    "\n",
    "# agg_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     num = np.prod((prob/(1-prob))**1/len(prob))**0.1\n",
    "#     denom = 1+np.prod((prob/(1-prob))**1/len(prob))**0.1\n",
    "#     agg_probs.append(num/denom)\n",
    "    \n",
    "# print([prob for prob in agg_probs if prob <= 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899cf82-0733-4d24-a887-c1609ebc1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms = poiss_ae_detection(list(set(rt_usernames)),24,True)\n",
    "\n",
    "poisson_anoms = poisson_anoms.drop_duplicates()\n",
    "poiss_anom_bool = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms['UserName']):\n",
    "        poiss_anom_bool.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c5810-6f33-4bb1-9d15-1bf221a0473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool,poisson_anoms,list(set(rt_usernames)),'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed259ffa-7244-420d-bfcf-67c78bcdc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_nc = poiss_ae_detection(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_nc = poisson_anoms_nc.drop_duplicates()\n",
    "poiss_anom_bool_nc = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_nc['UserName']):\n",
    "        poiss_anom_bool_nc.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_nc.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3deed7-6d91-4513-a4c5-020962d5935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_anoms_nc[(poisson_anoms_nc['UserName'] == rt_usernames[11])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968efc09-fb0b-45b2-aadb-db61b1c21bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_nc,poisson_anoms_nc,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce88e3-de7e-43e3-8d10-eab5e89411fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "poisson_anoms_n = poiss_ae_detection(rand_non_rt,24,False)\n",
    "\n",
    "poisson_anoms_n = poisson_anoms_n.drop_duplicates()\n",
    "poiss_anom_bool_n = []\n",
    "\n",
    "for un in rand_non_rt:\n",
    "    if un in list(poisson_anoms_n['UserName']):\n",
    "        poiss_anom_bool_n.append((1,un))\n",
    "    else:  \n",
    "        poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debaa30d-d509-4c9b-8d1f-3d11fd157fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_n,poisson_anoms_n,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837496da-7ee0-4417-a1cb-4d0cbb1aea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhom_poiss_ae_detection(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for i in range(n):\n",
    "            bayes_mean_hr = []\n",
    "            bayes_var_hr = []\n",
    "            for col in pois_df.columns:\n",
    "                dt = list(pd.concat([pois_df.iloc[:57*n],pois_df.iloc[83*n:]])[col])\n",
    "                dt_hspl = dt[i::n]\n",
    "                bayes_mean_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl)))\n",
    "                bayes_var_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl))**2)\n",
    "\n",
    "            bayes_mean.append(bayes_mean_hr)\n",
    "            bayes_var.append(bayes_var_hr)\n",
    "\n",
    "        probabilities = []\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            probabilities.append(stats.poisson.pmf(pois_df.iloc[57*n:82*n].iloc[i::n],bayes_mean[i]))\n",
    "\n",
    "        if comb == True:\n",
    "            for i in range(n):\n",
    "    \n",
    "                probs = probabilities[i]\n",
    "\n",
    "                for j in range(len(probs)):\n",
    "\n",
    "                    hr_prob = probs[j]\n",
    "\n",
    "                    f_probs.append(stats.combine_pvalues(hr_prob))\n",
    "                    \n",
    "                for i in range(len(f_probs)):\n",
    "\n",
    "                    pv = f_probs[i][1]\n",
    "\n",
    "                    if pv <= 0.05:\n",
    "                        events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "        elif comb == False:\n",
    "            for i in range(n):\n",
    "    \n",
    "                probs = probabilities[i]\n",
    "\n",
    "                for j in range(len(probs)):\n",
    "\n",
    "                    hr_prob = probs[j]\n",
    "\n",
    "                    if np.any(hr_prob <= 0.05):\n",
    "                        \n",
    "                        events_frames.append(poiss_orig_finder(un,n,57*n+j*n+i))\n",
    "        \n",
    "    events = pd.concat(events_frames)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502305d0-7cfe-4c18-bdb1-1886e97cf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhom_poiss_ae_detection_2(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for i in range(n):\n",
    "            bayes_mean_hr = []\n",
    "            bayes_var_hr = []\n",
    "            for col in pois_df.columns:\n",
    "                dt = list(pd.concat([pois_df.iloc[:57*n],pois_df.iloc[83*n:]])[col])\n",
    "                dt_hspl = dt[i::n]\n",
    "                bayes_mean_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl)))\n",
    "                bayes_var_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl))**2)\n",
    "\n",
    "            bayes_mean.append(bayes_mean_hr)\n",
    "            bayes_var.append(bayes_var_hr)\n",
    "        \n",
    "        find_prob = []\n",
    "        for i in range(n):\n",
    "            find_prob.append(pd.DataFrame(stats.poisson.pmf(pd.concat([pois_df.iloc[:57*n], pois_df.iloc[83*n:]]).iloc[i::n],bayes_mean[i])).min())\n",
    "                \n",
    "        probabilities = []\n",
    "        \n",
    "        for i in range(n):\n",
    "\n",
    "            probabilities.append(stats.poisson.pmf(pois_df.iloc[57*n:82*n].iloc[i::n],bayes_mean[i]))\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            probs = probabilities[i]\n",
    "\n",
    "            for j in range(len(probs)):\n",
    "\n",
    "                hr_prob = probs[j]\n",
    "                \n",
    "                thresh = find_prob[i]\n",
    "                \n",
    "                if [item1 for item1,item2 in zip(hr_prob,thresh) if item1 <= item2] != []:\n",
    "\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+j*n+i))\n",
    "\n",
    "    events = pd.concat(events_frames)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f51d3-2035-4cc6-941e-9669954e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_inhom = inhom_poiss_ae_detection(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_inhom = poisson_anoms_inhom.drop_duplicates()\n",
    "poiss_anom_bool_inhom = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_inhom['UserName']):\n",
    "        poiss_anom_bool_inhom.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_inhom.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7735bf-c682-437e-837a-36f5da9e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_inhom,poisson_anoms_inhom,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333155ca-922c-4e0b-9aee-c4efa348d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "rand_non_rt_p = random.sample(non_rt_users,400)\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poiss_ae_detection(rand_non_rt_p,24,False)\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poisson_anoms_n.drop_duplicates()\n",
    "inhom_poiss_anom_bool_n = []\n",
    "\n",
    "for un in rand_non_rt_p:\n",
    "    if un in list(inhom_poisson_anoms_n['UserName']):\n",
    "        inhom_poiss_anom_bool_n.append((1,un))\n",
    "    else:\n",
    "        inhom_poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37548f39-5b14-40d5-9d9d-668372e7d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(inhom_poiss_anom_bool_n,inhom_poisson_anoms_n,rand_non_rt_p,'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1585d5-f58b-4063-89fc-1a83d580ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_inhom_2 = inhom_poiss_ae_detection_2(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_inhom_2 = poisson_anoms_inhom_2.drop_duplicates()\n",
    "poiss_anom_bool_inhom_2 = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_inhom_2['UserName']):\n",
    "        poiss_anom_bool_inhom_2.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_inhom_2.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01d4c2-4f5f-4fc8-a08f-5b4a36a6b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_inhom_2,poisson_anoms_inhom_2,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022a6dd-c2ee-41cc-be29-726e302591f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "rand_non_rt_p = random.sample(non_rt_users,400)\n",
    "\n",
    "inhom_poisson_anoms_n_2 = inhom_poiss_ae_detection_2(rand_non_rt_p,24,False)\n",
    "\n",
    "inhom_poisson_anoms_n_2 = inhom_poisson_anoms_n_2.drop_duplicates()\n",
    "inhom_poiss_anom_bool_n_2 = []\n",
    "\n",
    "for un in rand_non_rt_p:\n",
    "    if un in list(inhom_poisson_anoms_n_2['UserName']):\n",
    "        inhom_poiss_anom_bool_n_2.append((1,un))\n",
    "    else:\n",
    "        inhom_poiss_anom_bool_n_2.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1179a-7764-41ba-acf1-1b3cad3ddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(inhom_poiss_anom_bool_n_2,inhom_poisson_anoms_n_2,rand_non_rt_p,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65abd251-8f49-4412-97f5-809e6d3b1609",
   "metadata": {},
   "source": [
    "#### UASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49344d-c404-4b7c-8bb9-a5d83f2a3a1a",
   "metadata": {},
   "source": [
    "We need to rewrite the data frame creation - we need all positions to be the same for this to perform effectively so we create our own adjacency creation here. This allows us to ensure that each username and each destination device is in the same position in each dataframe but is unfortunately incredibly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94519e77-d9d3-4f0e-a06d-95171a042106",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    data_frame_list_uase = pickle.load(open('C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/Data Frame List UASE.p','rb'))\n",
    "    index_sparse = pickle.load(open('Index UASE.p','rb'))\n",
    "    columns_sparse = pickle.load(open('Columns UASE.p','rb'))\n",
    "    \n",
    "except:\n",
    "    clear_output()\n",
    "    print('Creating Data Frames.')\n",
    "    data_frame_list_uase = []\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    for i in tqdm(range(len(auth_start_days)-1)):\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(index = list(authentication_data['DstDevice'].unique()))\n",
    "\n",
    "        chunk = authentication_data[auth_start_days[i]:auth_start_days[i+1]]\n",
    "        data_un ={}\n",
    "        for user in authentication_data['UserName'].unique():\n",
    "            dstdevice_data = chunk[chunk['UserName'] == user].groupby('DstDevice').size()\n",
    "            data_un[user] = data_frame_ind.index.to_series().map(dstdevice_data.to_dict())\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(data=data_un,index = list(authentication_data['DstDevice'].unique()))\n",
    "        data_frame_ind = data_frame_ind.notnull().astype('int')\n",
    "        data_frame_ind = data_frame_ind.fillna(0)\n",
    "        A = np.array(data_frame_ind)\n",
    "        sA = sparse.csr_matrix(A)\n",
    "        data_frame_list_uase.append(sA)    \n",
    "\n",
    "    index_sparse = data_frame_ind.index\n",
    "    columns_sparse = data_frame_ind.columns\n",
    "    pickle.dump(data_frame_list_uase, open('Data Frame List UASE.p', 'wb'))\n",
    "    pickle.dump(index_sparse, open('Index UASE.p', 'wb'))\n",
    "    pickle.dump(columns_sparse, open('Columns UASE.p', 'wb'))\n",
    "    print(datetime.datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d19af3-3016-478e-960e-568fb8e9530e",
   "metadata": {},
   "source": [
    "We plot the first 100 sorted eignevalues to determine where to cut off our k using the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7256b-52c1-4372-ab9e-19a46d45e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "for i in tqdm(range(weeks)):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "    \n",
    "        #user_week_matrix = week_matrix[:, k::10]\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix,k=100)\n",
    "    plt.plot(-np.sort(-s))\n",
    "plt.xticks(np.arange(0, 101, 1.0))\n",
    "plt.axvline(5)\n",
    "plt.axvline(6)\n",
    "plt.axvline(7)\n",
    "plt.show()\n",
    "        #Y = (v.transpose()*s**1/2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a1906-4def-405f-bb63-9a887aba658a",
   "metadata": {},
   "source": [
    "So for all the matrices, somewhere around 6/7 eigenvalues we obtain an elbow. We'll use k=7 since this is still fast to compute but also allows us to capture a lot of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbcb34-a647-411d-a46f-2b17c332cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "Y_arrays = []\n",
    "\n",
    "for i in range(weeks):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix.asfptype(),k=7)\n",
    "    Y = pd.DataFrame((v.transpose()*s**0.5).transpose(),columns=(list(columns_sparse)*7))\n",
    "    Y_arrays.append(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab0a0-3dd1-4c45-852c-b03289c9e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count = len(columns_sparse)\n",
    "anom = []\n",
    "\n",
    "for i in tqdm(range(u_count)):\n",
    "    \n",
    "    normal_vals = list([] for x in range(7))\n",
    "    \n",
    "    for j in range(len(Y_arrays)-1):\n",
    "        if j <=7:\n",
    "            Y1 = Y_arrays[j].iloc[:, i::u_count]\n",
    "            for k in range(7):\n",
    "                vals_1 = Y1.iloc[:,k]\n",
    "                normal_vals[k].append(list(vals_1))\n",
    "                \n",
    "        else:\n",
    "            Y1 = Y_arrays[j].iloc[:, i::u_count]\n",
    "            p_vals = []\n",
    "            for k in range(7):\n",
    "                vals_1 = Y1.iloc[:,k]\n",
    "                vals_1 = [i for i in vals_1 if i != 0]\n",
    "                comparison = normal_vals[k]\n",
    "                comparison = [item for sublist in comparison for item in sublist]\n",
    "                comparison = [i for i in comparison if i != 0]\n",
    "                t,p = stats.ttest_ind(vals_1,comparison, equal_var=False)\n",
    "                p_vals.append(p)\n",
    "            p = scipy.stats.combine_pvalues(p_vals)\n",
    "            if p[1] <= 0.05:\n",
    "                anom.append((columns_sparse[i],j+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed898c-7c1f-4b1a-b95a-ffb1d81f4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=zip(*anom)\n",
    "len(pd.Series(a).unique()), len(list(set(a) & set(rt_usernames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dcab5-7705-4126-b87d-45be2de7a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_list_uase_2 = []\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for i in tqdm(range(len(auth_start_days)-1)):\n",
    "\n",
    "    data_frame_ind_2 = pd.DataFrame(index = list(authentication_data['DstDevice'].unique()))\n",
    "\n",
    "    chunk = authentication_data[auth_start_days[i]:auth_start_days[i+1]]\n",
    "    data_un_2 ={}\n",
    "    for user in rt_usernames:\n",
    "        dstdevice_data = chunk[chunk['UserName'] == user].groupby('DstDevice').size()\n",
    "        data_un_2[user] = data_frame_ind_2.index.to_series().map(dstdevice_data.to_dict())\n",
    "\n",
    "    data_frame_ind_2 = pd.DataFrame(data=data_un_2,index = list(authentication_data['DstDevice'].unique()))\n",
    "    data_frame_ind_2 = data_frame_ind_2.notnull().astype('int')\n",
    "    data_frame_ind_2 = data_frame_ind_2.fillna(0)\n",
    "    A = np.array(data_frame_ind_2)\n",
    "    sA = sparse.csr_matrix(A)\n",
    "    data_frame_list_uase_2.append(sA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7fa40-25d2-4daa-ab4f-dcf03fd052a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "Y_arrays_2 = []\n",
    "\n",
    "for i in range(weeks):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase_2[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "    \n",
    "        #user_week_matrix = week_matrix[:, k::10]\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix.asfptype(),k=7)\n",
    "    Y = pd.DataFrame((v.transpose()*s**0.5).transpose(),columns=(list(set(rt_usernames))*7))\n",
    "    Y_arrays_2.append(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb8a1-4e74-43f2-bad8-c5f96fa3f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_2 = []\n",
    "\n",
    "for i in tqdm(range(len(set(rt_usernames)))):\n",
    "    for j in range(len(Y_arrays_2)-1):\n",
    "        Y1 = Y_arrays_2[j].iloc[:, i::len(set(rt_usernames))]\n",
    "        Y2 = Y_arrays_2[j+1].iloc[:, i::len(set(rt_usernames))]\n",
    "        p_vals = []\n",
    "        for k in range(7):\n",
    "            vals_1 = Y1.iloc[:,k]\n",
    "            vals_2 = Y2.iloc[:,k]\n",
    "            if all(v == 0 for v in vals_1) or all(v == 0 for v in vals_2):\n",
    "                   p = 1\n",
    "            else:\n",
    "                t,p = stats.ttest_ind(vals_1,vals_2, equal_var=False)\n",
    "                p_vals.append(p)\n",
    "        p = scipy.stats.combine_pvalues(p_vals)\n",
    "        if p[1] <= 0.05:\n",
    "            anom_2.append((columns_sparse[i],j+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324a734-668e-499c-8345-49aaaad28a4b",
   "metadata": {},
   "source": [
    "unsuccessful implementation of anomalous outlier detection using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8beb66-c2cf-4e92-99a1-43888f8841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.cluster import OPTICS\n",
    "\n",
    "# normal_anomalous_usernames = []\n",
    "# for j in range(8):\n",
    "#     for i in tqdm(range(7)):\n",
    "#         #dbscan = DBSCAN(eps=0.025).fit((Y_arrays[j].iloc[0:2, :28815*(i+1)]).transpose())\n",
    "#         Y = (Y_arrays[j].groupby(Y_arrays[j].columns.values, axis=1).agg(lambda x: x.values.tolist()).sum().apply(pd.Series).T).transpose()\n",
    "#         dbscan = DBSCAN().fit(Y)\n",
    "#         #plt.figure()\n",
    "#         #plt.scatter(Y_arrays_2[0].loc[0][:95*(i+1)],Y_arrays_2[0].loc[1][:95*(i+1)],c=dbscan.labels_.astype(float))\n",
    "#         #plt.figure()\n",
    "#         for un in list(columns_sparse[np.where(dbscan.labels_ == -1)]):\n",
    "#             if un not in normal_anomalous_usernames:\n",
    "#                 normal_anomalous_usernames.append(un)\n",
    "\n",
    "# anomalous_usernames = []\n",
    "# for j in range(7,len(Y_arrays)):\n",
    "#      for i in range(7):\n",
    "#         #dbscan = DBSCAN(e=0.025).fit((Y_arrays[j].iloc[0:2, :28815*(i+1)]).transpose())\n",
    "#         dbscan = DBSCAN(eps=0.025).fit((Y_arrays[j].groupby(Y_arrays[j].columns.values, axis=1).agg(lambda x: x.values.tolist()).sum().apply(pd.Series).T).transpose())\n",
    "#         for un in list(columns_sparse[np.where(dbscan.labels_ == -1)]):\n",
    "#             if un not in normal_anomalous_usernames:\n",
    "#                 anomalous_usernames.append(un)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3eea06-23bb-41b9-8679-3578d6f8d35b",
   "metadata": {},
   "source": [
    "#### Anomaly Detection Using ASE principles - https://arxiv.org/pdf/2008.10055.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4e19e-be3a-49cb-850a-35d4a93127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASE_anom_finder(window_length):\n",
    "\n",
    "    vertex_norm_tracker = []\n",
    "    graph_norm_tracker = []\n",
    "\n",
    "    graph_mam_tracker = []\n",
    "    graph_range_tracker = []\n",
    "\n",
    "    vertex_mam_tracker = []\n",
    "    vertex_range_tracker = []\n",
    "\n",
    "    for t in tqdm(range(len(data_frame_list_uase)-1)):\n",
    "        u1, s1, v1 = scipy.sparse.linalg.svds(data_frame_list_uase[t],k=7)\n",
    "        u2, s2, v2 = scipy.sparse.linalg.svds(data_frame_list_uase[t+1],k=7)\n",
    "        Y1 = pd.DataFrame((v1.transpose()*s1**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "        Y2 = pd.DataFrame((v2.transpose()*s2**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "\n",
    "        nrm = scipy.linalg.norm(Y2-Y1,ord=2)\n",
    "        graph_norm_tracker.append(nrm)\n",
    "\n",
    "        vertex_norms = []\n",
    "        for v in list(columns_sparse):\n",
    "            nrm = scipy.linalg.norm(Y2[v]-Y1[v],ord=2)\n",
    "            vertex_norms.append(nrm)\n",
    "\n",
    "        vertex_norm_tracker.append(vertex_norms)\n",
    "\n",
    "        if t >= window_length:\n",
    "            graph_mam = np.sum(graph_norm_tracker[(t-window_length+1):t-1])/(window_length-1)\n",
    "            graph_range = np.sum(np.linalg.norm(np.array(graph_norm_tracker[t-window_length+2:t-1])-np.array(graph_norm_tracker[t-window_length+1:t-2])))/(1.128*(window_length-2))\n",
    "            graph_mam_tracker.append(graph_mam)\n",
    "            graph_range_tracker.append(graph_range)\n",
    "\n",
    "            vertex_mam_tracker_day = []\n",
    "            vertex_range_tracker_day = []\n",
    "\n",
    "            for j in range(len(list(columns_sparse))):\n",
    "                vertex_mam = np.sum([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_mam_tracker_day.append(vertex_mam)\n",
    "                vertex_range = np.std([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_range_tracker_day.append(vertex_range)\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker_day)\n",
    "            vertex_range_tracker.append(vertex_range_tracker_day)\n",
    "\n",
    "    day_anomalies = []\n",
    "\n",
    "    for i,v in enumerate(list(columns_sparse)):\n",
    "        y_v = np.array([vertex_norm_tracker[k][2] for k in range(len(vertex_norm_tracker))])\n",
    "        ucl_v = np.array([vertex_mam_tracker[k][2] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][0] for k in range(len(vertex_range_tracker))])\n",
    "        anomalies = [j+window_length for j,y in enumerate(y_v[window_length:]) if y > ucl_v[j]]\n",
    "        day_anomalies.append((v,anomalies))\n",
    "        \n",
    "    return vertex_norm_tracker, graph_norm_tracker, graph_mam_tracker, graph_range_tracker, vertex_mam_tracker, vertex_range_tracker, day_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b0207-a8ba-4f05-b69d-22bb9cbfb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_norm_tracker_7, graph_norm_tracker_7, graph_mam_tracker_7, graph_range_tracker_7, vertex_mam_tracker_7, vertex_range_tracker_7, day_anomalies_7 = ASE_anom_finder(7)\n",
    "vertex_norm_tracker_14, graph_norm_tracker_14, graph_mam_tracker_14, graph_range_tracker_14, vertex_mam_tracker_14, vertex_range_tracker_14, day_anomalies_14 = ASE_anom_finder(14)\n",
    "vertex_norm_tracker_3, graph_norm_tracker_3, graph_mam_tracker_3, graph_range_tracker_3, vertex_mam_tracker_3, vertex_range_tracker_3, day_anomalies_3 = ASE_anom_finder(3)\n",
    "vertex_norm_tracker_2, graph_norm_tracker_2, graph_mam_tracker_2, graph_range_tracker_2, vertex_mam_tracker_2, vertex_range_tracker_2, day_anomalies_2 = ASE_anom_finder(2)\n",
    "vertex_norm_tracker_5, graph_norm_tracker_5, graph_mam_tracker_5, graph_range_tracker_5, vertex_mam_tracker_5, vertex_range_tracker_5, day_anomalies_5 = ASE_anom_finder(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a7a7a-4195-4540-874a-1f4b713488f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_creation(window_length,vertex_mam_tracker,vertex_range_tracker,vertex_norm_tracker,u):\n",
    "    \n",
    "    plt.figure(figsize = (16,8))  \n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),[vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])\n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),np.array([vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][0] for k in range(len(vertex_range_tracker))]),'--')\n",
    "    plt.plot([vertex_norm_tracker[k][u] for k in range(len(vertex_norm_tracker))],'r.')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('$||{X_{i}^{(t+1)}-X_{i}^{(t)}}||$')\n",
    "    plt.title('Window Length: {}'.format(window_length))\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345ced8-5ad6-4f19-928a-85b879e9e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in columns_sparse:\n",
    "    b = i in list(rt_usernames)\n",
    "    a.append(b)\n",
    "idx = [i for i,x in enumerate(a) if x == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a44c78-ecdd-4dcd-97e9-8c39a979d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation(14,vertex_mam_tracker_14,vertex_range_tracker_14,vertex_norm_tracker_14,idx[0])\n",
    "plot_creation(7,vertex_mam_tracker_7,vertex_range_tracker_7,vertex_norm_tracker_7,idx[0])\n",
    "plot_creation(5,vertex_mam_tracker_5,vertex_range_tracker_5,vertex_norm_tracker_5,idx[0])\n",
    "plot_creation(3,vertex_mam_tracker_3,vertex_range_tracker_3,vertex_norm_tracker_3,idx[0])\n",
    "plot_creation(2,vertex_mam_tracker_2,vertex_range_tracker_2,vertex_norm_tracker_2,idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43a491-2503-4f09-beb0-6381dbd8738b",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b916f-b5ea-4c2a-9938-9f6354627061",
   "metadata": {},
   "source": [
    "We make the assumption that traffic from the red team usernames during the 57-82 day period is always anomalous whereas the traffic between any other time periods and non-red teeam uesrnames is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d1d2c-b9a4-4acc-905d-e99c30b4cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_supervised(user,n,anom):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "            \n",
    "            hour = j\n",
    "            data = chunks[j]\n",
    "            \n",
    "            if 57 <= i+1 <= 82 and anom=True:\n",
    "                anombool = 1\n",
    "            else:\n",
    "                anombool = 0\n",
    "\n",
    "            authents = len(data[data['UserName'] == user])\n",
    "            failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "            srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "            dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "            uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "            feat_dict[i*n + j] = [user,srcunique,dstunique,authents,failures,anombool]\n",
    "            #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['UserName','SrcUnique','DstUnique','Authentications','Failures','Anomaly'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38449b-4006-4b7e-9afb-ef0c91441b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_sample = list(pd.Series(rt_usernames).unique()) + random.sample(non_rt_users,400)\n",
    "data = []\n",
    "for user in tqdm(username_sample):\n",
    "    if user in rt_usernames:\n",
    "        anom = 1\n",
    "    else:\n",
    "        anom = 0\n",
    "    df = feature_generation_supervised(user,24,anom)\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00281f0-0d06-46cc-8836-a4578d77bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_sample = list(pd.Series(rt_usernames).unique()) + non_rt_sample\n",
    "data = []\n",
    "for user in tqdm(username_sample):\n",
    "    if user in rt_usernames:\n",
    "        anom = 1\n",
    "    else:\n",
    "        anom = 0\n",
    "    df = feature_generation_supervised(user,24,anom)\n",
    "    data.append(df)\n",
    "nn_data = pd.concat(data)\n",
    "nn_data.to_csv('nn_supervised data.gz', compression='gzip')\n",
    "X_train, X_test, y_train, y_test = train_test_split(nn_data[['UserName','SrcUnique','DstUnique','Authentications','Failures']], nn_data['Anomaly'], test_size=0.1, random_state=42)\n",
    "X_train_un_list = list(X_train['UserName'])\n",
    "X_test_un_list = list(X_test['UserName'])\n",
    "X_train = X_train.drop('UserName',axis=1)\n",
    "X_test = X_test.drop('UserName',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d67597-dfe5-4ef5-9151-01d7ce3ad4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_anomaly_finder(x_train,x_test,y_train,y_test,train_user,test_user,n,BATCH_SIZE=256, EPOCHS=1000):\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train)\n",
    "    x_train = pipeline.transform(x_train)\n",
    "    x_test = pipeline.transform(x_test)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the nn\n",
    "    neural_network =Sequential([\n",
    "\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    neural_network.compile(optimizer=\"adam\", \n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = neural_network.fit(\n",
    "        x_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = neural_network.predict(x_test)\n",
    "    Y_pred = x_test_pred.argmax(axis=1)\n",
    "    test_loss, test_acc = neural_network.evaluate(X_test, y_test)\n",
    "    print(test_acc)\n",
    "    \n",
    "    anomalies = x_test[np.where(x_test_pred == 1)]\n",
    "    #anomaly_idx = anomalies[0]\n",
    "    \n",
    "    #frame = []\n",
    "    #for i in range(len(anomaly_idx)):\n",
    "    #    user = test_user[anomaly_idx[i]]\n",
    "    #    anomaly = nn_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "    #    if len(anomaly) == 0:\n",
    "    #        pass\n",
    "    #    else:\n",
    "    #        frame.append(anomaly)\n",
    "    #    \n",
    "    #if len(frame) != 0:\n",
    "    #    anomaly_df = pd.concat(frame)\n",
    "    #    return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    #else:\n",
    "    #    print('No anomalies found.')\n",
    "    #    return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4790614-d2ec-4c67-8ea2-6b6730873534",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nn_anomaly_finder(X_train,X_test,y_train,y_test,X_train_un_list,X_test_un_list,24,BATCH_SIZE=256, EPOCHS=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5732c-495b-4210-8490-2a326e871bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_pred).groupby(0).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
