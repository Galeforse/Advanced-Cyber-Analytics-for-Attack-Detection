{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8416ea61-e02e-4905-bf6b-8d96807140e2",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8039-eee4-484e-838e-3f8ec42ca268",
   "metadata": {},
   "source": [
    "In this notebook we finally perform our anomaly detection. We perform this in 6 steps:\n",
    "1. Create a data frame for each individual username showing authentication types for 8/24 hours of each day\n",
    "2. Use EDA, Isolation Forest's, Local Outlier Factor and other models to find 'normal' days or 'normal' usernames to allow us to train th CP_APR model\n",
    "3. Train the CP_APR model with the data we've identified in step 2\n",
    "4. Run the trained CP_APR model on the other data to identify anomalies in the 'test' data\n",
    "5. Use a function to return the anomalous entry from the original data frame based on the output of the CP_APR function\n",
    "6. Create a new data frame of anomalies\n",
    "\n",
    "Finally, we may verify this process through other means such as HTM studio for a subset or other anomaly detection techniques. We may also use the original red team authentication data to determine whether the events given there were picked up by the CP_APR method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee7621-65db-4fee-8767-ff6fced6f973",
   "metadata": {},
   "source": [
    "First we import our libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324bf39e-193d-443f-b8aa-f7d38b0ce455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCP_APR import CP_APR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import os.path\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import bz2\n",
    "import random\n",
    "import regex as re\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bac645-9484-4140-8cd1-fddaad31862d",
   "metadata": {},
   "source": [
    "We want to create reproducibility for our neural networks and doing the following permits this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1ab95c-a310-4eca-aecb-217a3145c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b403bc-8bf1-4fb1-ada6-bedb07590004",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb2728-63b8-46bb-bfca-d8199e11374c",
   "metadata": {},
   "source": [
    "Now we import the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e567a1-389c-4125-a73d-8f100841418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Attempting to read entire data set.')\n",
    "    authentication_data = pd.read_csv('../Data/Authentication data.gz', compression='gzip', index_col = 0)\n",
    "    process_data = pd.read_csv('../Data/Process data.gz', compression='gzip', index_col = 0)\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Unable to read entire data set, reading from original files.')\n",
    "    rootdir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls'\n",
    "    unzippeddir = 'C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/LANL/ATI Data/Summaries/wls/Unzipped'\n",
    "    frames = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if file[-3:] == '.gz':\n",
    "                filedir = rootdir + '/' + file\n",
    "                with gzip.open(filedir) as f:\n",
    "                    df = pd.read_csv(filedir, header=None)\n",
    "                    frames.append(df)\n",
    "                if 'authentications' in str(file):\n",
    "                    count = count + len(df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    authentication_data = df[:count]\n",
    "    authentication_data.columns = ['UserName', 'SrcDevice','DstDevice', 'Authent Type', 'Failure', 'DailyCount']\n",
    "\n",
    "    process_data = df[count:]\n",
    "    process_data = process_data[[0,1,2,3,4]]\n",
    "    process_data.columns = ['UserName', 'Device', 'ProcessName', 'ParentProcessName', 'DailyCount']\n",
    "\n",
    "    authentication_data.to_csv('../Data/Authentication data.gz', header=True, compression='gzip')\n",
    "    process_data.to_csv('../Data/Process data.gz', header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd6d06-a9fa-4db5-99c2-426ca02a0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication_data[authentication_data['UserName'] == 'User035855']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbb5e1-95c0-42de-81db-224622d790d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication_data['Authent Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b2ae0-becb-4d45-b680-eb5867a34fe0",
   "metadata": {},
   "source": [
    "### Other required data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2c11b-bca3-4c47-8de9-4134a94e2cf5",
   "metadata": {},
   "source": [
    "#### Possible Username Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3966cd5-bf94-4301-ad8d-79b81c856bea",
   "metadata": {},
   "source": [
    "We need a list of usernames we'll consider for training/testing. Currently at the beginning of all this we will consider all usernames for both training and testing and reduce this as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df6b7a-8fc0-4814-98bb-05227271c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = list(authentication_data['UserName'].unique())\n",
    "test_users = list(authentication_data['UserName'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac116b2-e39f-4e4c-bb81-dd503c0c3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e2983-bb66-4416-8535-5799ba253b0e",
   "metadata": {},
   "source": [
    "#### Authentication Red Team Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2593a077-e344-4498-b626-c27d6499c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_usernames = list(pd.read_csv('../Data/AuthUserNames.txt', header=None)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed820446-d320-4774-bc7b-8ebb0b025420",
   "metadata": {},
   "source": [
    "#### Authentication Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc959f95-cfef-4504-8e99-12ba9e80930c",
   "metadata": {},
   "source": [
    "We'll need a dictionary of authentication types for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6b3b6-607b-452a-9627-7911413bee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_t = list(authentication_data['Authent Type'].unique())\n",
    "AT_dict = { i : a_t[i] for i in range(0, len(a_t) ) }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb28cc-fe6c-45e5-8e2f-d32a450dad54",
   "metadata": {},
   "source": [
    "#### Authentication Day Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc7edb-5462-4328-aadb-9ff165d08a4a",
   "metadata": {},
   "source": [
    "The below code defines the indices where each day begins in the authentiation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244ce39-dffe-43f3-bb9e-30d2883e29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_index_list = authentication_data.index.tolist()\n",
    "auth_start_days = [i for i, e in enumerate(auth_index_list) if e == 0]\n",
    "auth_start_days.append(len(authentication_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c7cea-a0d2-4a09-a9a3-e4a4a1d04f26",
   "metadata": {},
   "source": [
    "### Step 1: DataFrame Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68659e-83da-4900-8f06-658cb8729480",
   "metadata": {},
   "source": [
    "This first function is used to split a data frame into equal chunks. Since we need to split each day into 8/24 hours we use this function to split into equal time periods - this may not be perfectly representitive of the actual hour split but should be a good estimate since we don't have the original time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd7347-8165-4a1a-8d48-cfef11033d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df,n):\n",
    "    chunks = list()\n",
    "    chunk_size = int(np.round(df.shape[0]/n))\n",
    "    num_chunks = n\n",
    "    for i in range(num_chunks):\n",
    "        if i != num_chunks-1:\n",
    "            chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "        else:\n",
    "            chunks.append(df[i*chunk_size:])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e87e56-af50-43cf-90d8-94a4047db507",
   "metadata": {},
   "source": [
    "This function creates the required data frames. It takes as input a username and a split by number (8/24) and returns a data frame of the user's authentiation events split by type over 90 days, split by 8/24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb198f-6cbe-4c06-874d-86394c9218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_type_un_df(user,n):\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    n = n\n",
    "    auth_type_dict = {}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                data = chunks[j]\n",
    "                auth_type_data = data[data['UserName'] == user].groupby('Authent Type')['DailyCount'].sum()\n",
    "                auth_type_dict[i*n + j] = auth_type_df.index.to_series().map(auth_type_data.to_dict())\n",
    "    \n",
    "    auth_type_df = pd.DataFrame(data=auth_type_dict,index = list(authentication_data['Authent Type'].unique()))\n",
    "    auth_type_df = auth_type_df.transpose()\n",
    "    auth_type_df = auth_type_df.fillna(0)\n",
    "    \n",
    "    return auth_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9f5ca-df35-4972-a2c6-ca78b6e29283",
   "metadata": {},
   "source": [
    "This function creates the inputs for our CP_APR model. We pass a list of usernames to the function and it returns the set of co-ordinate tuples (i,j,e) where we have non-zero entries in our data matrices, along with the corresponding values for that matrix. i is the row of the matrix i.e. time, j is the column i.e. authentication type and e is the username number. We can instead pass a single username which would return this for just one user but this is optimised to run for all users when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ac08a-4102-4d22-92ca-2e8bff478603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sparse_df(usernamelist,n):\n",
    "    \n",
    "#     coords = []\n",
    "#     vals_list = []\n",
    "    \n",
    "#     for e,user in enumerate(usernamelist):\n",
    "#         df = auth_type_un_df(user,n)\n",
    "    \n",
    "#         s = sparse.coo_matrix(df)\n",
    "#         co = [[s.row[i],s.col[i],e] for i in range(len(s.row))]\n",
    "#         vals = s.data\n",
    "        \n",
    "#         coords.append(co)\n",
    "#         vals_list.append(vals)\n",
    "    \n",
    "#     coords = np.array([item for sublist in coords for item in sublist])\n",
    "#     vals_list = np.array([item for sublist in vals_list for item in sublist])\n",
    "    \n",
    "#     return vals_list, coords\n",
    "\n",
    "# the function above does this for a list of usernames - doesn't work atm so needs fixing but the below does what we want for a single username\n",
    "def sparse_df(username,n):\n",
    "    \n",
    "    df = auth_type_un_df(username,n)\n",
    "\n",
    "    s = sparse.coo_matrix(df)\n",
    "    co = [[s.row[i],s.col[i],1] for i in range(len(s.row))]\n",
    "    vals = s.data\n",
    "    \n",
    "    return vals, co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb064a5-0c91-421f-87aa-aff2ae8142e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stime = datetime.datetime.now()\n",
    "\n",
    "# n=10\n",
    "# for i in range(n):\n",
    "#     vals,co = sparse_df(train_users[i],24)\n",
    "\n",
    "# etime = datetime.datetime.now()\n",
    "\n",
    "# print('Time taken for {} iterations: {}.'.format(n,etime-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265acb9-9286-41bc-9454-c5a8eb557129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stime = datetime.datetime.now()\n",
    "\n",
    "# n=10\n",
    "# for i in range(n):\n",
    "#     vals,co = sparse_df(train_users[i],24)\n",
    "\n",
    "# etime = datetime.datetime.now()\n",
    "\n",
    "# print('Time taken for {} iterations: {}.'.format(n,etime-stime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10ecba-3a94-4048-af4a-18f6c7ce59d8",
   "metadata": {},
   "source": [
    "So we take roughly 4 seconds to compute a single username - doing this for 28,815 usernames would take $28,815 * \\frac{4}{86400} = 1.3 $ days. Lets rewrite this with parallelisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0a1b7-ba9e-4f8a-82ab-29f40bfdbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i SparseDataFrameCreation.py 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d26457-1512-4dc9-a015-7112e9ca61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i SparseDataFrameCreation.py 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb8779-e061-4608-a82f-e523b04a7537",
   "metadata": {},
   "source": [
    "We see speed improvements for large numbers of usernames but not for small ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfc727-2a73-4090-b22c-884434e3f696",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb7cb5-eb60-491f-be44-ee341632ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of a full matrix of the data we're considering called on a single username over 24 hours\n",
    "#auth_type_un_df(train_users[0],24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfd98b-0772-4f95-9e5b-7e816c1451d7",
   "metadata": {},
   "source": [
    "From the above we see that we obtain a data set of each day split into 24 hours. Each column represents an authentication type and non-zero entries represent an authentication event in that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e30917-564c-4a65-8a0f-de953d22e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example of the sparse matrix representation we'll pass to the model - creates a big list so is commented but feel free to uncomment\n",
    "# sparse_df(train_users[1000],24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d342d13-c398-4787-85c8-d31a4dd99f52",
   "metadata": {},
   "source": [
    "The first list is the non-zero values in the matrix and the second list is the list of co-ordinates where those non-zero values occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bed4f4-dc9c-4106-87c9-9dcf3fa129a1",
   "metadata": {},
   "source": [
    "### Step 2: Determining Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66026c-b492-4cdf-a434-73066e6f4ea8",
   "metadata": {},
   "source": [
    "Days 1-56, and 83-90 are normal activity and therefore are training data whereas days 57-82 contain red team data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b585c-0e92-4f3c-8b6c-dcfbb252d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_iso_lof(data,plot=False,c='auto'):\n",
    "    \n",
    "    # scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data))\n",
    "    \n",
    "    # isolation forest predictions\n",
    "    if_model = IsolationForest(contamination=c)\n",
    "    if_predictions = if_model.fit_predict(data)\n",
    "    \n",
    "    # local outlier factor predictions\n",
    "    lof = LocalOutlierFactor(n_neighbors=2)\n",
    "    lof_predictions = lof.fit_predict(data)\n",
    "    \n",
    "    if plot == True:\n",
    "        \n",
    "        # PCA reduction for plotting\n",
    "        pca = PCA(n_components=2)\n",
    "        auth_types_pca = pd.DataFrame(pca.fit_transform(data))\n",
    "        \n",
    "        # finding anomaly locations\n",
    "        a_if = auth_types_pcapca.loc[if_predictions == -1]\n",
    "        a_lof = auth_types_pca.loc[lof_predictions == -1]\n",
    "        \n",
    "        anomalies = auth_types_pca.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(20,6))\n",
    "        ax.plot(auth_types_pca[0], auth_types_pca[1], color='black', label='Normal')\n",
    "        ax.scatter(anomalies[0], anomalies[1], color='red', label='Anomaly')\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Number of Events\")\n",
    "        ax.text(0,auth_types_pca[1].max()-0.1,('Number of combined anomalies found: {}. \\n Number of LOF anomalies found: {}. \\n Number of IF anomalies found: {}.'.format(len(anomalies), len(a_lof), len(a_if))))\n",
    "        plt.legend(loc=1)\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        a_if = data.loc[if_predictions == -1]\n",
    "        a_lof = data.loc[lof_predictions == -1]\n",
    "\n",
    "        anomalies = data.loc[list(set(a_lof.index) & set(a_if.index))]\n",
    "    \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bfece-4400-488d-9e05-c3e3c14754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run -i ParallelisedTrainingData.py 22815"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548ab1d-922e-438f-8bfd-2bfc013b59f8",
   "metadata": {},
   "source": [
    "### Step 3: Train the CP_APR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beacb46-1e6f-4d71-8cc4-b47bbdf11fc7",
   "metadata": {},
   "source": [
    "Here we define our CP_APR model. We then train it on the data we have determined to be 'normal' above to teach the model what is likely to be normal activity in the authentication sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522919e-1210-47f7-af89-8d2fe19d6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_apr = CP_APR(n_iters=10, random_state=42, verbose=200, method='numpy', return_type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b7f32-7771-483b-890e-2153b8ca0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#factors = cp_apr.fit(coords=train_coords, values=train_vals)\n",
    "#factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb2eb9-af37-46fa-a4a2-11cffc038eb9",
   "metadata": {},
   "source": [
    "### Step 4: Apply the CP_APR model to the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf3137-049f-4aed-bba3-675b8ff7d34d",
   "metadata": {},
   "source": [
    "Here we apply the model to the data we want to find anomalies in. This data will then be used to find the final set of anomalies to pass into the final stage of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36cb1d-9ba4-4d4d-8bce-dc8311807add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_values = cp_apr.predict_scores(coords=test_coords, values=test_vals)\n",
    "#p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c6823-c17a-4e25-9112-ce6f6da2c890",
   "metadata": {},
   "source": [
    "### Step 5: Obtain the data frame of anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cf321-c5f0-4a62-b80f-d43df88c7848",
   "metadata": {},
   "source": [
    "Here we use the p-values found above to retrieve the final set of anomalies from the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b583dd-abf8-4f3c-9fb7-7ce3908715d4",
   "metadata": {},
   "source": [
    "This function returns a single anomaly based on the test coordinates array we obtain i.e. the actual data we look for anomalies in, the entry value i.e. the position of the anomaly in the array output by our CP_APR model and n, the number of hours we split the data frame by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad662e-8ad8-4a37-846f-a710f867651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orig_finder(test_coords, entry_val, n):\n",
    "    \n",
    "    # gets the co-ordinates of the entry where we have the erro\n",
    "    orig_co = test_coords[entry_val]\n",
    "    \n",
    "    # gets the authentication type\n",
    "    authent = AT_dict[orig_co[1]]\n",
    "    \n",
    "    # gets the username of the individual who the anomaly occured with\n",
    "    username = test_users[orig_co[2]]\n",
    "    \n",
    "    # gets the day the anomaly occured (n is the number of hours we split the data frame into)\n",
    "    day = int(orig_co[0]/n)\n",
    "    \n",
    "    # gets the hour the anomaly occured in\n",
    "    hour = orig_co[0] - n * day\n",
    "    \n",
    "    # gets the n hour chunks for that day\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[day]:auth_start_days[day+1]],n)\n",
    "    \n",
    "    # gets the hour\n",
    "    data = chunks[hour]\n",
    "    \n",
    "    # finds the anomaly\n",
    "    anom = data[(data['UserName'] == username) & (data['Authent Type'] == authent)]\n",
    "    \n",
    "    return anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af207da-84f4-45d6-a09e-0d6241f386f7",
   "metadata": {},
   "source": [
    "The p-values array defined below will be the output of the CP_APR function. We then set a threshold for anomaly scores to determine what we will class as an anomaly. Using the np.where function we will find all instances where we are below the threshold and return a data frame of the anomalies that we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f567bd-70ab-4905-b8d1-06e355af5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames = []\n",
    "#threshold = 0.05\n",
    "\n",
    "#for i in range(len(np.where(p_values < threshold)[0])):\n",
    "#    entry = np.where(p_values < threshold)[0][i]\n",
    "#    anom = orig_finder(test_coords, entry_val, 24)\n",
    "#    frames.append(anom)\n",
    "    \n",
    "#anomalies = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ec7bc-9f7f-4351-9f0b-4c393e16ccec",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda737e-fe26-4e57-979a-7032b1efb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we find the first entry of the first day, where the Authentication type is TGS, for the 1000th user in the list of test_users\n",
    "orig_finder([[0,0,1000]],0,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292f89e-ca04-45a9-968b-2a3db3e4fdc0",
   "metadata": {},
   "source": [
    "Dan's Notes:\n",
    "- Possibly hard to work with a single machine since it may change role\n",
    "- Consider with computers, both source and destination computers\n",
    "- Natural extension is to throw more information at the prediction - features et\n",
    "- Plot p-values distribution (assume uniform) q-q plot (should see a gap and then you can set a threshold on them)\n",
    "- Counts over time, days of the week structure\n",
    "- Dismiss Saturdays/Sundays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a314449-486e-42f4-818d-c1de1d2e88d6",
   "metadata": {},
   "source": [
    "### Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd4f4b-9ddd-49ee-91e7-649d619e9a52",
   "metadata": {},
   "source": [
    "#### Convolutional Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8b124-2e06-4928-8e20-a9f781d140e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cae_feature_generation(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65e0f5-4d85-49d6-9dc8-7b6a0e45b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentication type data frames - didn't seem to work particularly well - sparse data structures arent particularly successful in neural networks\n",
    "#df = auth_type_un_df(rt_usernames[0],24)\n",
    "#df = df.drop('ScreensaverDismissed',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01c814-9097-4cec-afc0-a37bd3c1a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cae_feature_generation(rt_usernames[0],24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b2402-8283-4fc0-8653-8df5027bb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 4\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df[:24*57])\n",
    "x_test = create_sequences(df[24*57:])\n",
    "x_train = np.pad(x_train,[(0,0),(0,0),(0,1)], constant_values=0)\n",
    "x_test = np.pad(x_test,[(0,0),(0,0),(0,1)], constant_values=0)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8965089-e81e-4dac-a49c-474c3318c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_l = layers.Input(shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\",  activation=\"relu\")(input_l)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(filters=16, kernel_size=7, padding=\"same\",  activation=\"relu\")(x)\n",
    "x = layers.Dropout(rate=0.2)(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\")(encoded)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=7, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "convo_autoencoder = Model(input_l, decoded)\n",
    "convo_autoencoder.compile(metrics=['accuracy'], optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43484fe0-f386-4d65-9b9b-2ce0d42f732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8adc6-5561-489c-b3b1-65388ee7a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = convo_autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "        TqdmCallback(verbose=1)\n",
    "    ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c50a9-3dcd-44cf-8ec2-420b2a3fbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd1271-d8c3-4cef-b6b0-ac430bf1665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = convo_autoencoder.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred.reshape(x_train_pred.shape[0],x_train_pred.shape[1],x_train_pred.shape[2]) - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3d448-bf25-4c97-974c-e3e7804beaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(train_mae_loss,99)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e38511-372d-4a57-9b4b-7036861ebff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred = convo_autoencoder.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred.reshape(x_test_pred.shape[0],x_test_pred.shape[1],x_test_pred.shape[2]) - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87203328-51c9-41f6-9e01-c51a2e02b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff986e19-2804-4681-bcee-a63cf8aae146",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd686e6e-a49d-4310-87ed-8223681ffbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_data_indices = []\n",
    "for idx in range(TIME_STEPS - 1, len(df) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[idx - TIME_STEPS + 1 : idx]):\n",
    "        anomalous_data_indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778efcfb-ec6c-4ff8-b4aa-d3fcd58f759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies = df.iloc[pd.Series(anomalous_data_indices).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92264a57-49ca-47d3-bcd1-de5e9c9c66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebdb50-fa77-4b85-abba-6610399edbc3",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edb78a-724b-4dd3-94ea-b8716c087a88",
   "metadata": {},
   "source": [
    "Here we produce an auto encoder. This is semi-supervised learning since the NN is attempting to approximate the input data and then using mean squared error to determine the error in the prediction. High error indicates an anomalous entry since we couldn't predict this well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31ca41-87dd-4c00-9044-6d7075239cae",
   "metadata": {},
   "source": [
    "The first set of cells is a proof of concept allowing us to visualise the neural network before we define a full anomaly detecction function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2e4b8-9cac-43b1-adf8-46d60eef914b",
   "metadata": {},
   "source": [
    "We first get our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e524b6-d2e8-4b9f-8fa5-eb68099d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296c65a-aef2-459f-a04c-43c126d25dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_generation(rt_usernames[0],24)\n",
    "x_train_ae = np.array(df[0:57*24])\n",
    "x_test_ae = np.array(df[57*24:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5bd74-c90e-4120-a46c-df7bfde9d8ef",
   "metadata": {},
   "source": [
    "We define a pipeline to rescale and normalize our data. This means the data will be optimised for our neural network since large values will be scaled down and small values will be scaled up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a672438-6d18-4ac5-8176-e795b7dfa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309ea99-d8df-46e0-97e5-0346fd935c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train_ae)\n",
    "x_train_ae = pipeline.transform(x_train_ae)\n",
    "x_test_ae = pipeline.transform(x_test_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2fe4f-e245-4b80-876e-d24e9bb1074b",
   "metadata": {},
   "source": [
    "We define our autoencoder. The architecture is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d682af-33af-421d-86dd-6edbe5ad03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train_ae.shape[1]\n",
    "BATCH_SIZE = 120\n",
    "EPOCHS = 50\n",
    "\n",
    "autoencoder =Sequential([\n",
    "\n",
    "    # deconstruct / encode\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "    # reconstruction / decode\n",
    "    tf.keras.layers.Dense(2, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(4, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(8, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(16, activation='elu'),\n",
    "    tf.keras.layers.Dropout(rate=0.1),\n",
    "    tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "])\n",
    "\n",
    "autoencoder.compile(optimizer=\"adam\", \n",
    "                    loss=\"mse\",\n",
    "                    metrics=[\"acc\"])\n",
    "\n",
    "\n",
    "autoencoder.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e164584-1fac-4c63-b224-1587cbd01025",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    x_train_ae, x_train_ae,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=cb   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c32a94-aef6-4e4e-8388-86c7969f8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e70033-f21e-45a4-9c8a-a686f28ea3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = autoencoder.predict(x_train_ae)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train_ae), axis=1)\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "train_mae_loss = mmscaler.fit_transform(train_mae_loss.reshape(-1,1))\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc614ea6-28dc-4da7-adc0-7e9f5a0788ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pred = autoencoder.predict(x_test_ae)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ac353-ea13-47b0-807b-995fcaed41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler()\n",
    "test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e79e9-d7c2-4203-a983-5399dbfcc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_mae_loss > 0.95\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cfb14-1faa-4af3-a2c1-273b8e87a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies = df.iloc[pd.Series(np.where(anomalies)[0]).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cff01-a745-44ee-9f37-69cc68b92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029f5f1-bab1-4d95-918a-c110474a7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(authentication_data[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d38bda-9cc3-4bb6-851c-47a8cf4daf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_orig_finder(rt_usernames[0],24,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95537a5b-f55b-471b-9256-9eeb7c7fc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', StandardScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2653e-4e1c-49df-abab-adcaebf6fde4",
   "metadata": {},
   "source": [
    "This cell is just a quick test, as well as determining timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c5e1f-273e-4be7-b39d-a302f0eac371",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "anomalies = ae_anomaly_finder(rt_usernames[0],24)\n",
    "e_time=datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548abf7-72f3-443e-8d2d-b59a939b981f",
   "metadata": {},
   "source": [
    "In this cell we run the model over all the red team usernames to identify the anomalies in the defined period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0ba80-bb13-491b-b0cc-75fd568c30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae = pickle.load(open('Anomalies AE.p','rb'))\n",
    "    val_loss = pickle.load(open('Validation Loss.p','rb'))\n",
    "    anomaly_bool = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool.append((len(anomalies_ae[anomalies_ae['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames = []\n",
    "    anomaly_bool = []\n",
    "    val_loss = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames.append(f)\n",
    "            anomaly_bool.append((len(f),rt_usernames[i]))\n",
    "            val_loss.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool.append((0,rt_usernames[i]))\n",
    "            val_loss.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae = pd.concat(frames)\n",
    "    \n",
    "    pickle.dump(anomalies_ae, open('Anomalies AE.p','wb'))\n",
    "    pickle.dump(val_loss, open('Validation Loss.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013eb7b-cca2-4a05-bfe5-0ab9d9aafbab",
   "metadata": {},
   "source": [
    "Here we analyse the output of our auto encoder and review the amount of anomalies identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23106732-16db-4bf9-92ba-66fff31d6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom = []\n",
    "for i in range(len(anomaly_bool)):\n",
    "    if anomaly_bool[i][0] == 0:    \n",
    "        non_anom.append(anomaly_bool[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool]) > 0)[0]),len(anomaly_bool)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0f188-54cf-4a32-89d6-5d1dd269a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rt_users = [un for un in authentication_data['UserName'].unique() if un not in rt_usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a857e-84db-4963-8a0f-235698664c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_n = []\n",
    "anomaly_bool_N = []\n",
    "val_loss_n = []\n",
    "bound = 20\n",
    "rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "s_time = datetime.datetime.now()\n",
    "\n",
    "for i,un in enumerate(rand_non_rt):\n",
    "    clear_output(wait=True)\n",
    "    print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "    f,b,val = ae_anomaly_finder(un,24)\n",
    "    if b == 1:\n",
    "        frames_n.append(f)\n",
    "        anomaly_bool_N.append((len(f),un))\n",
    "        val_loss_n.append(val)\n",
    "    else:\n",
    "        anomaly_bool_N.append((0,un))\n",
    "        val_loss_n.append(val)\n",
    "        pass\n",
    "anomalies_ae_n = pd.concat(frames_n)\n",
    "\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddd5f5-1372-465d-8ccf-3a977df89072",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_n = []\n",
    "for i in range(len(anomaly_bool_N)):\n",
    "    if anomaly_bool_N[i][0] == 0:    \n",
    "        non_anom_n.append(anomaly_bool_N[i][1])\n",
    "        \n",
    "print('{} of the non red team usernames analysed were identified to have anomalies out of {} non red team usernames analysed. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_N]) > 0)[0]),len(anomaly_bool_N)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1805581-d900-4b5a-9766-ce21b17ca047",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea2131-5971-40ce-9dd0-efed635d6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae_n)/len(authentication_data[authentication_data['UserName'].isin(rand_non_rt)])\n",
    "print('{:.2f}% of the \"normal\" data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeb233-9800-4d5f-8670-8c2a2fbcc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f59bb-44e9-4075-a45e-4f510ec32dba",
   "metadata": {},
   "source": [
    "Analysing validation losses across both red team and non-red team usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15fa17-2f2d-466a-856f-9b7b6be3c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = []\n",
    "for i in range(len(val_loss)):\n",
    "    final_loss.append(val_loss[i][0][len(val_loss[i][0])-1])\n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(final_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd220a9-d00b-40fa-9349-b64b96bfd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_n = []\n",
    "for i in range(len(val_loss_n)):\n",
    "    final_loss_n.append(val_loss_n[i][len(val_loss_n[i])-1])\n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(final_loss_n)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d5c06-d9ed-4b23-9212-399c72d514e5",
   "metadata": {},
   "source": [
    "#### AE2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c185f3-665d-4c4e-bbcf-94d5d791ceb8",
   "metadata": {},
   "source": [
    "We add more features - giving both timing and day entries to the auto encoder to determine if this has an impact on the anomalies detected. We also change our scaling to MinMaxScaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d22c7-baf3-4ee4-9691-1348746b9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_2(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                #feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                feat_dict[i*n + j] = [hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82ef30-ac14-451c-a908-422eecc7f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_2(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_2(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23719dcb-aeb2-4a3c-bc1d-c218e7fbabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_2 = pickle.load(open('Anomalies AE Time.p','rb'))\n",
    "    val_loss_2 = pickle.load(open('Validation Loss Time.p','rb'))\n",
    "    anomaly_bool_2 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_2.append((len(anomalies_ae_2[anomalies_ae_2['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_2.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_2 = []\n",
    "    anomaly_bool_2 = []\n",
    "    val_loss_2 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_2(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_2.append(f)\n",
    "            anomaly_bool_2.append((len(f),rt_usernames[i]))\n",
    "            val_loss_2.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_2.append((0,rt_usernames[i]))\n",
    "            val_loss_2.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_2 = pd.concat(frames_2)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_2, open('Anomalies AE Time.p','wb'))\n",
    "    pickle.dump(val_loss_2, open('Validation Loss Time.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a935f-075f-4d0b-9506-e1dbd923be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_2 = []\n",
    "for i in range(len(anomaly_bool_2)):\n",
    "    if anomaly_bool_2[i][0] == 0:    \n",
    "        non_anom_2.append(anomaly_bool_2[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_2]) > 0)[0]),len(anomaly_bool_2)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493806bd-38ce-4cb6-9bf1-c75c728069ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_2 = 100*len(anomalies_ae_2)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e8776-8df2-48c6-bfd2-f6bea176ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_2 = []\n",
    "for i in range(len(val_loss_2)):\n",
    "    final_loss_2.append(val_loss_2[i][0][len(val_loss_2[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61a928-e609-4f8b-a55c-512a256e2a4a",
   "metadata": {},
   "source": [
    "So this reduced the number of usernames identified as anamolous but did also reduce the error - I think this is likely due to the min max scaling so I'll implement this without the additional features and see what impact it has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57846e-c3ae-4655-8099-64e950bf1a66",
   "metadata": {},
   "source": [
    "#### AE3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c173102-497c-41f7-aef5-466aed8b20e7",
   "metadata": {},
   "source": [
    "This autoencoder runs under the MinMaxScaling to determine the impact of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc48033-3437-4bf1-96a7-1bbbd6b3cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_3(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff8d51-763d-4ec9-ab9d-8bc4167d0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_3 = pickle.load(open('Anomalies AE MMS.p','rb'))\n",
    "    val_loss_3 = pickle.load(open('Validation Loss MMS.p','rb'))\n",
    "    anomaly_bool_3 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_3.append((len(anomalies_ae_3[anomalies_ae_3['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_3.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_3 = []\n",
    "    anomaly_bool_3 = []\n",
    "    val_loss_3 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_3(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_3.append(f)\n",
    "            anomaly_bool_3.append((len(f),rt_usernames[i]))\n",
    "            val_loss_3.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_3.append((0,rt_usernames[i]))\n",
    "            val_loss_3.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_3 = pd.concat(frames_3)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_3, open('Anomalies AE MMS.p','wb'))\n",
    "    pickle.dump(val_loss_3, open('Validation Loss MMS.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67cde3-cfd6-421b-928d-1e3e1fc8d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_3 = []\n",
    "for i in range(len(anomaly_bool_3)):\n",
    "    if anomaly_bool_3[i][0] == 0:    \n",
    "        non_anom_3.append(anomaly_bool_3[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_3]) > 0)[0]),len(anomaly_bool_3)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962591c7-c80d-46ae-ae45-1cb9ed0361c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_3 = 100*len(anomalies_ae_3)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b63fb-3752-4fff-a219-f20ec4049ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_3 = []\n",
    "for i in range(len(val_loss_3)):\n",
    "    final_loss_3.append(val_loss_3[i][0][len(val_loss_3[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7e3bb-7e93-4da6-97a3-ae71f1e2bfc5",
   "metadata": {},
   "source": [
    " A similar amount of data is identified as anomalous and the validation error is much lower while still avoiding overfitting so we'll stick with this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d49576-6a2c-4f3c-961f-683509757679",
   "metadata": {},
   "source": [
    "#### Analysis of 3rd DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de1849-306c-4330-9582-21ba4b91e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f336563-c9c5-4f7b-bf44-34c3be19a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_3.groupby('Authent Type').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f921dd-5703-42d0-a8b5-c07f3aadb7b9",
   "metadata": {},
   "source": [
    "Some of these authentications are unlikely to be anomalies - for example, ScreensaverInvoked is most likely not a malicious anomaly since the attacker wouldn't gain anything from this, whereas NetworkLogons may be malicious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb18f6c-c97a-4bd5-9d19-281d2dd0c684",
   "metadata": {},
   "source": [
    "#### AE4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb732f-d152-4896-98c8-c0254ac66e33",
   "metadata": {},
   "source": [
    "We'll work with an autoencoder where we also pass the number of network logons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838094f4-76d9-4868-b8f5-d17c41f9609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0840a-69aa-4601-ba26-f8ef19fb259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_CAT(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    auth_type_df = pd.DataFrame(index = list(authentication_data['Authent Type'].unique()))\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                \n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    auth_df = auth_type_un_df(user,n)\n",
    "    df = pd.concat([df,auth_df])\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f9bc6-8ddd-43b7-a723-7315a5ffd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_4(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_CAT(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "    \n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    # min max scaler to transform to 0-1 so that we have 'p-values'\n",
    "    mmscaler = MinMaxScaler()\n",
    "    test_mae_loss = mmscaler.fit_transform(test_mae_loss.reshape(-1,1))\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > 0.95)\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb60713-3fd1-4fed-85d6-ee2a3cd593e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_4 = pickle.load(open('Anomalies AE CAT.p','rb'))\n",
    "    val_loss_4 = pickle.load(open('Validation Loss CAT.p','rb'))\n",
    "    anomaly_bool_4 = []\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_4.append((len(anomalies_ae_4[anomalies_ae_4['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_4.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_4 = []\n",
    "    anomaly_bool_4 = []\n",
    "    val_loss_4 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_4(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_4.append(f)\n",
    "            anomaly_bool_4.append((len(f),rt_usernames[i]))\n",
    "            val_loss_4.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_4.append((0,rt_usernames[i]))\n",
    "            val_loss_4.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_4 = pd.concat(frames_4)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_4, open('Anomalies AE CAT.p','wb'))\n",
    "    pickle.dump(val_loss_4, open('Validation Loss CAT.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06e93e-0e3a-479b-a525-b9e4adc96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_anom_4 = []\n",
    "for i in range(len(anomaly_bool_4)):\n",
    "    if anomaly_bool_4[i][0] == 0:    \n",
    "        non_anom_4.append(anomaly_bool_4[i][1])\n",
    "        \n",
    "print('{} of the red team usernames were identified to have anomalies out of {} red team usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "      format(len(np.where(np.array([i[0] for i in anomaly_bool_4]) > 0)[0]),len(anomaly_bool_4)))\n",
    "print('--------------------------------------------------------------------------------------------')        \n",
    "print(', '.join(map(str,non_anom_4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110b5ae-cf8f-444a-ba45-c7a3b3a4dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_4 = 100*len(anomalies_ae_4)/len(authentication_data[authentication_data['UserName'].isin(rt_usernames)])\n",
    "print('{:.2f}% of the red team data was identified as anomalous.'.format(perc_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a4e95-132b-481a-90f3-e01e13193052",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss_4 = []\n",
    "for i in range(len(val_loss_4)):\n",
    "    final_loss_4.append(val_loss_4[i][0][len(val_loss_4[i][0])-1]) \n",
    "    \n",
    "plt.figure()\n",
    "sns.boxplot(data = final_loss_4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b087ac-613f-4fe3-bfec-40495fcf25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_ae_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785df5f-43e3-4e05-bd77-2c152784d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(anomalies_ae_3,anomalies_ae_4,how='inner',on=list(anomalies_ae_4.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bafc2-a7c6-418e-a9a8-a0ba5d725605",
   "metadata": {},
   "source": [
    "So we find all (we miss 5) anomalies from the original auto encoder when including authentication type as a feature as well as finding an extra 22. The extra 22 are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458687e-e90c-4030-880d-23eb48412015",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([anomalies_ae_4, anomalies_ae_3, anomalies_ae_3]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c30d55-9ab8-4d14-8e6e-7511ff4a5f15",
   "metadata": {},
   "source": [
    "Reviewing these shows that they are 'interesting' authentiations - they are mostly authentication types that are of interest and are likely to be compromisable such as a network logon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b6fdc-b22d-45fe-8c0c-b3c073843ab0",
   "metadata": {},
   "source": [
    "We'll test this algorithm to see how many anomalies it picks up in a random subset of non-red team usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8521e5-8c9c-453a-9f90-91e866a94949",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_n_4 = []\n",
    "anomaly_bool_N_4 = []\n",
    "val_loss_n_4 = []\n",
    "bound = 20\n",
    "rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "s_time = datetime.datetime.now()\n",
    "\n",
    "for i,un in enumerate(rand_non_rt):\n",
    "    clear_output(wait=True)\n",
    "    print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "    f,b,val = ae_anomaly_finder_4(un,24)\n",
    "    if b == 1:\n",
    "        frames_n_4.append(f)\n",
    "        anomaly_bool_N_4.append((len(f),un))\n",
    "        val_loss_n_4.append(val)\n",
    "    else:\n",
    "        anomaly_bool_N_4.append((0,un))\n",
    "        val_loss_n_4.append(val)\n",
    "        pass\n",
    "anomalies_ae_n_4 = pd.concat(frames_n_4)\n",
    "\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04612f1-7d50-431d-9b48-c542e02b4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*len(anomalies_ae_n_4)/len(authentication_data[authentication_data['UserName'].isin(rand_non_rt)])\n",
    "print('{:.2f}% of the \"normal\" data was identified as anomalous.'.format(perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722694c-64a1-492f-b7b0-158f1a38d33f",
   "metadata": {},
   "source": [
    "Worryingly, we have a large amount of anomalies found in the 'normal' data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be209556-b6f7-4761-9d07-0afc5c0cb234",
   "metadata": {},
   "source": [
    "#### AE5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae4aa4-c5e0-4f86-92ce-5845b09b44c0",
   "metadata": {},
   "source": [
    "I'm not 100% sure but I think minmax scaling the output data is having adverse affects on our results - the third auto encoder seemed to do the 'best' so we'll use that as a basis but we'll remove scaling of the 'anomaly scores' and instead use the validation loss as our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ead163e-796b-48c3-844c-26bdc1aa0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_5(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > np.array(history.history[\"val_loss\"]).min())\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e05ddf-935f-49b2-980e-5ac7cc5ba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_5 = pickle.load(open('Anomalies AE VL.p','rb'))\n",
    "    val_loss_5 = pickle.load(open('Validation Loss VL.p','rb'))\n",
    "    anomaly_bool_5 = []\n",
    "    anomalies_ae_5 = anomalies_ae_5.drop_duplicates()\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_5.append((len(anomalies_ae_5[anomalies_ae_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_5 = []\n",
    "    anomaly_bool_5 = []\n",
    "    val_loss_5 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_5(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_5.append(f)\n",
    "            anomaly_bool_5.append((len(f),rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_5.append((0,rt_usernames[i]))\n",
    "            val_loss_5.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_5 = pd.concat(frames_5)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_5, open('Anomalies AE VL.p','wb'))\n",
    "    pickle.dump(val_loss_5, open('Validation Loss VL.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7452fdf-098e-408b-8fa6-999204c7ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ae_eval(anomaly_bools,anomalies,val_losses,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    perc_2 = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(anom)])\n",
    "    print('{:.2f}% of the anomalous user name data was identified as anomalous.'.format(perc_2,type_un))\n",
    "    \n",
    "    final_losses = []\n",
    "    for i in range(len(val_losses)):\n",
    "        final_losses.append(val_losses[i][0][len(val_losses[i][0])-1]) \n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(data = final_losses)\n",
    "    plt.show()\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788fc59-9443-40ca-8a72-0d17dc824a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_5,anomalies_ae_5,val_loss_5,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588045b-55d4-4e78-8f40-cc2fe22f46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    rand_non_rt = random.sample(non_rt_users,bound)\n",
    "    anomalies_ae_n_5 = pickle.load(open('Non RT Anomalies.p','rb'))\n",
    "    val_loss_n_5 = pickle.load(open('Non RT VL.p','rb'))\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "    anomaly_bool_N_5 = []\n",
    "    anomalies_ae_n_5 = anomalies_ae_n_5.drop_duplicates()\n",
    "\n",
    "    for un in rand_non_rt:\n",
    "        try:\n",
    "            anomaly_bool_N_5.append((len(anomalies_ae_n_5[anomalies_ae_n_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_n_5 = []\n",
    "    anomaly_bool_N_5 = []\n",
    "    val_loss_n_5 = []\n",
    "    bound = 200\n",
    "    rand_non_rt = random.sample(non_rt_users,bound)\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i,un in enumerate(rand_non_rt):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "        f,b,val = ae_anomaly_finder_5(un,24)\n",
    "        if b == 1:\n",
    "            frames_n_5.append(f)\n",
    "            anomaly_bool_N_5.append((len(f),un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "        else:\n",
    "            anomaly_bool_N_5.append((0,un))\n",
    "            val_loss_n_5.append((val,un))\n",
    "            pass\n",
    "    anomalies_ae_n_5 = pd.concat(frames_n_5)\n",
    "    pickle.dump(anomalies_ae_n_5,open('Non RT Anomalies.p','wb'))\n",
    "    pickle.dump(val_loss_n_5,open('Non RT VL.p','wb'))\n",
    "    pickle.dump(rand_non_rt,open('Random Sample of Non Red Team Usernames.p','wb'))\n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a59b9-58ce-4910-979b-ca0a044b0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_N_5,anomalies_ae_n_5,val_loss_n_5,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bfb9b-db68-4460-b963-7f50fb7b47e6",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacae89-9c13-463b-bcab-48415e59b570",
   "metadata": {},
   "source": [
    "We'll use data from the decision tree that Alex created to retry this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ac04b-e992-4db4-ae1c-263b6b4c6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = pd.read_csv('../Data/AuthScores.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536e770-756c-44ea-b6cc-681907757a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_w_scores = authentication_data\n",
    "auth_w_scores['Score'] = dt_scores['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0df96-1212-4874-a711-789d3912b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_score(user,n):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(auth_w_scores[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "                hour = j\n",
    "                data = chunks[j]\n",
    "                authents = len(data[data['UserName'] == user])\n",
    "                failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "                srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "                dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "                uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "                score_sum = np.sum(data[data['UserName'] == user]['Score'])\n",
    "                feat_dict[i*n + j] = [srcunique,dstunique,authents,failures,score_sum]\n",
    "                #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['SrcUnique','DstUnique','Authentications','Failures','score_sum'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7bd59-e8e7-4939-b5c5-aeff332c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_anomaly_finder_6(user,n,BATCH_SIZE=64, EPOCHS=1000):\n",
    "    \n",
    "    # get the data frame of features\n",
    "    df = feature_generation_score(user,n)\n",
    "    x_train_ae = np.concatenate([np.array(df[0:57*n]),np.array(df[82*n:])])\n",
    "    x_test_ae = np.array(df[57*n:82*n])\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train_ae)\n",
    "    x_train_ae = pipeline.transform(x_train_ae)\n",
    "    x_test_ae = pipeline.transform(x_test_ae)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train_ae.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the autoencoder\n",
    "    autoencoder =Sequential([\n",
    "\n",
    "        # deconstruct / encode\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "\n",
    "        # reconstruction / decode\n",
    "        tf.keras.layers.Dense(2, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    autoencoder.compile(optimizer=\"adam\", \n",
    "                        loss=\"mse\",\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = autoencoder.fit(\n",
    "        x_train_ae, x_train_ae,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = autoencoder.predict(x_test_ae)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test_ae), axis=1)\n",
    "    \n",
    "    anomalies = np.where(test_mae_loss > np.array(history.history[\"val_loss\"]).min())\n",
    "    anomaly_idx = anomalies[0]\n",
    "    \n",
    "    frame = []\n",
    "    for i in range(len(anomaly_idx)):\n",
    "        anomaly = ae_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "        if len(anomaly) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            frame.append(anomaly)\n",
    "        \n",
    "    if len(frame) != 0:\n",
    "        anomaly_df = pd.concat(frame)\n",
    "        return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    else:\n",
    "        print('No anomalies found.')\n",
    "        return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedf47d-ee0e-4495-81a0-3c131e750947",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_6 = pickle.load(open('Anomalies AE score.p','rb'))\n",
    "    val_loss_6 = pickle.load(open('Validation Loss score.p','rb'))\n",
    "    anomaly_bool_6 = []\n",
    "    anomalies_ae_6 = anomalies_ae_6.drop_duplicates()\n",
    "    \n",
    "    for un in rt_usernames:\n",
    "        try:\n",
    "            anomaly_bool_6.append((len(anomalies_ae_6[anomalies_ae_6['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_6.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_6 = []\n",
    "    anomaly_bool_6 = []\n",
    "    val_loss_6 = []\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i in range(len(rt_usernames)):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(i+1,len(rt_usernames),100*((i+1)/len(rt_usernames))))\n",
    "        f,b,val = ae_anomaly_finder_6(rt_usernames[i],24)\n",
    "        if b == 1:\n",
    "            frames_6.append(f)\n",
    "            anomaly_bool_6.append((len(f),rt_usernames[i]))\n",
    "            val_loss_6.append((val,rt_usernames[i]))\n",
    "        else:\n",
    "            anomaly_bool_6.append((0,rt_usernames[i]))\n",
    "            val_loss_6.append((val,rt_usernames[i]))\n",
    "            pass\n",
    "    anomalies_ae_6 = pd.concat(frames_6)\n",
    "    \n",
    "    pickle.dump(anomalies_ae_6, open('Anomalies AE score.p','wb'))\n",
    "    pickle.dump(val_loss_6, open('Validation Loss score.p','wb'))\n",
    "    \n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240185e-ba9c-4f2b-a100-d4dbe7346354",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_6,anomalies_ae_6,val_loss_6,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ffe00-9417-4ac0-83e9-d037eed203e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    anomalies_ae_n_6 = pickle.load(open('Non RT Anomalies score.p','rb'))\n",
    "    val_loss_n_6 = pickle.load(open('Non RT score.p','rb'))\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "    anomaly_bool_N_6 = []\n",
    "    anomalies_ae_n_6 = anomalies_ae_n_6.drop_duplicates()\n",
    "\n",
    "    for un in rand_non_rt:\n",
    "        try:\n",
    "            anomaly_bool_N_6.append((len(anomalies_ae_n_5[anomalies_ae_n_5['UserName'] == un]),un))\n",
    "        except KeyError:\n",
    "            anomaly_bool_N_6.append((0,un))\n",
    "    clear_output()\n",
    "    print('Data available.')\n",
    "except:\n",
    "    clear_output()\n",
    "    print('Running AutoEncoder')\n",
    "    frames_n_6 = []\n",
    "    anomaly_bool_N_6 = []\n",
    "    val_loss_n_6 = []\n",
    "    bound = 200\n",
    "    rand_non_rt = pickle.load(open('Random Sample of Non Red Team Usernames.p','rb'))\n",
    "\n",
    "    s_time = datetime.datetime.now()\n",
    "\n",
    "    for i,un in enumerate(rand_non_rt):\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {} percentage complete.'.format(i+1,bound,100*((i+1)/bound)))\n",
    "        f,b,val = ae_anomaly_finder_6(un,24)\n",
    "        if b == 1:\n",
    "            frames_n_6.append(f)\n",
    "            anomaly_bool_N_6.append((len(f),un))\n",
    "            val_loss_n_6.append((val,un))\n",
    "        else:\n",
    "            anomaly_bool_N_6.append((0,un))\n",
    "            val_loss_n_6.append((val,un))\n",
    "            pass\n",
    "    anomalies_ae_n_6 = pd.concat(frames_n_6)\n",
    "    pickle.dump(anomalies_ae_n_6,open('Non RT Anomalies score.p','wb'))\n",
    "    pickle.dump(val_loss_n_6,open('Non RT score.p','wb'))\n",
    "    e_time = datetime.datetime.now()\n",
    "    print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3695c8-b968-4de9-b38c-ad386c29b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae_eval(anomaly_bool_N_6,anomalies_ae_n_6,val_loss_n_6,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d7ecb8-0621-440c-b94f-e9a580ba052a",
   "metadata": {},
   "source": [
    "#### Bayes Poisson Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c1ed6-f7da-4b7d-bd37-d41f2f99d033",
   "metadata": {},
   "source": [
    "Based on a gamma prior for a poisson distribution, we can estimate $ \\lambda $ by $ \\frac{(\\sum{x_{i}} + \\alpha)}{(n + \\beta)}$. This is because under a $\\Gamma(\\alpha,\\beta)$ prior for $\\lambda$ we have that the posterior is $$ \\pi(\\lambda|x) \\propto \\lambda^{\\sum{x_{i}}+\\alpha-1} e^{-(n+\\beta)\\lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87b3bf-c577-4fd7-865a-0a6b42713347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_orig_finder(user,n,idx):\n",
    "    \n",
    "    j_idx = idx % n\n",
    "    i_idx = int(idx/n)\n",
    "\n",
    "    chunks = split_dataframe(auth_w_scores[auth_start_days[i_idx]:auth_start_days[i_idx+1]],n)\n",
    "    data = chunks[j_idx][chunks[j_idx]['UserName'] == user]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203234-dcd3-4680-8ada-793b2d40ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poiss_ae_detection(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for col in pois_df.columns:\n",
    "            dt = pois_df.iloc[:57*n][col]\n",
    "            bayes_mean.append((alpha+sum(dt))/(beta+len(dt)))\n",
    "            bayes_var.append((alpha+sum(dt))/(beta+len(dt))**2)\n",
    "\n",
    "        probabilities = stats.poisson.pmf(pois_df.iloc[57*n:82*n],bayes_mean)\n",
    "        \n",
    "        if comb == True:\n",
    "            for i in range(len(probabilities)):\n",
    "\n",
    "                prob = probabilities[i]\n",
    "\n",
    "                f_probs.append(stats.combine_pvalues(prob))\n",
    "            for i in range(len(f_probs)):\n",
    "\n",
    "                pv = f_probs[i][1]\n",
    "\n",
    "                if pv <= 0.05:\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "        elif comb == False:\n",
    "            for i in range(len(probabilities)):\n",
    "                \n",
    "                prob = probabilities[i]\n",
    "                \n",
    "                if any(prob <= 0.05):\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "    events = pd.concat(events_frames)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a770d1f-e3e5-4788-9ea4-46f06aa2c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_poiss_eval(anomaly_bools,anomalies,usernames,type_un):\n",
    "    \n",
    "    non_anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] == 0:    \n",
    "            non_anom.append(anomaly_bools[i][1])\n",
    "\n",
    "    anom = []\n",
    "    for i in range(len(anomaly_bools)):\n",
    "        if anomaly_bools[i][0] != 0:    \n",
    "            anom.append(anomaly_bools[i][1])\n",
    "    \n",
    "    print('{} of the {} usernames were identified to have anomalies out of {} {} usernames. The folloiwng usernames were found to have no anomalies'.\n",
    "          format(len(np.where(np.array([i[0] for i in anomaly_bools]) > 0)[0]),type_un,len(anomaly_bools),type_un))\n",
    "    print('--------------------------------------------------------------------------------------------')        \n",
    "    print(', '.join(map(str,non_anom)))\n",
    "    \n",
    "    perc = 100*len(anomalies)/len(authentication_data[authentication_data['UserName'].isin(usernames)])\n",
    "    print('{:.2f}% of the {} data was identified as anomalous.'.format(perc,type_un))\n",
    "    \n",
    "    print(anomalies.groupby('Authent Type').size())\n",
    "    \n",
    "    anomalies.head()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e28ad3-b804-466a-aeb7-3a8f442f5d02",
   "metadata": {},
   "source": [
    "The commented code below was testing methods of combining probabilities based on: https://www.sciencedirect.com/science/article/pii/S0169207013001635 and https://link.springer.com/article/10.1007%2Fs11004-012-9396-3#citeas. Ultimately none of them were fruitful and only fishers method of combining produces meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5b103-a206-4bfd-91a3-0e1b882ff3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "\n",
    "#     prob = probabilities[i]\n",
    "\n",
    "#     f_probs.append(stats.combine_pvalues(prob))\n",
    "\n",
    "# print([prob for prob in f_probs if prob[1] <= 0.1])\n",
    "\n",
    "# geom_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     geom_probs.append(stats.mstats.gmean(prob))\n",
    "    \n",
    "# print([prob for prob in geom_probs if prob <= 0.1])\n",
    "\n",
    "# elop_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     numer = np.prod(prob**(1/len(prob)))\n",
    "#     denom = np.prod(prob**(1/len(prob))) + np.prod((1-prob)**(1/len(prob)))\n",
    "#     logit_probs.append(numer/denom)\n",
    "# print([prob for prob in elop_probs if prob <= 0.1])\n",
    "\n",
    "# beta_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     x = np.sum(prob*(1/len(prob)))\n",
    "#     beta_probs.append(stats.beta.cdf(x,a=1,b=1))\n",
    "    \n",
    "# print([prob for prob in beta_probs if prob <= 0.1])\n",
    "\n",
    "# agg_probs = []\n",
    "# for i in range(len(probabilities)):\n",
    "    \n",
    "#     prob = probabilities[i]\n",
    "#     num = np.prod((prob/(1-prob))**1/len(prob))**0.1\n",
    "#     denom = 1+np.prod((prob/(1-prob))**1/len(prob))**0.1\n",
    "#     agg_probs.append(num/denom)\n",
    "    \n",
    "# print([prob for prob in agg_probs if prob <= 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899cf82-0733-4d24-a887-c1609ebc1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms = poiss_ae_detection(list(set(rt_usernames)),24,True)\n",
    "\n",
    "poisson_anoms = poisson_anoms.drop_duplicates()\n",
    "poiss_anom_bool = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms['UserName']):\n",
    "        poiss_anom_bool.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c5810-6f33-4bb1-9d15-1bf221a0473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool,poisson_anoms,list(set(rt_usernames)),'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed259ffa-7244-420d-bfcf-67c78bcdc9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_nc = poiss_ae_detection(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_nc = poisson_anoms_nc.drop_duplicates()\n",
    "poiss_anom_bool_nc = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_nc['UserName']):\n",
    "        poiss_anom_bool_nc.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_nc.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3deed7-6d91-4513-a4c5-020962d5935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_anoms_nc[(poisson_anoms_nc['UserName'] == rt_usernames[11])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968efc09-fb0b-45b2-aadb-db61b1c21bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_nc,poisson_anoms_nc,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce88e3-de7e-43e3-8d10-eab5e89411fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "poisson_anoms_n = poiss_ae_detection(rand_non_rt,24,False)\n",
    "\n",
    "poisson_anoms_n = poisson_anoms_n.drop_duplicates()\n",
    "poiss_anom_bool_n = []\n",
    "\n",
    "for un in rand_non_rt:\n",
    "    if un in list(poisson_anoms_n['UserName']):\n",
    "        poiss_anom_bool_n.append((1,un))\n",
    "    else:  \n",
    "        poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debaa30d-d509-4c9b-8d1f-3d11fd157fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_n,poisson_anoms_n,rand_non_rt,'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837496da-7ee0-4417-a1cb-4d0cbb1aea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhom_poiss_ae_detection(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for i in range(n):\n",
    "            bayes_mean_hr = []\n",
    "            bayes_var_hr = []\n",
    "            for col in pois_df.columns:\n",
    "                dt = list(pd.concat([pois_df.iloc[:57*n],pois_df.iloc[83*n:]])[col])\n",
    "                dt_hspl = dt[i::n]\n",
    "                bayes_mean_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl)))\n",
    "                bayes_var_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl))**2)\n",
    "\n",
    "            bayes_mean.append(bayes_mean_hr)\n",
    "            bayes_var.append(bayes_var_hr)\n",
    "\n",
    "        probabilities = []\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            probabilities.append(stats.poisson.pmf(pois_df.iloc[57*n:82*n].iloc[i::n],bayes_mean[i]))\n",
    "\n",
    "        if comb == True:\n",
    "            for i in range(n):\n",
    "    \n",
    "                probs = probabilities[i]\n",
    "\n",
    "                for j in range(len(probs)):\n",
    "\n",
    "                    hr_prob = probs[j]\n",
    "\n",
    "                    f_probs.append(stats.combine_pvalues(hr_prob))\n",
    "                    \n",
    "                for i in range(len(f_probs)):\n",
    "\n",
    "                    pv = f_probs[i][1]\n",
    "\n",
    "                    if pv <= 0.05:\n",
    "                        events_frames.append(poiss_orig_finder(un,n,57*n+i))\n",
    "        \n",
    "        elif comb == False:\n",
    "            for i in range(n):\n",
    "    \n",
    "                probs = probabilities[i]\n",
    "\n",
    "                for j in range(len(probs)):\n",
    "\n",
    "                    hr_prob = probs[j]\n",
    "\n",
    "                    if np.any(hr_prob <= 0.05):\n",
    "                        \n",
    "                        events_frames.append(poiss_orig_finder(un,n,57*n+j*n+i))\n",
    "        \n",
    "    events = pd.concat(events_frames)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502305d0-7cfe-4c18-bdb1-1886e97cf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inhom_poiss_ae_detection_2(usernames,n,comb,alpha=1,beta=1):\n",
    "    events_frames = []\n",
    "    \n",
    "    for (un_ct,un) in enumerate(usernames):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Working with username {} of {}. {:.2f} percentage complete.'.format(un_ct+1,len(usernames),100*((un_ct+1)/len(usernames))))\n",
    "        pois_df = auth_type_un_df(un,n)\n",
    "        bayes_mean = []\n",
    "        bayes_var = []\n",
    "        alpha = alpha\n",
    "        beta = beta\n",
    "\n",
    "        f_probs = []\n",
    "\n",
    "        for i in range(n):\n",
    "            bayes_mean_hr = []\n",
    "            bayes_var_hr = []\n",
    "            for col in pois_df.columns:\n",
    "                dt = list(pd.concat([pois_df.iloc[:57*n],pois_df.iloc[83*n:]])[col])\n",
    "                dt_hspl = dt[i::n]\n",
    "                bayes_mean_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl)))\n",
    "                bayes_var_hr.append((alpha+sum(dt_hspl))/(beta+len(dt_hspl))**2)\n",
    "\n",
    "            bayes_mean.append(bayes_mean_hr)\n",
    "            bayes_var.append(bayes_var_hr)\n",
    "        \n",
    "        find_prob = []\n",
    "        for i in range(n):\n",
    "            find_prob.append(pd.DataFrame(stats.poisson.pmf(pd.concat([pois_df.iloc[:57*n], pois_df.iloc[83*n:]]).iloc[i::n],bayes_mean[i])).min())\n",
    "                \n",
    "        probabilities = []\n",
    "        \n",
    "        for i in range(n):\n",
    "\n",
    "            probabilities.append(stats.poisson.pmf(pois_df.iloc[57*n:82*n].iloc[i::n],bayes_mean[i]))\n",
    "\n",
    "        for i in range(n):\n",
    "\n",
    "            probs = probabilities[i]\n",
    "\n",
    "            for j in range(len(probs)):\n",
    "\n",
    "                hr_prob = probs[j]\n",
    "                \n",
    "                thresh = find_prob[i]\n",
    "                \n",
    "                if [item1 for item1,item2 in zip(hr_prob,thresh) if item1 <= item2] != []:\n",
    "\n",
    "                    events_frames.append(poiss_orig_finder(un,n,57*n+j*n+i))\n",
    "\n",
    "    events = pd.concat(events_frames)\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f51d3-2035-4cc6-941e-9669954e48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_inhom = inhom_poiss_ae_detection(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_inhom = poisson_anoms_inhom.drop_duplicates()\n",
    "poiss_anom_bool_inhom = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_inhom['UserName']):\n",
    "        poiss_anom_bool_inhom.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_inhom.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7735bf-c682-437e-837a-36f5da9e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_inhom,poisson_anoms_inhom,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333155ca-922c-4e0b-9aee-c4efa348d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "rand_non_rt_p = random.sample(non_rt_users,400)\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poiss_ae_detection(rand_non_rt_p,24,False)\n",
    "\n",
    "inhom_poisson_anoms_n = inhom_poisson_anoms_n.drop_duplicates()\n",
    "inhom_poiss_anom_bool_n = []\n",
    "\n",
    "for un in rand_non_rt_p:\n",
    "    if un in list(inhom_poisson_anoms_n['UserName']):\n",
    "        inhom_poiss_anom_bool_n.append((1,un))\n",
    "    else:\n",
    "        inhom_poiss_anom_bool_n.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37548f39-5b14-40d5-9d9d-668372e7d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(inhom_poiss_anom_bool_n,inhom_poisson_anoms_n,rand_non_rt_p,'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1585d5-f58b-4063-89fc-1a83d580ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "poisson_anoms_inhom_2 = inhom_poiss_ae_detection_2(rt_usernames,24,False)\n",
    "\n",
    "poisson_anoms_inhom_2 = poisson_anoms_inhom_2.drop_duplicates()\n",
    "poiss_anom_bool_inhom_2 = []\n",
    "\n",
    "for un in rt_usernames:\n",
    "    if un in list(poisson_anoms_inhom_2['UserName']):\n",
    "        poiss_anom_bool_inhom_2.append((1,un))\n",
    "    else:\n",
    "        poiss_anom_bool_inhom_2.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01d4c2-4f5f-4fc8-a08f-5b4a36a6b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(poiss_anom_bool_inhom_2,poisson_anoms_inhom_2,rt_usernames,'red team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022a6dd-c2ee-41cc-be29-726e302591f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = datetime.datetime.now()\n",
    "\n",
    "rand_non_rt_p = random.sample(non_rt_users,400)\n",
    "\n",
    "inhom_poisson_anoms_n_2 = inhom_poiss_ae_detection_2(rand_non_rt_p,24,False)\n",
    "\n",
    "inhom_poisson_anoms_n_2 = inhom_poisson_anoms_n_2.drop_duplicates()\n",
    "inhom_poiss_anom_bool_n_2 = []\n",
    "\n",
    "for un in rand_non_rt_p:\n",
    "    if un in list(inhom_poisson_anoms_n_2['UserName']):\n",
    "        inhom_poiss_anom_bool_n_2.append((1,un))\n",
    "    else:\n",
    "        inhom_poiss_anom_bool_n_2.append((0,un))\n",
    "e_time = datetime.datetime.now()\n",
    "print(e_time-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1179a-7764-41ba-acf1-1b3cad3ddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_poiss_eval(inhom_poiss_anom_bool_n_2,inhom_poisson_anoms_n_2,rand_non_rt_p,'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65abd251-8f49-4412-97f5-809e6d3b1609",
   "metadata": {},
   "source": [
    "#### UASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49344d-c404-4b7c-8bb9-a5d83f2a3a1a",
   "metadata": {},
   "source": [
    "We need to rewrite the data frame creation - we need all positions to be the same for this to perform effectively so we create our own adjacency creation here. This allows us to ensure that each username and each destination device is in the same position in each dataframe but is unfortunately incredibly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94519e77-d9d3-4f0e-a06d-95171a042106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Reading Data.')\n",
    "    data_frame_list_uase = pickle.load(open('C:/Users/corri/OneDrive/Documents/Uni/Postgraduate/Final Project/Data Frame List UASE.p','rb'))\n",
    "    index_sparse = pickle.load(open('Index UASE.p','rb'))\n",
    "    columns_sparse = pickle.load(open('Columns UASE.p','rb'))\n",
    "    \n",
    "except:\n",
    "    clear_output()\n",
    "    print('Creating Data Frames.')\n",
    "    data_frame_list_uase = []\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    for i in tqdm(range(len(auth_start_days)-1)):\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(index = list(authentication_data['DstDevice'].unique()))\n",
    "\n",
    "        chunk = authentication_data[auth_start_days[i]:auth_start_days[i+1]]\n",
    "        data_un ={}\n",
    "        for user in authentication_data['UserName'].unique():\n",
    "            dstdevice_data = chunk[chunk['UserName'] == user].groupby('DstDevice').size()\n",
    "            data_un[user] = data_frame_ind.index.to_series().map(dstdevice_data.to_dict())\n",
    "\n",
    "        data_frame_ind = pd.DataFrame(data=data_un,index = list(authentication_data['DstDevice'].unique()))\n",
    "        data_frame_ind = data_frame_ind.notnull().astype('int')\n",
    "        data_frame_ind = data_frame_ind.fillna(0)\n",
    "        A = np.array(data_frame_ind)\n",
    "        sA = sparse.csr_matrix(A)\n",
    "        data_frame_list_uase.append(sA)    \n",
    "\n",
    "    index_sparse = data_frame_ind.index\n",
    "    columns_sparse = data_frame_ind.columns\n",
    "    pickle.dump(data_frame_list_uase, open('Data Frame List UASE.p', 'wb'))\n",
    "    pickle.dump(index_sparse, open('Index UASE.p', 'wb'))\n",
    "    pickle.dump(columns_sparse, open('Columns UASE.p', 'wb'))\n",
    "    print(datetime.datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d19af3-3016-478e-960e-568fb8e9530e",
   "metadata": {},
   "source": [
    "We plot the first 100 sorted eignevalues to determine where to cut off our k using the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7256b-52c1-4372-ab9e-19a46d45e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "for i in tqdm(range(weeks)):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "    \n",
    "        #user_week_matrix = week_matrix[:, k::10]\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix,k=100)\n",
    "    plt.plot(-np.sort(-s))\n",
    "plt.xticks(np.arange(0, 101, 1.0))\n",
    "plt.axvline(5)\n",
    "plt.axvline(6)\n",
    "plt.axvline(7)\n",
    "plt.show()\n",
    "        #Y = (v.transpose()*s**1/2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a1906-4def-405f-bb63-9a887aba658a",
   "metadata": {},
   "source": [
    "So for all the matrices, somewhere around 6/7 eigenvalues we obtain an elbow. We'll use k=7 since this is still fast to compute but also allows us to capture a lot of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbcb34-a647-411d-a46f-2b17c332cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "Y_arrays = []\n",
    "\n",
    "for i in range(weeks):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix.asfptype(),k=7)\n",
    "    Y = pd.DataFrame((v.transpose()*s**0.5).transpose(),columns=(list(columns_sparse)*7))\n",
    "    Y_arrays.append(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab0a0-3dd1-4c45-852c-b03289c9e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_count = len(columns_sparse)\n",
    "anom = []\n",
    "\n",
    "for i in tqdm(range(u_count)):\n",
    "    \n",
    "    normal_vals = list([] for x in range(7))\n",
    "    \n",
    "    for j in range(len(Y_arrays)-1):\n",
    "        if j <=7:\n",
    "            Y1 = Y_arrays[j].iloc[:, i::u_count]\n",
    "            for k in range(7):\n",
    "                vals_1 = Y1.iloc[:,k]\n",
    "                normal_vals[k].append(list(vals_1))\n",
    "                \n",
    "        else:\n",
    "            Y1 = Y_arrays[j].iloc[:, i::u_count]\n",
    "            p_vals = []\n",
    "            for k in range(7):\n",
    "                vals_1 = Y1.iloc[:,k]\n",
    "                vals_1 = [i for i in vals_1 if i != 0]\n",
    "                comparison = normal_vals[k]\n",
    "                comparison = [item for sublist in comparison for item in sublist]\n",
    "                comparison = [i for i in comparison if i != 0]\n",
    "                t,p = stats.ttest_ind(vals_1,comparison, equal_var=False)\n",
    "                p_vals.append(p)\n",
    "            p = scipy.stats.combine_pvalues(p_vals)\n",
    "            if p[1] <= 0.05:\n",
    "                anom.append((columns_sparse[i],j+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed898c-7c1f-4b1a-b95a-ffb1d81f4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=zip(*anom)\n",
    "len(pd.Series(a).unique()), len(list(set(a) & set(rt_usernames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dcab5-7705-4126-b87d-45be2de7a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_list_uase_2 = []\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for i in tqdm(range(len(auth_start_days)-1)):\n",
    "\n",
    "    data_frame_ind_2 = pd.DataFrame(index = list(authentication_data['DstDevice'].unique()))\n",
    "\n",
    "    chunk = authentication_data[auth_start_days[i]:auth_start_days[i+1]]\n",
    "    data_un_2 ={}\n",
    "    for user in rt_usernames:\n",
    "        dstdevice_data = chunk[chunk['UserName'] == user].groupby('DstDevice').size()\n",
    "        data_un_2[user] = data_frame_ind_2.index.to_series().map(dstdevice_data.to_dict())\n",
    "\n",
    "    data_frame_ind_2 = pd.DataFrame(data=data_un_2,index = list(authentication_data['DstDevice'].unique()))\n",
    "    data_frame_ind_2 = data_frame_ind_2.notnull().astype('int')\n",
    "    data_frame_ind_2 = data_frame_ind_2.fillna(0)\n",
    "    A = np.array(data_frame_ind_2)\n",
    "    sA = sparse.csr_matrix(A)\n",
    "    data_frame_list_uase_2.append(sA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7fa40-25d2-4daa-ab4f-dcf03fd052a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "Y_arrays_2 = []\n",
    "\n",
    "for i in range(weeks):\n",
    "    week_matrices = []\n",
    "    for j in range(7):\n",
    "        week_matrices.append(data_frame_list_uase_2[i*7+j])\n",
    "    \n",
    "    week_matrix = scipy.sparse.hstack(week_matrices)\n",
    "    \n",
    "        #user_week_matrix = week_matrix[:, k::10]\n",
    "        \n",
    "    u, s, v = scipy.sparse.linalg.svds(week_matrix.asfptype(),k=7)\n",
    "    Y = pd.DataFrame((v.transpose()*s**0.5).transpose(),columns=(list(set(rt_usernames))*7))\n",
    "    Y_arrays_2.append(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb8a1-4e74-43f2-bad8-c5f96fa3f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_2 = []\n",
    "\n",
    "for i in tqdm(range(len(set(rt_usernames)))):\n",
    "    for j in range(len(Y_arrays_2)-1):\n",
    "        Y1 = Y_arrays_2[j].iloc[:, i::len(set(rt_usernames))]\n",
    "        Y2 = Y_arrays_2[j+1].iloc[:, i::len(set(rt_usernames))]\n",
    "        p_vals = []\n",
    "        for k in range(7):\n",
    "            vals_1 = Y1.iloc[:,k]\n",
    "            vals_2 = Y2.iloc[:,k]\n",
    "            if all(v == 0 for v in vals_1) or all(v == 0 for v in vals_2):\n",
    "                   p = 1\n",
    "            else:\n",
    "                t,p = stats.ttest_ind(vals_1,vals_2, equal_var=False)\n",
    "                p_vals.append(p)\n",
    "        p = scipy.stats.combine_pvalues(p_vals)\n",
    "        if p[1] <= 0.05:\n",
    "            anom_2.append((columns_sparse[i],j+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324a734-668e-499c-8345-49aaaad28a4b",
   "metadata": {},
   "source": [
    "unsuccessful implementation of anomalous outlier detection using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8beb66-c2cf-4e92-99a1-43888f8841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.cluster import OPTICS\n",
    "\n",
    "# normal_anomalous_usernames = []\n",
    "# for j in range(8):\n",
    "#     for i in tqdm(range(7)):\n",
    "#         #dbscan = DBSCAN(eps=0.025).fit((Y_arrays[j].iloc[0:2, :28815*(i+1)]).transpose())\n",
    "#         Y = (Y_arrays[j].groupby(Y_arrays[j].columns.values, axis=1).agg(lambda x: x.values.tolist()).sum().apply(pd.Series).T).transpose()\n",
    "#         dbscan = DBSCAN().fit(Y)\n",
    "#         #plt.figure()\n",
    "#         #plt.scatter(Y_arrays_2[0].loc[0][:95*(i+1)],Y_arrays_2[0].loc[1][:95*(i+1)],c=dbscan.labels_.astype(float))\n",
    "#         #plt.figure()\n",
    "#         for un in list(columns_sparse[np.where(dbscan.labels_ == -1)]):\n",
    "#             if un not in normal_anomalous_usernames:\n",
    "#                 normal_anomalous_usernames.append(un)\n",
    "\n",
    "# anomalous_usernames = []\n",
    "# for j in range(7,len(Y_arrays)):\n",
    "#      for i in range(7):\n",
    "#         #dbscan = DBSCAN(e=0.025).fit((Y_arrays[j].iloc[0:2, :28815*(i+1)]).transpose())\n",
    "#         dbscan = DBSCAN(eps=0.025).fit((Y_arrays[j].groupby(Y_arrays[j].columns.values, axis=1).agg(lambda x: x.values.tolist()).sum().apply(pd.Series).T).transpose())\n",
    "#         for un in list(columns_sparse[np.where(dbscan.labels_ == -1)]):\n",
    "#             if un not in normal_anomalous_usernames:\n",
    "#                 anomalous_usernames.append(un)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3eea06-23bb-41b9-8679-3578d6f8d35b",
   "metadata": {},
   "source": [
    "#### Anomaly Detection Using ASE principles - https://arxiv.org/pdf/2008.10055.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4e19e-be3a-49cb-850a-35d4a93127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASE_anom_finder(window_length):\n",
    "\n",
    "    vertex_norm_tracker = []\n",
    "    graph_norm_tracker = []\n",
    "\n",
    "    graph_mam_tracker = []\n",
    "    graph_range_tracker = []\n",
    "\n",
    "    vertex_mam_tracker = []\n",
    "    vertex_range_tracker = []\n",
    "\n",
    "    for t in tqdm(range(len(data_frame_list_uase)-1)):\n",
    "        u1, s1, v1 = scipy.sparse.linalg.svds(data_frame_list_uase[t],k=7)\n",
    "        u2, s2, v2 = scipy.sparse.linalg.svds(data_frame_list_uase[t+1],k=7)\n",
    "        Y1 = pd.DataFrame((v1.transpose()*s1**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "        Y2 = pd.DataFrame((v2.transpose()*s2**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "\n",
    "        nrm = scipy.linalg.norm(Y2-Y1,ord=2)\n",
    "        graph_norm_tracker.append(nrm)\n",
    "\n",
    "        vertex_norms = []\n",
    "        for v in list(columns_sparse):\n",
    "            nrm = scipy.linalg.norm(Y2[v]-Y1[v],ord=2)\n",
    "            vertex_norms.append(nrm)\n",
    "\n",
    "        vertex_norm_tracker.append(vertex_norms)\n",
    "\n",
    "        if t >= window_length:\n",
    "            graph_mam = np.sum(graph_norm_tracker[(t-window_length+1):t-1])/(window_length-1)\n",
    "            graph_range = np.sum(np.linalg.norm(np.array(graph_norm_tracker[t-window_length+2:t-1])-np.array(graph_norm_tracker[t-window_length+1:t-2])))/(1.128*(window_length-2))\n",
    "            graph_mam_tracker.append(graph_mam)\n",
    "            graph_range_tracker.append(graph_range)\n",
    "\n",
    "            vertex_mam_tracker_day = []\n",
    "            vertex_range_tracker_day = []\n",
    "\n",
    "            for j in range(len(list(columns_sparse))):\n",
    "                vertex_mam = np.sum([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_mam_tracker_day.append(vertex_mam)\n",
    "                vertex_range = np.sqrt((vertex_mam*(1-vertex_mam))/(window_length-1))\n",
    "                vertex_range_tracker_day.append(vertex_range)\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker_day)\n",
    "            vertex_range_tracker.append(vertex_range_tracker_day)\n",
    "\n",
    "    day_anomalies = []\n",
    "\n",
    "    for i,v in enumerate(list(columns_sparse)):\n",
    "        y_v = np.array([vertex_norm_tracker[k][i] for k in range(len(vertex_norm_tracker))])\n",
    "        ucl_v = np.array([vertex_mam_tracker[k][i] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][i] for k in range(len(vertex_range_tracker))])\n",
    "        anomalies = [j+window_length+1 for j,y in enumerate(y_v[window_length:]) if y > ucl_v[j]]\n",
    "        if len(anomalies) > 0:\n",
    "            day_anomalies.append((v,anomalies))\n",
    "        \n",
    "    return vertex_norm_tracker, graph_norm_tracker, graph_mam_tracker, graph_range_tracker, vertex_mam_tracker, vertex_range_tracker, day_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b0207-a8ba-4f05-b69d-22bb9cbfb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_norm_tracker_7, graph_norm_tracker_7, graph_mam_tracker_7, graph_range_tracker_7, vertex_mam_tracker_7, vertex_range_tracker_7, day_anomalies_7 = ASE_anom_finder(7)\n",
    "vertex_norm_tracker_14, graph_norm_tracker_14, graph_mam_tracker_14, graph_range_tracker_14, vertex_mam_tracker_14, vertex_range_tracker_14, day_anomalies_14 = ASE_anom_finder(14)\n",
    "vertex_norm_tracker_3, graph_norm_tracker_3, graph_mam_tracker_3, graph_range_tracker_3, vertex_mam_tracker_3, vertex_range_tracker_3, day_anomalies_3 = ASE_anom_finder(3)\n",
    "vertex_norm_tracker_2, graph_norm_tracker_2, graph_mam_tracker_2, graph_range_tracker_2, vertex_mam_tracker_2, vertex_range_tracker_2, day_anomalies_2 = ASE_anom_finder(2)\n",
    "vertex_norm_tracker_5, graph_norm_tracker_5, graph_mam_tracker_5, graph_range_tracker_5, vertex_mam_tracker_5, vertex_range_tracker_5, day_anomalies_5 = ASE_anom_finder(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0da2ad-47d2-4ae1-9430-f06d0cd96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_count_rt(day_anomalies,day):\n",
    "    anomaly_users = []\n",
    "    for i in range(len(day_anomalies)):\n",
    "        anomaly_users.append(day_anomalies[i][0])\n",
    "\n",
    "    rt_ct = 0\n",
    "    for un in set(rt_usernames):\n",
    "        if un in anomaly_users:\n",
    "            rt_ct += 1\n",
    "            \n",
    "    n_ct = 0\n",
    "    for un in set(rt_usernames) ^ set(list(columns_sparse)):\n",
    "        if un in anomaly_users:\n",
    "            n_ct += 1\n",
    "    \n",
    "    print('Using a sliding window of length {} captures {}% of rt users as anomalous and {}% of normal users as anomalous.'.format(day,rt_ct/len(set(rt_usernames)),(n_ct/len(set(rt_usernames) ^ set(list(columns_sparse))))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7838e7-6b8f-45cf-b3e0-fd0da3b90274",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_count_rt(day_anomalies_14,14)\n",
    "anomaly_count_rt(day_anomalies_7,7)\n",
    "anomaly_count_rt(day_anomalies_5,5)\n",
    "anomaly_count_rt(day_anomalies_3,3)\n",
    "anomaly_count_rt(day_anomalies_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9a7a7a-4195-4540-874a-1f4b713488f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_creation(window_length,vertex_mam_tracker,vertex_range_tracker,vertex_norm_tracker,u,vertex=True):\n",
    "\n",
    "    plt.figure(figsize = (16,8))  \n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),[vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])\n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1),np.array([vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][u] for k in range(len(vertex_range_tracker))]),'--')\n",
    "    plt.plot([vertex_norm_tracker[k][u] for k in range(len(vertex_norm_tracker))],'r.')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('$||{X_{i}^{(t+1)}-X_{i}^{(t)}}||$')\n",
    "    plt.title('Window Length: {}'.format(window_length))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345ced8-5ad6-4f19-928a-85b879e9e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in columns_sparse:\n",
    "    b = i in list(rt_usernames)\n",
    "    a.append(b)\n",
    "idx = [i for i,x in enumerate(a) if x == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521e82c-0fab-4534-86a8-e9d161cc53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation(14,vertex_mam_tracker_14,vertex_range_tracker_14,vertex_norm_tracker_14,23988)\n",
    "plot_creation(7,vertex_mam_tracker_7,vertex_range_tracker_7,vertex_norm_tracker_7,23988)\n",
    "plot_creation(5,vertex_mam_tracker_5,vertex_range_tracker_5,vertex_norm_tracker_5,23988)\n",
    "plot_creation(3,vertex_mam_tracker_3,vertex_range_tracker_3,vertex_norm_tracker_3,23988)\n",
    "plot_creation(2,vertex_mam_tracker_2,vertex_range_tracker_2,vertex_norm_tracker_2,23988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a44c78-ecdd-4dcd-97e9-8c39a979d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation(14,vertex_mam_tracker_14,vertex_range_tracker_14,vertex_norm_tracker_14,idx[0])\n",
    "plot_creation(7,vertex_mam_tracker_7,vertex_range_tracker_7,vertex_norm_tracker_7,idx[0])\n",
    "plot_creation(5,vertex_mam_tracker_5,vertex_range_tracker_5,vertex_norm_tracker_5,idx[0])\n",
    "plot_creation(3,vertex_mam_tracker_3,vertex_range_tracker_3,vertex_norm_tracker_3,idx[0])\n",
    "plot_creation(2,vertex_mam_tracker_2,vertex_range_tracker_2,vertex_norm_tracker_2,idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf754eb-b96c-4abd-9421-e1554d96d6bc",
   "metadata": {},
   "source": [
    "Final attempt where we take the mean across the entire normal range and then use that to determine if days 57-82 are anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185316d8-0d3c-4f3c-a4c0-28a9c765bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASE_anom_finder_normal(window_length=56):\n",
    "\n",
    "    vertex_norm_tracker = []\n",
    "    graph_norm_tracker = []\n",
    "\n",
    "    graph_mam_tracker = []\n",
    "    graph_range_tracker = []\n",
    "\n",
    "    vertex_mam_tracker = []\n",
    "    vertex_range_tracker = []\n",
    "\n",
    "    for t in tqdm(range(len(data_frame_list_uase)-1)):\n",
    "        u1, s1, v1 = scipy.sparse.linalg.svds(data_frame_list_uase[t],k=7)\n",
    "        u2, s2, v2 = scipy.sparse.linalg.svds(data_frame_list_uase[t+1],k=7)\n",
    "        Y1 = pd.DataFrame((v1.transpose()*s1**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "        Y2 = pd.DataFrame((v2.transpose()*s2**0.5).transpose(),columns=(list(set(columns_sparse))))\n",
    "\n",
    "        nrm = scipy.linalg.norm(Y2-Y1,ord=2)\n",
    "        graph_norm_tracker.append(nrm)\n",
    "\n",
    "        vertex_norms = []\n",
    "        for v in list(columns_sparse):\n",
    "            nrm = scipy.linalg.norm(Y2[v]-Y1[v],ord=2)\n",
    "            vertex_norms.append(nrm)\n",
    "\n",
    "        vertex_norm_tracker.append(vertex_norms)\n",
    "\n",
    "        if t == window_length:\n",
    "            graph_mam = np.sum(graph_norm_tracker[(t-window_length+1):t-1])/(window_length-1)\n",
    "            graph_range = np.sum(np.linalg.norm(np.array(graph_norm_tracker[t-window_length+2:t-1])-np.array(graph_norm_tracker[t-window_length+1:t-2])))/(1.128*(window_length-2))\n",
    "            graph_mam_tracker.append(graph_mam)\n",
    "            graph_range_tracker.append(graph_range)\n",
    "\n",
    "            vertex_mam_tracker_day = []\n",
    "            vertex_range_tracker_day = []\n",
    "\n",
    "            for j in range(len(list(columns_sparse))):\n",
    "                vertex_mam = np.sum([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_mam_tracker_day.append(vertex_mam)\n",
    "                vertex_range = np.sqrt((vertex_mam*(1-vertex_mam))/(window_length-1))\n",
    "                vertex_range_tracker_day.append(vertex_range)\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker_day)\n",
    "            vertex_range_tracker.append(vertex_range_tracker_day)\n",
    "        elif t > window_length:\n",
    "            graph_mam_tracker.append(graph_mam_tracker[0])\n",
    "            graph_range_tracker.append(graph_range_tracker[0])\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker[0])\n",
    "            vertex_range_tracker.append(vertex_range_tracker[0])\n",
    "    day_anomalies = []\n",
    "\n",
    "    for i,v in enumerate(list(columns_sparse)):\n",
    "        y_v = np.array([vertex_norm_tracker[k][i] for k in range(len(vertex_norm_tracker))])\n",
    "        ucl_v = np.array([vertex_mam_tracker[k][i] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][i] for k in range(len(vertex_range_tracker))])\n",
    "        anomalies = [j+window_length+1 for j,y in enumerate(y_v[window_length:]) if y > ucl_v[j]]\n",
    "        if len(anomalies) > 0:\n",
    "            day_anomalies.append((v,anomalies))\n",
    "        \n",
    "    return vertex_norm_tracker, graph_norm_tracker, graph_mam_tracker, graph_range_tracker, vertex_mam_tracker, vertex_range_tracker, day_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2951b2c-0d07-49cb-86a2-4e6befdf03e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099903d2135a4f96aa35115f4487979d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-87c9bcc8c996>:40: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vertex_range = np.sqrt((vertex_mam*(1-vertex_mam))/(window_length-1))\n"
     ]
    }
   ],
   "source": [
    "vertex_norm_tracker_normal, graph_norm_tracker_normal, graph_mam_tracker_normal, graph_range_tracker_normal, vertex_mam_tracker_normal, vertex_range_tracker_normal, day_anomalies_normal = ASE_anom_finder_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7d7367-a6d6-4512-b5e3-5daea30860dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_creation(56,vertex_mam_tracker_normal,vertex_range_tracker_normal,vertex_norm_tracker_normal,23991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de773d6-4270-43b0-a29c-e76554ee1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sliding window of length 56 captures 0.5263157894736842% of rt users as anomalous and 0.4967618384401114% of normal users as anomalous.\n"
     ]
    }
   ],
   "source": [
    "anomaly_count_rt(day_anomalies_normal,56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "717b60bc-30d8-4982-bcf3-ab1f53cd6f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('User384215', [57, 59, 60, 80, 83]),\n",
       " ('User682124', [60]),\n",
       " ('User280464', [57, 59, 60, 63, 65, 67, 68, 71, 72, 78, 87]),\n",
       " ('User451666', [57, 60, 65, 71, 72, 74, 87, 88]),\n",
       " ('User844991', [57, 58, 64, 87, 88]),\n",
       " ('User486765',\n",
       "  [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 84, 85, 86, 87, 88, 89]),\n",
       " ('User968259', [58, 59, 65, 66, 72, 73, 78, 80]),\n",
       " ('User736129',\n",
       "  [58, 60, 61, 66, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 80, 81]),\n",
       " ('Anonymous', [57, 59, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User015915', [57, 58, 64, 65, 66, 67, 74, 79, 81]),\n",
       " ('Comp503732$', [57, 59, 71, 72, 78, 80, 87]),\n",
       " ('User796410', [78]),\n",
       " ('User492761', [57, 72, 73, 77, 78, 80, 81, 82, 84, 85]),\n",
       " ('Comp612667$', [66, 67, 70, 71]),\n",
       " ('Comp983832$', [77, 78]),\n",
       " ('Comp571011$', [60, 67]),\n",
       " ('User902798', [58, 71]),\n",
       " ('User827052', [58, 64]),\n",
       " ('Comp296412$', [63, 64]),\n",
       " ('User278366', [78, 79]),\n",
       " ('User322352', [57, 59, 60, 65, 72, 74, 78, 87, 88]),\n",
       " ('User760196', [74, 75, 77, 78, 79, 80, 81]),\n",
       " ('User411060', [57]),\n",
       " ('User291694', [57, 59, 60, 65, 71, 72, 78, 87, 88]),\n",
       " ('User181865', [57, 60, 64, 74]),\n",
       " ('User326218',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81]),\n",
       " ('Comp212472$', [76, 77, 78, 79, 80, 81, 83, 84, 87, 88]),\n",
       " ('User036955', [57, 79, 80]),\n",
       " ('User600456', [57, 58, 66, 87]),\n",
       " ('Comp113078$',\n",
       "  [57, 58, 59, 60, 64, 65, 67, 68, 72, 73, 74, 78, 79, 84, 85, 86]),\n",
       " ('Comp276690$', [87]),\n",
       " ('User025683', [60, 64, 71, 72, 73, 74, 87, 88]),\n",
       " ('User115238', [75, 88]),\n",
       " ('Comp303229$', [64]),\n",
       " ('Comp985693$', [81, 82]),\n",
       " ('User023987', [58, 63, 64, 65, 67, 68, 70, 71, 72, 73, 75, 77, 80]),\n",
       " ('Scanner', [58, 59, 63, 64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 80, 81]),\n",
       " ('Comp170567$', [57]),\n",
       " ('Comp693138$', [81, 86, 87, 88]),\n",
       " ('Comp016710$', [57, 60, 87, 88]),\n",
       " ('Comp200205$', [60, 61]),\n",
       " ('Comp130896$', [57, 60, 71, 72, 74, 78, 88]),\n",
       " ('User276734', [58, 67]),\n",
       " ('Comp946948$', [85, 86]),\n",
       " ('User396065', [57, 58]),\n",
       " ('Comp276131$', [66]),\n",
       " ('Comp884116$', [60, 61, 67, 68]),\n",
       " ('Comp560428$', [57, 58, 59, 60, 65, 66, 70, 71]),\n",
       " ('User033825', [57, 66, 78, 80, 81, 87]),\n",
       " ('User124967', [60, 67]),\n",
       " ('Comp312776$', [62, 63, 64, 65, 72, 73, 74, 75, 76, 79, 80, 87]),\n",
       " ('User085709', [64, 65, 85, 86]),\n",
       " ('User408610', [57, 59, 60, 65, 71, 72, 78, 87, 88]),\n",
       " ('User388732', [57, 59, 60, 71, 72, 74, 78, 87, 88]),\n",
       " ('User024728', [65, 67, 71, 72, 87, 88]),\n",
       " ('User875093', [57, 60, 67, 71, 72, 74, 78, 88]),\n",
       " ('User109616', [57, 58, 66, 79]),\n",
       " ('User517044', [76]),\n",
       " ('Comp992005$', [65, 71, 72, 78, 87, 88]),\n",
       " ('Comp253114$', [58, 60, 63, 64, 65, 80]),\n",
       " ('User251851', [57, 58, 64]),\n",
       " ('User402116', [65, 67]),\n",
       " ('User773151', [63]),\n",
       " ('User089300', [60, 67, 71, 78, 83, 87, 88]),\n",
       " ('User508552',\n",
       "  [62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User626556', [64, 65, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp383893$', [61, 62]),\n",
       " ('User493761', [63, 64, 66, 67, 68, 72, 74, 82, 84, 87, 88]),\n",
       " ('User833373', [72, 73, 74]),\n",
       " ('User667009', [60, 61, 64, 65, 66, 67, 74, 75, 80, 81]),\n",
       " ('Comp568713$', [58, 59, 84, 85, 86, 87]),\n",
       " ('User335879', [57, 58, 59, 63, 64, 72, 73, 86, 87]),\n",
       " ('User162060', [78, 83]),\n",
       " ('User023014', [64]),\n",
       " ('User165990', [60, 65, 67, 80]),\n",
       " ('User219183', [77, 78, 79, 80, 81, 86, 87]),\n",
       " ('User779740', [58, 63, 64]),\n",
       " ('Comp372347$', [87, 88]),\n",
       " ('User802553', [57, 63, 64, 78, 79, 80, 85, 86]),\n",
       " ('Comp259443$', [57, 59, 60, 65, 67, 68, 71, 72, 74, 78, 87, 88, 89]),\n",
       " ('User442328', [71, 78, 87]),\n",
       " ('User158041', [65, 67, 68, 87]),\n",
       " ('Comp799175$', [57, 64, 65, 66, 67, 68, 71, 86, 87]),\n",
       " ('User507827', [67, 88]),\n",
       " ('User286674', [86, 87]),\n",
       " ('Comp755661$',\n",
       "  [57, 58, 60, 63, 64, 65, 66, 67, 71, 72, 79, 81, 82, 86, 87, 88]),\n",
       " ('User003141', [62, 87, 88]),\n",
       " ('User090946', [60, 63]),\n",
       " ('User195881', [59]),\n",
       " ('User269985', [60]),\n",
       " ('User287673',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User521285', [60, 68, 78, 87, 88]),\n",
       " ('Comp913004$', [65, 71, 87, 88]),\n",
       " ('User957693', [86, 87]),\n",
       " ('Comp211712$', [57]),\n",
       " ('User938386', [63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 77, 78, 79, 80, 81]),\n",
       " ('User695857', [58, 65, 67, 72, 73, 80, 87]),\n",
       " ('Comp737841$', [62]),\n",
       " ('User969785', [57, 60, 67, 87]),\n",
       " ('User003554',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User028618', [57, 58, 63, 64, 70, 71, 72, 79, 80]),\n",
       " ('Comp284510$', [66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81]),\n",
       " ('User832275',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User124127', [83]),\n",
       " ('User619008', [68, 83]),\n",
       " ('Comp308350$', [86, 87]),\n",
       " ('User308595', [83]),\n",
       " ('User667501', [60, 87, 88]),\n",
       " ('Comp496922$', [59, 60, 88, 89]),\n",
       " ('User249909', [57, 58, 60, 62, 63, 64, 66, 67, 68, 72, 74]),\n",
       " ('Comp447048$', [67, 87]),\n",
       " ('User385126', [66, 67, 68, 70, 71, 73, 74, 85, 86, 87]),\n",
       " ('User216338', [71]),\n",
       " ('Comp839181$', [59, 60]),\n",
       " ('Comp307122$', [66, 78, 86]),\n",
       " ('Comp681230$', [57, 59, 65, 67, 71, 72, 78, 87]),\n",
       " ('User291748', [70, 71, 74, 80]),\n",
       " ('User291995', [83]),\n",
       " ('User559100', [57, 60, 71, 72, 78, 87, 88]),\n",
       " ('Comp014603$', [57, 67, 71]),\n",
       " ('User916116', [63, 67, 83, 87, 88]),\n",
       " ('Comp938010$', [63, 64, 81, 82]),\n",
       " ('Comp841954$', [71, 72, 73, 77, 78, 79, 80, 81, 84, 86, 87, 88, 89]),\n",
       " ('User501180', [62, 63]),\n",
       " ('User501412', [65, 87]),\n",
       " ('User897131', [79, 80, 87, 88]),\n",
       " ('Comp729526$', [57, 58, 64, 66, 67, 80, 81, 86, 87, 88]),\n",
       " ('User161011', [59, 65, 67, 78, 87, 88]),\n",
       " ('User434867', [72, 73, 74]),\n",
       " ('Comp362128$', [64, 65, 71, 72, 73, 78, 79]),\n",
       " ('User038230', [57, 65]),\n",
       " ('User849884', [79, 80]),\n",
       " ('User352028', [71, 72]),\n",
       " ('User641851', [87, 88]),\n",
       " ('User426187', [57, 58, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]),\n",
       " ('User382932', [58, 60, 64, 65, 67, 71, 72]),\n",
       " ('User827203', [57, 59, 65, 67, 71, 72, 78, 87, 88]),\n",
       " ('User778361', [75, 76, 77, 78, 79, 80, 83, 84, 87, 88, 89]),\n",
       " ('User020489', [57, 58, 64, 67, 81, 87, 88]),\n",
       " ('Comp043827$', [65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User013315', [57]),\n",
       " ('User825954', [59, 60, 63, 65, 67, 71, 76, 77, 87, 88]),\n",
       " ('User407257', [57, 58, 64, 66]),\n",
       " ('Comp961343$', [63, 64]),\n",
       " ('User795633', [87]),\n",
       " ('User372837', [60, 67, 76, 77, 87]),\n",
       " ('User345224', [61, 62, 83]),\n",
       " ('User452708', [87]),\n",
       " ('User839955', [57, 65, 71, 72, 87]),\n",
       " ('User985849', [57, 59, 60, 65, 67, 87]),\n",
       " ('User504533', [64, 65, 73, 74, 80, 81, 87]),\n",
       " ('Comp975290$', [77, 78, 79, 80, 86, 87]),\n",
       " ('User426581', [57, 59, 60, 62, 63, 65, 72, 78, 87]),\n",
       " ('User574182',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User637253',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   84,\n",
       "   85]),\n",
       " ('User351238', [57, 58, 65, 68, 78]),\n",
       " ('Comp899662$', [57, 71, 72, 78, 87]),\n",
       " ('Comp105581$', [72]),\n",
       " ('User860048', [60, 63, 67, 68, 71, 83]),\n",
       " ('Comp901049$', [65, 71, 72, 78, 87, 88]),\n",
       " ('User385613', [57, 58, 64, 65, 66, 67, 68, 71, 72, 77, 78]),\n",
       " ('User894304', [60, 61, 62, 63, 65, 67, 71, 78, 87]),\n",
       " ('User104303', [72]),\n",
       " ('User952785', [78, 79, 80, 81, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp925805$', [74, 83]),\n",
       " ('User007982', [57, 65, 67, 71, 78, 87, 88]),\n",
       " ('User971758',\n",
       "  [57,\n",
       "   59,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   84,\n",
       "   87]),\n",
       " ('User380206', [64, 65, 67, 80]),\n",
       " ('User139725',\n",
       "  [57, 58, 60, 63, 64, 65, 66, 67, 74, 79, 81, 82, 84, 86, 87, 88]),\n",
       " ('User763485', [60, 83, 88]),\n",
       " ('Comp086663$', [87]),\n",
       " ('Comp414471$', [64, 65, 80, 81, 87, 88]),\n",
       " ('User080370', [61, 62]),\n",
       " ('Comp735222$', [84, 86, 87, 88, 89]),\n",
       " ('Comp435376$', [85, 86]),\n",
       " ('User970013', [57, 60, 67, 87]),\n",
       " ('User867875', [63, 76, 77]),\n",
       " ('Comp521821$', [58, 60, 67, 71, 80]),\n",
       " ('User546487',\n",
       "  [57, 58, 60, 64, 65, 66, 67, 71, 72, 79, 80, 81, 83, 84, 86, 87, 88]),\n",
       " ('Comp514928$',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User712897', [60, 88]),\n",
       " ('User788931', [71, 72, 80, 81]),\n",
       " ('Comp954789$',\n",
       "  [58,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   72,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp823541$', [66, 67, 72, 84, 85, 87, 88]),\n",
       " ('User815098', [67]),\n",
       " ('User453580', [65, 67, 71, 72, 78]),\n",
       " ('Comp328244$',\n",
       "  [58, 60, 64, 65, 67, 68, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84]),\n",
       " ('Comp084803$', [84, 85]),\n",
       " ('User527260', [58, 62, 63, 64, 65, 66, 67, 70, 71, 75, 76, 77, 78, 79, 80]),\n",
       " ('User529426', [59, 65, 74, 76, 83]),\n",
       " ('User621716', [75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89]),\n",
       " ('User462620', [61, 87]),\n",
       " ('User877846',\n",
       "  [58,\n",
       "   60,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   67,\n",
       "   72,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User051549', [83]),\n",
       " ('User211156', [61]),\n",
       " ('User104998', [79, 80, 81, 82]),\n",
       " ('User452941',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   87,\n",
       "   88]),\n",
       " ('User199463', [67]),\n",
       " ('Comp310268$', [87]),\n",
       " ('User951500', [67]),\n",
       " ('User873903', [57, 58, 63, 64, 65, 67, 68, 71, 72, 78, 87, 88]),\n",
       " ('Comp940027$', [66, 67]),\n",
       " ('User232605', [63, 64, 74, 75, 76, 84, 85]),\n",
       " ('User945564', [60, 63, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User997660', [58, 64, 65, 67, 80]),\n",
       " ('User442876', [58, 59, 60, 64, 65, 73, 74, 75, 78, 79, 87, 88, 89]),\n",
       " ('User476473', [83, 87]),\n",
       " ('User911003', [76, 77]),\n",
       " ('User055660', [80, 83]),\n",
       " ('Comp355169$',\n",
       "  [57,\n",
       "   59,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   81,\n",
       "   82,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User411492', [60, 61, 62, 67, 71, 78, 87, 88]),\n",
       " ('Comp095673$', [85, 86]),\n",
       " ('Comp947788$', [60, 61, 88]),\n",
       " ('User179950', [63]),\n",
       " ('User443441', [72, 73]),\n",
       " ('Comp371860$', [59, 60]),\n",
       " ('User790892', [57, 77, 78]),\n",
       " ('User474013', [64, 65, 72, 73, 79]),\n",
       " ('Comp527729$', [83]),\n",
       " ('User147906', [59, 65, 67, 78, 87]),\n",
       " ('User950865', [75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89]),\n",
       " ('User454866', [72]),\n",
       " ('User480439',\n",
       "  [70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User082522', [78, 83]),\n",
       " ('User311556', [67, 68, 83]),\n",
       " ('User290880', [57, 58, 79, 87, 88]),\n",
       " ('Comp339223$', [60, 88]),\n",
       " ('User442825', [79, 80, 81, 82]),\n",
       " ('User322077',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User616490', [72, 73]),\n",
       " ('Comp709070$', [57, 59, 60, 61, 62, 63, 65, 67, 68, 71, 74, 83]),\n",
       " ('User598119', [63, 64, 65, 66, 67, 68, 69, 70, 71]),\n",
       " ('Comp794515$', [57, 74, 75]),\n",
       " ('Comp698292$', [74, 76, 83]),\n",
       " ('User592191', [57, 59, 60, 65, 67, 71, 72, 74, 87, 88]),\n",
       " ('Comp094099$', [57, 65, 71, 72, 78]),\n",
       " ('User039298', [58, 64, 78, 81]),\n",
       " ('User261674',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User733798', [57, 58]),\n",
       " ('Comp519978$', [57, 65, 67, 71, 72, 87]),\n",
       " ('User777510', [57, 58, 71, 72, 73, 74, 77, 78, 87, 88]),\n",
       " ('User252309', [60, 61, 62, 63, 65, 67, 68, 71, 74, 88]),\n",
       " ('User660674', [88, 89]),\n",
       " ('Comp509237$', [67]),\n",
       " ('User018262', [72, 73]),\n",
       " ('User191175', [59, 60, 64, 65, 67, 68, 84]),\n",
       " ('User514222', [64, 65, 66]),\n",
       " ('Comp895797$',\n",
       "  [57, 58, 60, 63, 64, 66, 67, 68, 75, 76, 77, 78, 79, 81, 86, 87, 88]),\n",
       " ('Comp484644$', [72, 74]),\n",
       " ('Comp213123$',\n",
       "  [64, 65, 66, 67, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89]),\n",
       " ('User348137', [83]),\n",
       " ('User177073', [60, 63, 67, 71, 88]),\n",
       " ('Comp074888$', [68]),\n",
       " ('User511284', [57, 65, 67, 87]),\n",
       " ('User205727', [57]),\n",
       " ('User915766', [67, 68, 71, 72, 77, 78, 86, 87]),\n",
       " ('User990904',\n",
       "  [57, 58, 59, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 85]),\n",
       " ('User037993', [67, 88]),\n",
       " ('User408074', [58]),\n",
       " ('Comp940295$', [67, 71, 72, 78, 87]),\n",
       " ('User219760', [73, 74, 75, 76, 78, 79, 80]),\n",
       " ('User923659', [77, 78, 79, 80, 88, 89]),\n",
       " ('Comp408079$', [87]),\n",
       " ('Comp642557$', [65, 66, 71, 72, 73, 79, 80, 81, 87, 88, 89]),\n",
       " ('User930318', [64, 65, 67, 71, 73, 74, 78]),\n",
       " ('User935119', [65, 66, 67, 68, 69, 70]),\n",
       " ('User214594', [85, 86]),\n",
       " ('Comp760017$', [65, 66, 67, 82, 83, 84, 85, 86]),\n",
       " ('Comp715721$', [59, 60, 65, 67, 72, 74, 87, 88]),\n",
       " ('Comp779859$', [65, 67, 71, 72, 78, 87]),\n",
       " ('User857572', [57, 65, 71, 87]),\n",
       " ('User882755', [57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 78, 83, 87]),\n",
       " ('User422875',\n",
       "  [64, 65, 66, 67, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp324399$', [87, 89]),\n",
       " ('Comp119617$', [57, 59, 60, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User195256', [78, 83, 87]),\n",
       " ('User449100', [77, 78]),\n",
       " ('Comp046216$', [78, 79, 80, 81, 86, 87]),\n",
       " ('User913501', [60, 63, 67]),\n",
       " ('Comp480120$', [67]),\n",
       " ('User125943', [57, 58, 60, 62, 76, 78, 80, 81, 83, 87, 88]),\n",
       " ('Comp618493$', [58, 59, 60, 61, 62, 63, 64, 65]),\n",
       " ('User651501', [58, 64]),\n",
       " ('User181140', [57]),\n",
       " ('User673640', [57, 65, 67, 71, 78, 83]),\n",
       " ('User820082', [57, 58, 60, 61, 62, 63, 64, 65]),\n",
       " ('User424524', [64, 65, 70, 71, 79, 80]),\n",
       " ('Comp993654$', [57, 58, 64, 66, 71, 72, 78, 79, 80, 81, 86, 87]),\n",
       " ('Comp327587$', [64]),\n",
       " ('User195766', [60, 87, 88]),\n",
       " ('Comp229842$', [66, 67]),\n",
       " ('User403510', [85, 86]),\n",
       " ('User867381', [59, 60, 64, 65, 67, 68, 72, 73, 86, 87]),\n",
       " ('Comp553152$', [57, 59, 60, 63, 65, 67, 68, 78, 87, 88]),\n",
       " ('Comp567435$', [87, 88]),\n",
       " ('Comp515823$', [60, 63]),\n",
       " ('User203227', [80, 81]),\n",
       " ('User230494', [58, 60, 63, 64, 65, 66, 67, 73, 74, 77, 78, 80, 81, 82]),\n",
       " ('User269876', [87, 88, 89]),\n",
       " ('User481911', [63, 67, 68]),\n",
       " ('User539544',\n",
       "  [71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp614355$', [57, 71, 72, 87, 88]),\n",
       " ('User018098', [57, 60, 61, 62, 63, 70, 71, 72, 73]),\n",
       " ('User353410', [60, 63, 64, 65, 67, 87]),\n",
       " ('Comp885407$', [65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76]),\n",
       " ('Comp413862$', [71, 87, 88]),\n",
       " ('User946597', [60, 63, 67, 68, 87, 88]),\n",
       " ('User582850', [57, 60, 65, 71, 74, 78, 87]),\n",
       " ('Comp861545$',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User868987', [65, 67, 71, 80, 81]),\n",
       " ('User906791', [74, 80, 87]),\n",
       " ('User326831', [66, 67]),\n",
       " ('User844533', [65, 66, 70, 71, 72]),\n",
       " ('User011772', [57, 87, 88]),\n",
       " ('User684358', [65, 67, 71, 72, 78, 83, 87]),\n",
       " ('User244825', [58, 59, 63, 65, 67, 71, 72, 74, 78, 83, 87]),\n",
       " ('User153948', [58, 59, 66, 67, 70, 71, 72, 77, 78, 79]),\n",
       " ('Comp003688$', [58, 59, 60, 61, 64, 65, 67, 75, 76, 80, 81, 82, 83]),\n",
       " ('Comp793844$', [73, 74, 80]),\n",
       " ('User590954', [63]),\n",
       " ('User447630', [64, 65, 67]),\n",
       " ('Comp358250$', [57, 60, 65, 71, 78, 87, 88]),\n",
       " ('User119737', [66, 67, 68]),\n",
       " ('Comp461980$', [67, 68, 70, 71, 72, 74]),\n",
       " ('User666860', [63, 64]),\n",
       " ('User538126', [67]),\n",
       " ('Comp833053$', [71, 72]),\n",
       " ('User560414', [58, 59, 60, 63, 64, 66, 67, 78, 80, 81, 87, 88]),\n",
       " ('Comp965673$', [64, 65]),\n",
       " ('Comp384119$', [60, 61, 62, 63, 67, 68, 71, 74]),\n",
       " ('User803021', [60, 61, 64, 65, 67]),\n",
       " ('User190795', [87]),\n",
       " ('User678570', [57, 59, 71, 72, 78, 87]),\n",
       " ('User886182', [66, 67, 71, 72, 74, 75, 80, 81]),\n",
       " ('Comp853260$', [64, 67, 68, 72, 74, 80, 81]),\n",
       " ('Comp796065$',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User098652', [74, 77, 83, 84, 87]),\n",
       " ('User622146', [57]),\n",
       " ('User832836', [63, 64, 67, 68, 70, 71, 72, 79, 80, 81, 84, 86, 87, 88]),\n",
       " ('User779135', [58, 89]),\n",
       " ('User254551', [85, 86, 87, 88, 89]),\n",
       " ('User401773', [63]),\n",
       " ('User367957', [60, 88]),\n",
       " ('Comp497190$', [59, 60, 65, 66]),\n",
       " ('Comp306919$', [59, 60, 65, 67, 76, 77, 78, 88, 89]),\n",
       " ('User718825', [58, 67, 68]),\n",
       " ('User175187', [57, 58, 64, 65]),\n",
       " ('Comp154101$', [58, 59, 72, 73, 74, 75]),\n",
       " ('User351277', [60, 63, 71, 78, 87, 88]),\n",
       " ('User397253', [57]),\n",
       " ('Comp731083$', [60, 71]),\n",
       " ('User830673', [60, 65, 71, 78, 87, 88]),\n",
       " ('User299355', [57, 59, 60, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User140215', [65]),\n",
       " ('Comp847747$', [87]),\n",
       " ('User388291', [78, 80]),\n",
       " ('User063256', [86, 87, 88]),\n",
       " ('Comp812793$', [58, 63, 64, 65, 67, 75, 76, 77, 80, 83, 84]),\n",
       " ('Comp526662$', [67, 68, 71]),\n",
       " ('Comp352652$', [83]),\n",
       " ('User724397', [57, 59, 60, 65, 71, 74, 78, 87, 88]),\n",
       " ('User154180', [63]),\n",
       " ('Comp642252$', [57, 58, 60, 64, 67, 74, 79, 81, 87, 88]),\n",
       " ('Comp817584$', [83]),\n",
       " ('User803491', [60, 67, 68, 71, 78, 87, 88]),\n",
       " ('Comp811647$', [63]),\n",
       " ('Comp449872$', [60]),\n",
       " ('User202803', [58, 67]),\n",
       " ('User980348', [60, 65, 71, 78, 87, 88]),\n",
       " ('Comp038950$', [60, 67]),\n",
       " ('Comp299853$', [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]),\n",
       " ('Comp734923$', [57, 58]),\n",
       " ('User209304',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp324833$', [63, 83]),\n",
       " ('Comp679648$', [87, 88, 89]),\n",
       " ('User067230', [71, 72, 73, 74]),\n",
       " ('User982478', [65]),\n",
       " ('User135713', [60, 63]),\n",
       " ('User312747', [65, 67, 83, 89]),\n",
       " ('User369977', [57, 58, 60, 79, 80, 81, 83, 87, 88, 89]),\n",
       " ('User764120', [60, 78, 87]),\n",
       " ('User208451', [81, 83, 86, 87]),\n",
       " ('User600951', [57, 60, 63, 76, 83]),\n",
       " ('User827923', [65, 71, 74, 78, 87, 88]),\n",
       " ('Comp668800$', [67]),\n",
       " ('User098534', [57, 60, 78, 87]),\n",
       " ('User248755', [57, 58, 59, 60, 63, 64, 65, 66, 72, 73, 87]),\n",
       " ('User524701', [60, 67, 71, 87]),\n",
       " ('Comp831222$', [57, 64, 65, 67, 71, 72, 88]),\n",
       " ('User387124', [75]),\n",
       " ('Comp834776$', [63]),\n",
       " ('Comp528070$', [57]),\n",
       " ('User218994',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User176730', [62, 67]),\n",
       " ('User236478', [58, 64]),\n",
       " ('User729874', [67]),\n",
       " ('Comp220825$', [79, 80, 85, 86, 87, 88, 89]),\n",
       " ('Comp567642$', [67, 88]),\n",
       " ('Comp378365$', [64, 81, 87, 88, 89]),\n",
       " ('Comp070002$', [58, 60, 63, 64, 65, 75, 76, 77, 80]),\n",
       " ('User050532', [60]),\n",
       " ('Comp999606$', [57, 59, 65, 67, 68, 71, 72, 74, 87]),\n",
       " ('User538831',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp450533$', [60, 63, 67]),\n",
       " ('User079067',\n",
       "  [58, 60, 61, 62, 63, 65, 67, 68, 69, 72, 73, 79, 80, 81, 83, 84]),\n",
       " ('User567566', [63]),\n",
       " ('User887044', [78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89]),\n",
       " ('User006926', [63, 64]),\n",
       " ('Comp491397$', [64, 65, 66, 67, 70, 71, 72, 73, 77, 78, 87, 88]),\n",
       " ('Comp646726$', [66, 67]),\n",
       " ('User800600', [65, 67, 71, 72, 78, 87]),\n",
       " ('User955165', [57, 60, 65, 67, 78, 87, 88]),\n",
       " ('Comp282613$',\n",
       "  [65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User907104', [87]),\n",
       " ('User592445', [70, 71, 72, 77, 78]),\n",
       " ('Comp178241$', [58, 64]),\n",
       " ('Comp176743$', [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 76, 77]),\n",
       " ('Comp432007$',\n",
       "  [60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86]),\n",
       " ('Comp023150$', [58, 63, 64, 65, 67, 68, 71, 72, 73, 80, 81]),\n",
       " ('User504749', [57, 62, 63, 67, 78, 83]),\n",
       " ('Comp568003$', [60]),\n",
       " ('User071404', [57, 58]),\n",
       " ('Comp438842$', [79, 80]),\n",
       " ('User822399', [71, 87]),\n",
       " ('User026984', [83]),\n",
       " ('Comp396773$', [78, 83, 87, 88]),\n",
       " ('Comp310034$', [74, 75]),\n",
       " ('Comp040151$', [57, 59, 65, 67, 71, 78, 87]),\n",
       " ('User747145', [89]),\n",
       " ('User439703', [63, 64, 65, 66, 67, 68, 77, 78]),\n",
       " ('Comp068881$', [75]),\n",
       " ('User679549', [74, 75, 80, 81]),\n",
       " ('User054962', [57, 58, 78, 79, 85, 86]),\n",
       " ('Comp487358$', [57, 65, 66, 67, 70, 71, 72, 73, 74, 75, 76, 77, 78]),\n",
       " ('User616089', [86, 87, 88]),\n",
       " ('User525218', [78, 87]),\n",
       " ('User264954', [77, 78, 84, 85]),\n",
       " ('User622656', [60, 67]),\n",
       " ('Comp932452$', [59, 60, 86, 87]),\n",
       " ('Comp979943$', [64]),\n",
       " ('Comp438982$', [81]),\n",
       " ('User783656', [62, 63, 67, 71, 78, 83, 87]),\n",
       " ('User220102',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp478064$', [59, 60, 61, 68, 69, 70, 71, 77, 78]),\n",
       " ('User285866', [57, 58, 60, 63, 64, 78]),\n",
       " ('User238726',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp165339$', [60, 61]),\n",
       " ('User400594', [58, 64]),\n",
       " ('User835863', [84, 85, 86, 87]),\n",
       " ('User372208', [78, 79]),\n",
       " ('User501371', [65, 67, 87, 88]),\n",
       " ('Comp522072$', [78, 79, 80, 81, 83, 86, 87, 88, 89]),\n",
       " ('Comp274438$', [64, 78]),\n",
       " ('User494874', [70, 71, 72, 74, 75, 84, 85, 86, 87]),\n",
       " ('User874695', [66, 67]),\n",
       " ('Comp707817$', [60, 63, 87]),\n",
       " ('User623515', [75, 76, 78, 79, 80, 81, 83, 87, 88, 89]),\n",
       " ('Comp967398$', [68]),\n",
       " ('User117330', [83]),\n",
       " ('Comp172540$', [57]),\n",
       " ('User729911', [57, 58, 60, 61, 64, 65, 66, 67, 68, 74, 79, 82, 86, 87, 88]),\n",
       " ('User586791', [65, 72, 87]),\n",
       " ('User180336', [65, 67, 71, 76, 78, 80, 83]),\n",
       " ('User341843', [60, 67, 74, 88]),\n",
       " ('Comp753366$', [67, 71, 78, 87]),\n",
       " ('User921873', [57, 58, 64, 66, 79, 88]),\n",
       " ('User900926', [88]),\n",
       " ('User742603', [57, 58]),\n",
       " ('User124765', [84, 85]),\n",
       " ('User848220', [79, 80]),\n",
       " ('Comp197187$', [58, 60, 63, 64, 65, 67, 68, 77, 78, 80, 81, 82, 83]),\n",
       " ('Comp518595$', [66, 71, 79, 86, 88]),\n",
       " ('User324575',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User221924', [63, 67, 68]),\n",
       " ('Comp184469$', [57, 59, 60, 71, 72, 87, 88]),\n",
       " ('User175427', [57, 58, 63, 64, 65, 71, 72, 80, 83]),\n",
       " ('User457678', [58, 64]),\n",
       " ('User024817', [61, 62, 63, 87]),\n",
       " ('User896943', [83]),\n",
       " ('Comp547882$', [58, 60, 63, 64, 65, 66, 67, 77, 78, 79, 80, 81]),\n",
       " ('User321714', [85, 86]),\n",
       " ('Comp535288$', [58, 60, 64, 65, 71, 72, 74, 78, 80, 87]),\n",
       " ('User142760', [73, 74, 78, 79, 80, 81]),\n",
       " ('User107654', [57, 59, 65, 67, 71, 78, 87]),\n",
       " ('Comp479711$', [71, 72, 77, 78, 79, 80, 81, 82, 84, 85, 87, 88]),\n",
       " ('Comp060584$', [88]),\n",
       " ('User795072',\n",
       "  [57,\n",
       "   58,\n",
       "   64,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User107880', [66, 67, 86, 87]),\n",
       " ('User514203', [79, 80, 81, 84, 85, 86, 87, 88, 89]),\n",
       " ('User744327', [87, 88]),\n",
       " ('User428733', [71, 72, 73]),\n",
       " ('User261791', [57, 63, 64, 65, 66, 67, 72, 73, 77, 78, 86, 87]),\n",
       " ('User001516', [57, 60, 65, 76, 78, 87]),\n",
       " ('User710586', [66, 67, 68, 74, 75]),\n",
       " ('User259774', [81, 82]),\n",
       " ('Comp256209$', [65, 67, 68, 70, 71, 72, 73, 79, 80]),\n",
       " ('User843663', [67, 80]),\n",
       " ('User853977', [60, 67, 71, 74, 87]),\n",
       " ('Comp212554$', [58, 64, 65, 78, 80]),\n",
       " ('User004104', [64, 65]),\n",
       " ('Comp263447$',\n",
       "  [63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User949482', [64, 65, 67, 80, 89]),\n",
       " ('User593020',\n",
       "  [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('User573915', [57, 64, 87]),\n",
       " ('User996182', [57, 58, 66, 67]),\n",
       " ('Comp016367$',\n",
       "  [66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User602878',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User319164', [74, 75]),\n",
       " ('User566633', [60, 61, 71]),\n",
       " ('Comp216310$',\n",
       "  [63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User707774', [57, 60]),\n",
       " ('User345110', [70, 71]),\n",
       " ('User840883', [71, 74, 87, 88]),\n",
       " ('User451411', [57, 58, 72, 86, 88]),\n",
       " ('Comp344695$', [78, 79]),\n",
       " ('User010356', [79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp286162$', [66, 67, 70, 71, 78, 79, 80, 81, 82]),\n",
       " ('User381544',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User420344', [72]),\n",
       " ('User882297', [57]),\n",
       " ('User189536', [70, 71]),\n",
       " ('User632422', [67]),\n",
       " ('User039182', [60, 71, 72, 74, 76, 78, 83, 87, 88]),\n",
       " ('User593136', [58]),\n",
       " ('User300622', [58, 60, 62, 63, 64, 67, 72, 74, 87, 88]),\n",
       " ('User129531',\n",
       "  [59,\n",
       "   60,\n",
       "   61,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp273141$', [57, 58, 63, 64, 75]),\n",
       " ('Comp823722$', [60, 63]),\n",
       " ('User559188',\n",
       "  [60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User540518', [60, 61, 62, 63, 84, 85]),\n",
       " ('User636921', [68, 87]),\n",
       " ('Comp962287$', [57, 58, 60, 62, 63, 64, 66, 67, 68, 86, 87, 88]),\n",
       " ('User275515', [58, 60, 63, 64, 65, 67, 70, 71, 72, 73, 77, 80, 81]),\n",
       " ('Comp918239$', [87]),\n",
       " ('User715003', [60, 62]),\n",
       " ('User971564',\n",
       "  [64,\n",
       "   65,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88]),\n",
       " ('User232519', [78, 86, 87, 88]),\n",
       " ('Comp426187$', [57]),\n",
       " ('User069116', [78]),\n",
       " ('User898578', [87]),\n",
       " ('Comp579056$', [57, 58, 64, 67, 78, 81, 87]),\n",
       " ('User070851',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User132248', [63, 87, 88]),\n",
       " ('User287092', [57, 67, 78]),\n",
       " ('User884630', [67, 68, 75, 76, 77, 78, 79, 80, 81, 83]),\n",
       " ('Comp895966$', [57, 58, 60, 64, 65, 66, 74, 79, 88]),\n",
       " ('User135416', [86, 87, 88, 89]),\n",
       " ('Comp496131$', [87]),\n",
       " ('User987548', [64, 65, 67, 76, 80, 83]),\n",
       " ('User769693',\n",
       "  [64, 65, 66, 67, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 85, 86, 87]),\n",
       " ('Comp958935$', [71, 72, 77, 78]),\n",
       " ('User312719', [57, 58, 59, 60, 61, 66, 67, 68, 69, 71, 72, 73]),\n",
       " ('User569311', [57, 64, 65, 66, 67, 79, 87, 88]),\n",
       " ('User782912',\n",
       "  [57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 78, 80, 81]),\n",
       " ('Comp141954$',\n",
       "  [57, 58, 60, 63, 64, 65, 66, 67, 68, 72, 73, 74, 77, 78, 80, 81, 82]),\n",
       " ('User513132', [65, 76, 89]),\n",
       " ('User490250', [60]),\n",
       " ('User067318',\n",
       "  [58, 60, 63, 64, 65, 67, 68, 71, 72, 73, 75, 76, 77, 80, 81, 82, 83, 84]),\n",
       " ('User155824', [62]),\n",
       " ('User608472', [58, 64, 67, 71, 80, 81, 86, 87]),\n",
       " ('User067891', [64, 88]),\n",
       " ('User665358',\n",
       "  [57, 58, 60, 62, 63, 64, 66, 67, 68, 72, 74, 75, 76, 77, 78, 79]),\n",
       " ('User893612', [63, 67]),\n",
       " ('Comp142503$', [59, 60, 61, 62, 63, 68, 76, 78, 83, 87]),\n",
       " ('User148555', [89]),\n",
       " ('User866083',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User849485', [85, 86, 87, 88, 89]),\n",
       " ('Comp122029$', [57, 59, 60, 65, 71, 72, 74, 87, 88]),\n",
       " ('User654053', [64, 65, 67, 71, 78, 80]),\n",
       " ('Comp457871$', [67, 78, 87, 88]),\n",
       " ('User866613', [58, 60, 62, 63, 64, 67, 72, 74, 76, 80, 81, 86, 87, 88, 89]),\n",
       " ('User242513', [58, 64, 65, 71]),\n",
       " ('User381853', [58, 60, 64, 65, 67, 68, 75, 80, 81, 82, 83, 84]),\n",
       " ('User913162',\n",
       "  [57, 58, 60, 62, 63, 64, 65, 67, 74, 76, 78, 80, 81, 86, 87, 88]),\n",
       " ('Comp998743$', [71]),\n",
       " ('Comp786561$', [63, 65, 68, 74]),\n",
       " ('User316580', [58, 60, 62, 63, 64, 67, 74, 76, 80, 81, 88]),\n",
       " ('Comp881322$', [65, 66, 67, 68, 70, 71, 72, 73, 74]),\n",
       " ('Comp101860$', [64, 66, 67, 68, 79, 80, 81, 86, 87, 88]),\n",
       " ('Comp740278$', [83]),\n",
       " ('User302481', [80, 81]),\n",
       " ('User380933', [67, 71, 78, 87]),\n",
       " ('User873613', [87]),\n",
       " ('Comp604557$', [86, 87]),\n",
       " ('User723934', [57, 58, 59, 60, 71, 72]),\n",
       " ('User864375',\n",
       "  [58, 60, 63, 64, 65, 67, 68, 71, 75, 76, 77, 80, 81, 82, 83, 84]),\n",
       " ('User725221', [57, 58, 59, 65, 66, 67, 68, 71, 72, 73, 74]),\n",
       " ('Comp726231$', [58, 64, 66, 79]),\n",
       " ('User628768', [77, 78, 86, 87]),\n",
       " ('User991875', [71, 87]),\n",
       " ('Comp393623$', [66, 72, 76, 77]),\n",
       " ('User224088', [60, 88]),\n",
       " ('Comp487162$',\n",
       "  [63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User575683', [88]),\n",
       " ('Comp546437$', [87]),\n",
       " ('User433141', [60, 61, 63, 64]),\n",
       " ('Comp406243$', [63, 72, 88]),\n",
       " ('User500596', [60, 61, 67, 68, 74, 83, 88, 89]),\n",
       " ('Comp900685$', [60, 88, 89]),\n",
       " ('User506232', [72]),\n",
       " ('Comp300575$', [83]),\n",
       " ('User635515', [64, 65]),\n",
       " ('Comp806235$', [79, 80, 81]),\n",
       " ('User039148', [57, 77, 78, 86, 87, 88]),\n",
       " ('User089730', [58, 63, 64, 65, 67, 74, 76, 85]),\n",
       " ('User778270', [60, 61, 65, 68, 76, 77, 89]),\n",
       " ('User006443',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83]),\n",
       " ('User005646', [78, 79]),\n",
       " ('User796623', [57, 78, 87]),\n",
       " ('Comp296276$', [87]),\n",
       " ('Comp492180$', [58, 59, 65, 66]),\n",
       " ('User710812',\n",
       "  [57, 58, 60, 62, 63, 64, 66, 67, 68, 75, 76, 77, 78, 79, 80, 86, 87]),\n",
       " ('User766250', [60, 63]),\n",
       " ('Comp637769$', [57, 58, 63, 64, 74, 75]),\n",
       " ('Comp468862$', [67, 68, 80, 81, 84, 85, 87, 88]),\n",
       " ('User583438', [57, 66, 67, 78, 79, 80, 86]),\n",
       " ('User754295', [58, 64, 65, 67, 80, 81]),\n",
       " ('User729157', [63, 67]),\n",
       " ('User515934', [58, 67, 87]),\n",
       " ('Comp436081$', [58, 60, 64, 65, 66, 71, 74, 78, 87, 88]),\n",
       " ('Comp594125$', [65]),\n",
       " ('User076941', [83]),\n",
       " ('User608134', [59, 60]),\n",
       " ('User719573', [84, 85]),\n",
       " ('User741247', [57, 58, 59, 60, 79, 80]),\n",
       " ('User892983', [70, 71, 72, 73, 74, 75, 76, 77, 84, 85, 86, 88, 89]),\n",
       " ('User712299', [80, 81]),\n",
       " ('Comp792241$', [57, 65, 68, 78]),\n",
       " ('User331469', [57, 58, 60, 63, 64, 65, 66, 67, 79, 80, 81, 86, 87, 88]),\n",
       " ('User821508', [59, 60, 77, 78]),\n",
       " ('User302297', [64, 65, 67]),\n",
       " ('User209951', [60, 87, 88]),\n",
       " ('User268654', [66, 67]),\n",
       " ('Comp534252$', [84, 85]),\n",
       " ('User553543', [60]),\n",
       " ('Comp202912$', [74, 87, 88]),\n",
       " ('User342691', [57, 58, 64]),\n",
       " ('User256419', [58]),\n",
       " ('Comp658103$', [58, 59, 72]),\n",
       " ('User858442', [60, 62, 63, 68, 80]),\n",
       " ('Comp029081$', [57, 58, 66, 78, 79, 80, 86]),\n",
       " ('User387697', [60, 66, 79, 80, 81]),\n",
       " ('User695317', [57, 72, 73, 74, 75, 76, 77, 78, 79, 80]),\n",
       " ('User995995', [77, 78, 88]),\n",
       " ('Comp845368$', [62, 87]),\n",
       " ('User927980', [57, 60, 71, 83, 87]),\n",
       " ('User781306', [60]),\n",
       " ('User460067', [64, 65]),\n",
       " ('User294954', [57, 58, 78, 81, 88]),\n",
       " ('User393358', [70, 71]),\n",
       " ('User211743', [58, 60, 64, 65, 67, 80, 81]),\n",
       " ('User512491',\n",
       "  [63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 78, 79, 81, 82, 86, 87]),\n",
       " ('User627024', [73, 74]),\n",
       " ('User417027', [63, 64, 88, 89]),\n",
       " ('Comp417869$', [80, 81, 87, 88]),\n",
       " ('Comp420310$', [57]),\n",
       " ('Comp519634$', [57, 59, 60, 65, 72, 74, 78, 87, 88]),\n",
       " ('User723903',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76]),\n",
       " ('User594390', [60, 87]),\n",
       " ('User558012', [57, 60, 65, 78, 87, 88]),\n",
       " ('User185800', [57, 65, 67, 78, 87]),\n",
       " ('User589043', [70, 71]),\n",
       " ('User933680', [57, 58, 60, 70, 71, 72, 73, 74]),\n",
       " ('User039439', [70, 71]),\n",
       " ('User960372', [63]),\n",
       " ('User779198', [64, 65, 87]),\n",
       " ('User705770', [60, 61]),\n",
       " ('Comp916101$', [77, 78, 88, 89]),\n",
       " ('User120569', [64, 77, 78, 80]),\n",
       " ('User634347', [58, 60, 63, 64, 67, 74]),\n",
       " ('User417389', [59, 60]),\n",
       " ('User953479',\n",
       "  [63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User281813', [65, 67, 87, 88]),\n",
       " ('User117435', [79, 80, 87, 88]),\n",
       " ('User618447', [65, 67, 89]),\n",
       " ('User481692', [57, 58, 60, 63, 64, 66, 67, 72, 74, 75, 81, 86, 87]),\n",
       " ('User525621', [65, 67, 71, 72, 78, 87]),\n",
       " ('User307261', [58, 60, 64, 65, 69, 70, 75, 76, 77, 80, 81]),\n",
       " ('User929309', [66, 67, 68]),\n",
       " ('User616842', [60, 71, 78, 83, 87, 88]),\n",
       " ('User434349', [57, 71, 80, 89]),\n",
       " ('User822781', [58, 60, 63, 64, 65, 66, 67, 70, 71, 72, 73, 80, 81]),\n",
       " ('User224120', [58, 65, 67, 72]),\n",
       " ('User647279', [57, 59, 60, 61, 85, 86]),\n",
       " ('User894159',\n",
       "  [57, 58, 60, 62, 63, 64, 66, 67, 68, 72, 74, 75, 81, 83, 84, 86]),\n",
       " ('User495390', [66, 67, 69, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82]),\n",
       " ('User421162', [72, 74, 86, 87]),\n",
       " ('Comp956722$', [59, 60, 65, 67, 71, 72, 78, 81]),\n",
       " ('Comp584214$', [59, 65, 67, 71, 72, 78, 87]),\n",
       " ('User794004', [59, 64, 65, 67, 73, 76, 80]),\n",
       " ('User563048', [60, 83]),\n",
       " ('Comp508182$', [59, 60]),\n",
       " ('User922310', [57, 78, 81]),\n",
       " ('User212062', [57, 59, 71, 72, 87]),\n",
       " ('User266426',\n",
       "  [65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User869255', [57, 60, 65, 71, 72, 74, 78, 83, 87, 88]),\n",
       " ('User807649', [57, 58, 59, 60, 61, 63, 64, 65, 66, 72, 73, 74]),\n",
       " ('User904906', [58, 62, 63, 64, 65, 67, 68, 71, 76, 80, 83, 84, 89]),\n",
       " ('User444779', [78, 87]),\n",
       " ('User512806', [57, 58, 59, 60, 65, 66, 67, 78, 79, 81, 82, 86, 87]),\n",
       " ('Comp780947$', [57, 65, 67, 71, 72, 78, 87]),\n",
       " ('Comp835750$', [57, 58, 64, 66]),\n",
       " ('User053432', [57]),\n",
       " ('Comp506128$', [65]),\n",
       " ('User579503', [67, 80]),\n",
       " ('Comp989307$', [73, 74]),\n",
       " ('Comp342866$', [57]),\n",
       " ('User255103',\n",
       "  [64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User491929', [64, 67, 88]),\n",
       " ('User048369', [63, 64, 65, 67, 68, 70, 71, 72, 73, 77, 80, 81, 88, 89]),\n",
       " ('Comp658570$', [62]),\n",
       " ('User460887', [87, 88, 89]),\n",
       " ('User656484', [78, 79]),\n",
       " ('User617912',\n",
       "  [58,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84]),\n",
       " ('User787968', [65]),\n",
       " ('User790331', [60, 63, 65, 67, 71]),\n",
       " ('User120738', [58, 60, 73, 74]),\n",
       " ('Comp688526$', [71, 72, 87, 88]),\n",
       " ('User214782', [78]),\n",
       " ('Comp145652$', [58, 65, 67, 71, 80, 88]),\n",
       " ('Comp694428$', [67]),\n",
       " ('User085602', [74]),\n",
       " ('User192765', [63, 64]),\n",
       " ('User230495', [57, 58, 64, 81, 86, 87]),\n",
       " ('User044808',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87]),\n",
       " ('User688332', [64, 65, 77, 78, 87, 88]),\n",
       " ('Comp518665$', [58, 64, 87]),\n",
       " ('Comp533747$', [87, 88]),\n",
       " ('Comp560842$', [57, 65, 67, 71, 72, 78, 87]),\n",
       " ('User592122',\n",
       "  [57, 58, 60, 63, 64, 66, 67, 75, 77, 78, 79, 80, 81, 86, 87, 88]),\n",
       " ('Comp178823$', [58, 64, 65, 67, 71, 72, 73, 80, 81]),\n",
       " ('Comp209770$', [57, 63, 64]),\n",
       " ('User205034',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User590319', [65, 67, 71, 72, 74, 78, 87]),\n",
       " ('User942064', [84, 85]),\n",
       " ('Comp433344$',\n",
       "  [63, 64, 65, 67, 70, 71, 72, 73, 74, 75, 77, 78, 80, 81, 83, 84]),\n",
       " ('User340968', [84, 85, 86, 87, 88, 89]),\n",
       " ('Comp643092$', [57, 59, 60, 61, 63, 65, 67, 71, 72, 78, 83, 87, 88]),\n",
       " ('User826475', [65, 71, 72, 74]),\n",
       " ('User475186', [84, 85]),\n",
       " ('User259016', [78, 79]),\n",
       " ('User144337', [84, 85]),\n",
       " ('Comp098086$', [60, 67, 83, 88]),\n",
       " ('User492606',\n",
       "  [58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84]),\n",
       " ('User385670', [78]),\n",
       " ('User035571', [58, 59, 60, 61, 74, 79, 80, 81, 87, 88]),\n",
       " ('User942450', [61, 63, 67, 78]),\n",
       " ('User021317', [57, 65, 67, 71, 72, 78, 87]),\n",
       " ('User628445', [73, 74]),\n",
       " ('User938831', [57, 60, 61, 74, 76, 83, 87]),\n",
       " ('User702245', [67, 74, 81, 88]),\n",
       " ('User220182', [57, 59, 77, 78, 82]),\n",
       " ('User080536', [57, 65, 67, 71, 78, 87]),\n",
       " ('Comp576890$', [60, 76, 77, 80]),\n",
       " ('Comp316839$', [65, 67]),\n",
       " ('User112390', [65, 87]),\n",
       " ('Comp717529$', [85, 86, 87, 88, 89]),\n",
       " ('Comp592480$', [70, 71, 72, 73, 75, 76, 77, 78, 79, 85, 86, 87, 88, 89]),\n",
       " ('User001173', [63, 87]),\n",
       " ('Comp972417$', [88]),\n",
       " ('User380806', [57, 58, 60, 74, 88]),\n",
       " ('Comp285967$', [64, 66, 67, 78, 80, 81, 87, 88]),\n",
       " ('User460312', [74, 75]),\n",
       " ('User750861', [57, 59, 65, 71, 78, 87]),\n",
       " ('User196135', [64, 65, 86, 87, 88]),\n",
       " ('Comp556624$', [58, 59, 87, 88]),\n",
       " ('Comp980094$', [57, 58, 59, 60, 63, 64, 65, 66, 70, 71, 72, 73, 77, 78]),\n",
       " ('Comp436980$', [57, 58, 64, 87, 88]),\n",
       " ('User355548', [57, 60, 64, 66, 74, 78, 79, 80, 87, 88]),\n",
       " ('User929303', [65, 67, 71, 72, 74, 87]),\n",
       " ('User407245', [71, 72]),\n",
       " ('User363382', [57, 59, 60, 71, 72, 74, 78, 87, 88]),\n",
       " ('Comp596992$', [86, 87, 88, 89]),\n",
       " ('Comp849583$', [85, 86]),\n",
       " ('Comp626608$', [64, 66, 67, 84]),\n",
       " ('User537540', [57, 59, 60, 64, 65, 72, 88]),\n",
       " ('User247613', [59, 60, 64, 65, 67, 68]),\n",
       " ('User232625', [67, 78]),\n",
       " ('Comp135323$', [57, 58, 64, 66, 80, 87]),\n",
       " ('User447020',\n",
       "  [57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 79, 80, 84, 85, 86, 87]),\n",
       " ('User404334', [57, 60, 61, 62, 63, 67, 68, 89]),\n",
       " ('User263357', [88]),\n",
       " ('User365039', [57, 60, 87, 88]),\n",
       " ('User046042', [57, 60, 65, 71, 77, 87, 88]),\n",
       " ('Comp710478$', [63, 64, 77, 78]),\n",
       " ('Comp660402$', [67, 71, 87]),\n",
       " ('User581656', [58, 59, 60, 63, 64, 70, 71, 72, 75, 76, 78, 79]),\n",
       " ('Comp015423$', [67, 87]),\n",
       " ('User546150', [60, 64, 67, 74, 88]),\n",
       " ('Comp248434$', [63, 71]),\n",
       " ('User704031', [79, 80]),\n",
       " ('User366923', [88]),\n",
       " ('User052455',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User954710', [60]),\n",
       " ('Comp007792$', [57, 60, 61, 62, 63, 65, 67, 68, 71, 74, 83, 84]),\n",
       " ('Comp544724$', [57, 58, 64, 66, 81, 87, 88]),\n",
       " ('User140362', [67, 71, 72, 74, 76, 78, 80, 81, 83, 86, 87, 88]),\n",
       " ('Comp323086$',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84]),\n",
       " ('User987198', [78, 79, 80, 84, 85]),\n",
       " ('User941559', [87]),\n",
       " ('User121588',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User942336', [57, 58, 59, 64, 65, 66, 67, 68]),\n",
       " ('Comp106840$', [79, 80, 81, 87, 88]),\n",
       " ('User356595', [58, 60, 61, 65]),\n",
       " ('User127445', [68, 69]),\n",
       " ('User834036', [57, 79, 81, 83, 86]),\n",
       " ('Comp675163$', [57, 59, 60, 64, 65, 66, 67, 71, 74, 87, 88]),\n",
       " ('Comp403942$', [64, 67, 80, 88]),\n",
       " ('User601685', [77, 78, 79, 80, 81, 83, 84]),\n",
       " ('Comp383370$', [60, 67]),\n",
       " ('User613599',\n",
       "  [71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86]),\n",
       " ('User541473', [78, 79, 80, 81, 82, 84, 85, 87, 88]),\n",
       " ('User706206',\n",
       "  [59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User463971', [57, 58, 59, 64, 65, 71, 72, 74, 78, 87]),\n",
       " ('User446850', [64]),\n",
       " ('Comp206966$', [57, 65, 67, 78, 87]),\n",
       " ('Comp223937$', [64, 65, 67, 76, 80, 83]),\n",
       " ('User109731', [61, 62, 63, 83]),\n",
       " ('Comp580919$', [60, 63, 68, 83]),\n",
       " ('User647847', [57]),\n",
       " ('User323136', [83]),\n",
       " ('Comp315439$',\n",
       "  [73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('User163038', [87]),\n",
       " ('User302303',\n",
       "  [59,\n",
       "   60,\n",
       "   61,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp913066$', [57, 58, 66, 79, 86, 87]),\n",
       " ('User960153', [57, 58, 60, 61, 66, 67, 68]),\n",
       " ('Comp003403$', [58, 64, 65, 67]),\n",
       " ('User352425', [58]),\n",
       " ('User735730', [57, 60, 87]),\n",
       " ('User110747',\n",
       "  [58, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]),\n",
       " ('User026525', [75, 76, 77, 78, 79, 80, 81, 83, 84, 86, 87, 88, 89]),\n",
       " ('User587044', [80, 81, 85, 86]),\n",
       " ('User541704', [57, 58, 59]),\n",
       " ('User657813', [67]),\n",
       " ('User003210', [65, 67, 71, 72, 78, 87]),\n",
       " ('User031630', [60, 64]),\n",
       " ('User934129', [57, 60]),\n",
       " ('User917652', [57, 64, 65, 66, 67]),\n",
       " ('Comp107204$', [87]),\n",
       " ('User864509', [63, 83]),\n",
       " ('User633443', [60, 68, 76, 83]),\n",
       " ('User129100', [62, 63, 65, 71, 72, 74, 80, 81, 87, 88]),\n",
       " ('User767480', [83]),\n",
       " ('Comp768882$', [67, 71, 78, 87]),\n",
       " ('Comp139547$', [87, 88, 89]),\n",
       " ('User776021', [83]),\n",
       " ('User197323', [57, 58, 70, 71, 72, 73, 77, 78]),\n",
       " ('User201772', [57, 59, 60, 61, 62, 63, 64, 65, 67, 68, 71, 72, 74, 80]),\n",
       " ('User179462', [58, 60, 63, 64, 67, 74, 80, 87, 88]),\n",
       " ('Comp320132$', [57, 58, 64, 65, 66, 67, 68, 69, 71, 72, 78, 79, 87, 88]),\n",
       " ('Comp020889$',\n",
       "  [74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('User114850', [74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88]),\n",
       " ('Comp569713$', [67, 74, 78, 87, 88]),\n",
       " ('Comp073828$', [86]),\n",
       " ('User627688', [57, 59, 60, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('Comp224949$', [57, 58, 59, 60, 61, 77, 78, 79, 80, 85, 86, 87, 88, 89]),\n",
       " ('User934247',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88]),\n",
       " ('Comp247334$', [79, 80, 81, 87, 88]),\n",
       " ('Comp168348$', [78, 83]),\n",
       " ('Comp516901$', [65, 66, 67, 71, 72, 73, 85, 86, 87, 88]),\n",
       " ('User737290', [57, 59, 71, 82, 83, 84]),\n",
       " ('Comp182521$', [64, 65]),\n",
       " ('User863956', [79, 80, 84, 85]),\n",
       " ('User275852',\n",
       "  [63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User003259', [70, 71, 84, 85, 86, 87]),\n",
       " ('User805380', [63, 83, 87, 88]),\n",
       " ('User337263', [79, 80]),\n",
       " ('Comp864044$', [57, 59, 65, 71, 78]),\n",
       " ('User950451', [60, 61, 63, 83]),\n",
       " ('User468662',\n",
       "  [57, 58, 59, 60, 64, 65, 67, 68, 72, 73, 74, 78, 79, 84, 85, 86]),\n",
       " ('User536716', [64, 67, 87]),\n",
       " ('Comp392050$', [58, 59, 60, 61]),\n",
       " ('Comp017671$', [77]),\n",
       " ('User807441', [57, 59, 65, 71, 72, 78, 87, 88]),\n",
       " ('User494212', [64, 65, 67, 80, 83]),\n",
       " ('Comp425174$', [58, 64, 65, 80]),\n",
       " ('Comp078004$', [64, 65, 66, 67, 79, 81, 82, 84, 86, 87, 88, 89]),\n",
       " ('User914799', [67, 68, 70, 71, 83]),\n",
       " ('Comp655835$', [81, 87]),\n",
       " ('User571310', [65, 66, 67, 71, 72, 74, 78, 84, 85, 87]),\n",
       " ('Comp224172$', [79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp122574$', [88]),\n",
       " ('User439036', [67, 68, 72, 87, 88]),\n",
       " ('User516651', [77, 78]),\n",
       " ('Comp803953$', [78, 79]),\n",
       " ('User932372', [57, 58, 64, 66, 67]),\n",
       " ('User029104', [58, 60, 61, 62, 63, 66, 67, 68, 69, 72, 73, 78, 80, 81]),\n",
       " ('User070895', [74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]),\n",
       " ('Comp787298$', [87]),\n",
       " ('Comp900362$',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User792011', [57, 59, 71, 72, 74, 78, 87, 88]),\n",
       " ('Comp767858$', [63, 78, 87]),\n",
       " ('User234720',\n",
       "  [70, 71, 72, 73, 74, 75, 76, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp465550$', [87]),\n",
       " ('User216117', [63, 64]),\n",
       " ('User012099', [88, 89]),\n",
       " ('User942461', [64, 67, 87]),\n",
       " ('User750600', [58, 59, 63, 64, 65, 70, 71]),\n",
       " ('User589671',\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp718727$', [64]),\n",
       " ('Comp351038$', [57, 60, 65, 87]),\n",
       " ('User576476', [73, 74, 75, 81, 82]),\n",
       " ('Comp211906$', [78, 88]),\n",
       " ('Comp966232$', [57, 58, 60, 61, 63, 64, 65, 70, 71]),\n",
       " ('Comp703579$', [57, 63, 64, 65, 66]),\n",
       " ('User014512', [64, 66, 78, 79, 80, 86]),\n",
       " ('Comp495328$', [57, 58, 63, 64, 66, 67, 80, 81, 87, 88]),\n",
       " ('User110856', [74, 87]),\n",
       " ('User277651', [57, 67, 78, 87, 88]),\n",
       " ('User429101', [83]),\n",
       " ('User421762',\n",
       "  [66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User121700', [57, 59, 78]),\n",
       " ('User829692', [58, 59, 65, 66, 72, 73]),\n",
       " ('User051327', [60, 67, 71, 78, 87]),\n",
       " ('User289276', [64, 65]),\n",
       " ('User304067', [78, 79, 80, 82, 83, 84]),\n",
       " ('User831012', [57, 59, 60, 65, 67, 71, 72, 78, 87, 88]),\n",
       " ('User776349', [77, 78]),\n",
       " ('Comp513049$', [67, 68]),\n",
       " ('Comp468321$', [58, 59, 85, 86, 87, 88]),\n",
       " ('User333262', [87, 88]),\n",
       " ('User066060', [67]),\n",
       " ('User804383', [60, 61, 71, 72, 74, 78, 87, 88]),\n",
       " ('Comp091289$', [60, 87]),\n",
       " ('Comp454270$', [57, 59, 60, 61, 62, 63, 65, 67, 68, 71, 72, 78, 83, 87]),\n",
       " ('User292789', [57, 58, 59]),\n",
       " ('User596933', [64]),\n",
       " ('User387926', [57, 58, 64, 78, 79, 86]),\n",
       " ('User912956', [77, 78, 79, 80, 81]),\n",
       " ('User183118', [84, 85]),\n",
       " ('User117113',\n",
       "  [57, 58, 60, 62, 63, 64, 67, 68, 71, 74, 76, 78, 80, 81, 87, 88]),\n",
       " ('User501042', [59, 60, 67, 71, 78, 87, 88]),\n",
       " ('User381685', [78]),\n",
       " ('Comp024761$', [57, 58, 64, 65, 67, 87, 88]),\n",
       " ('User191602', [78]),\n",
       " ('User339926', [71, 72, 77, 78, 89]),\n",
       " ('User006226', [60, 65, 67, 68, 87]),\n",
       " ('Comp504415$', [64, 65, 67, 72, 74, 76, 78]),\n",
       " ('User001338',\n",
       "  [58, 60, 61, 64, 65, 67, 68, 72, 73, 74, 75, 77, 78, 79, 80, 81]),\n",
       " ('Comp018827$', [63, 65, 66, 67, 71, 78, 88]),\n",
       " ('User919535', [57, 72, 73, 85, 86, 87, 88, 89]),\n",
       " ('User225648',\n",
       "  [57, 59, 60, 61, 62, 63, 65, 67, 68, 71, 72, 74, 78, 80, 81, 83, 87, 88]),\n",
       " ('User792262', [87, 88, 89]),\n",
       " ('User225859', [60, 61, 63, 65, 67, 68, 71, 78, 83, 87]),\n",
       " ('User032663', [57, 58, 64, 66, 79, 88]),\n",
       " ('Comp268685$', [58, 59, 77, 78]),\n",
       " ('User868964',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User846127', [60, 62, 63, 89]),\n",
       " ('Comp828480$', [57, 59, 60, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('User024066', [57, 58, 67, 68, 73, 74, 79, 80]),\n",
       " ('Comp814456$', [76, 77]),\n",
       " ('Comp626866$', [58, 60, 63, 64, 65, 67, 68, 81, 82, 83]),\n",
       " ('User071989', [60, 63, 67, 78, 83]),\n",
       " ('Comp574739$', [57, 59, 64, 65, 67, 68, 71, 72, 78, 80, 87]),\n",
       " ('Comp072716$', [57]),\n",
       " ('Comp031604$',\n",
       "  [58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('User497107', [60]),\n",
       " ('User277352', [59, 60, 65, 71, 87, 88]),\n",
       " ('User669965', [57, 59, 67, 78, 87]),\n",
       " ('Comp064442$', [57]),\n",
       " ('User550265', [60, 87]),\n",
       " ('Comp992531$', [60, 67, 71, 78, 87, 88]),\n",
       " ('User810658', [57, 58, 74, 75]),\n",
       " ('User162017', [64, 65, 66, 67, 78, 79]),\n",
       " ('Comp823749$', [63, 67, 83]),\n",
       " ('User741602', [68]),\n",
       " ('User951667', [57, 58, 59, 60, 65, 66, 73, 74]),\n",
       " ('User250801', [58, 60, 63, 64, 65, 67, 68, 75, 76, 77, 80, 81, 83]),\n",
       " ('User769713', [80]),\n",
       " ('Comp756996$', [78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89]),\n",
       " ('Comp369838$',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89]),\n",
       " ('Comp198590$', [64, 65]),\n",
       " ('User409209', [58, 64, 79]),\n",
       " ('User872572', [63, 64, 78, 79, 87, 88]),\n",
       " ('User998275', [57, 59, 65, 71, 72, 78, 87]),\n",
       " ('User881339', [60, 67]),\n",
       " ('User170037', [57, 60, 65, 71, 72, 74, 78, 87, 88]),\n",
       " ('Comp281021$', [64, 65, 71, 78, 81]),\n",
       " ('User631093', [57, 58, 64, 67, 81, 86, 88]),\n",
       " ('User466213', [61]),\n",
       " ('Comp309102$', [87]),\n",
       " ('Comp938095$', [64]),\n",
       " ('User855043', [65, 67, 71, 72, 78, 87]),\n",
       " ('User464601', [65, 67, 72, 78, 87, 88]),\n",
       " ('Comp249277$', [58, 59, 65, 73, 74, 78, 79, 84, 85, 86, 87]),\n",
       " ('Comp262388$', [63, 64, 65, 70, 71]),\n",
       " ('Comp776029$', [58, 60, 64, 65, 71, 72, 73, 74, 80]),\n",
       " ('User664045', [59, 60]),\n",
       " ('User319297', [64, 67, 88]),\n",
       " ('Comp003192$', [59, 60, 65, 71, 72, 87]),\n",
       " ('User573151', [57, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75]),\n",
       " ('User820122', [83]),\n",
       " ('User565727', [58, 59, 77, 78, 79, 80, 81]),\n",
       " ('Comp497980$', [57, 60, 87, 88]),\n",
       " ('Comp506722$', [89]),\n",
       " ('User011846', [87]),\n",
       " ('User007609', [76, 80, 81, 87, 88]),\n",
       " ('User322926', [67, 83]),\n",
       " ('User058369', [63, 67, 83, 88, 89]),\n",
       " ('Comp023011$',\n",
       "  [57,\n",
       "   58,\n",
       "   60,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   87]),\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_anomalies_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31c541-a782-4b3a-9f9f-5e4396c88113",
   "metadata": {},
   "source": [
    "#### Implementation with UASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99ba86-a1d5-4099-bb11-74a9b3248bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UASE_anom_finder(window_length):\n",
    "\n",
    "    vertex_norm_tracker = []\n",
    "    graph_norm_tracker = []\n",
    "\n",
    "    graph_mam_tracker = []\n",
    "    graph_range_tracker = []\n",
    "\n",
    "    vertex_mam_tracker = []\n",
    "    vertex_range_tracker = []\n",
    "\n",
    "    weeks = int(len(data_frame_list_uase)/window_length)\n",
    "    Y_arrays = []\n",
    "\n",
    "    for i in range(weeks):\n",
    "        matrices = []\n",
    "        for j in range(window_length):\n",
    "            matrices.append(data_frame_list_uase[i*window_length+j])\n",
    "\n",
    "        wl_matrix = scipy.sparse.hstack(matrices)\n",
    "\n",
    "        u, s, v = scipy.sparse.linalg.svds(wl_matrix.asfptype(),k=7)\n",
    "\n",
    "        for j in range(window_length):\n",
    "            v_j = pd.DataFrame(v).iloc[: ,28815*j:28815*(j+1)]\n",
    "            Y = pd.DataFrame((v_j.transpose()*s**0.5).transpose())\n",
    "            Y_arrays.append(Y)\n",
    "\n",
    "    for t in tqdm(range(len(Y_arrays)-1)):\n",
    "        \n",
    "        Y2 = Y_arrays[t+1]\n",
    "        Y1 = Y_arrays[t]\n",
    "        nrm = scipy.linalg.norm(np.subtract(Y2,Y1),ord=2)\n",
    "        graph_norm_tracker.append(nrm)\n",
    "\n",
    "        vertex_norms = []\n",
    "        \n",
    "        for i,v in enumerate(list(columns_sparse)):\n",
    "            nrm = scipy.linalg.norm(np.subtract(Y2.iloc[:,i],Y1.iloc[:,i]),ord=2)\n",
    "            vertex_norms.append(nrm)\n",
    "\n",
    "        vertex_norm_tracker.append(vertex_norms)\n",
    "\n",
    "        if t >= window_length:\n",
    "            graph_mam = np.sum(graph_norm_tracker[(t-window_length+1):t-1])/(window_length-1)\n",
    "            graph_range = np.sum(np.linalg.norm(np.array(graph_norm_tracker[t-window_length+2:t-1])-np.array(graph_norm_tracker[t-window_length+1:t-2])))/(1.128*(window_length-2))\n",
    "            graph_mam_tracker.append(graph_mam)\n",
    "            graph_range_tracker.append(graph_range)\n",
    "\n",
    "            vertex_mam_tracker_day = []\n",
    "            vertex_range_tracker_day = []\n",
    "\n",
    "            for j in range(len(list(columns_sparse))):\n",
    "                vertex_mam = np.sum([vertex_norm_tracker[k][j] for k in range(t-window_length+1,t)])/(window_length-1)\n",
    "                vertex_mam_tracker_day.append(vertex_mam)\n",
    "                vertex_range = np.sqrt((vertex_mam*(1-vertex_mam))/(window_length-1))\n",
    "                vertex_range_tracker_day.append(vertex_range)\n",
    "            vertex_mam_tracker.append(vertex_mam_tracker_day)\n",
    "            vertex_range_tracker.append(vertex_range_tracker_day)\n",
    "\n",
    "    day_anomalies = []\n",
    "\n",
    "    for i,v in enumerate(list(columns_sparse)):\n",
    "        y_v = np.array([vertex_norm_tracker[k][i] for k in range(len(vertex_norm_tracker))])\n",
    "        ucl_v = np.array([vertex_mam_tracker[k][i] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][i] for k in range(len(vertex_range_tracker))])\n",
    "        anomalies = [j+window_length+1 for j,y in enumerate(y_v[window_length:]) if y > ucl_v[j]]\n",
    "        if len(anomalies) > 0:\n",
    "            day_anomalies.append((v,anomalies))\n",
    "        \n",
    "    return vertex_norm_tracker, graph_norm_tracker, graph_mam_tracker, graph_range_tracker, vertex_mam_tracker, vertex_range_tracker, day_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a793353-56b6-4668-9aa1-dc0dcd3b02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_norm_tracker_7, graph_norm_tracker_7, graph_mam_tracker_7, graph_range_tracker_7, vertex_mam_tracker_7, vertex_range_tracker_7, day_anomalies_7 = UASE_anom_finder(7)\n",
    "vertex_norm_tracker_14, graph_norm_tracker_14, graph_mam_tracker_14, graph_range_tracker_14, vertex_mam_tracker_14, vertex_range_tracker_14, day_anomalies_14 = UASE_anom_finder(14)\n",
    "vertex_norm_tracker_3, graph_norm_tracker_3, graph_mam_tracker_3, graph_range_tracker_3, vertex_mam_tracker_3, vertex_range_tracker_3, day_anomalies_3 = UASE_anom_finder(3)\n",
    "vertex_norm_tracker_2, graph_norm_tracker_2, graph_mam_tracker_2, graph_range_tracker_2, vertex_mam_tracker_2, vertex_range_tracker_2, day_anomalies_2 = UASE_anom_finder(2)\n",
    "vertex_norm_tracker_5, graph_norm_tracker_5, graph_mam_tracker_5, graph_range_tracker_5, vertex_mam_tracker_5, vertex_range_tracker_5, day_anomalies_5 = UASE_anom_finder(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28484fd-6659-4aca-ae5e-60bf4d62aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_creation_UASE(window_length,vertex_mam_tracker,vertex_range_tracker,vertex_norm_tracker,u,vertex=True):\n",
    "\n",
    "    plt.figure(figsize = (16,8))  \n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1-(len(data_frame_list_uase)%window_length)),[vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])\n",
    "    plt.plot(np.arange(window_length,len(data_frame_list_uase)-1-(len(data_frame_list_uase)%window_length)),np.array([vertex_mam_tracker[k][u] for k in range(len(vertex_mam_tracker))])+3*np.array([vertex_range_tracker[k][u] for k in range(len(vertex_range_tracker))]),'--')\n",
    "    plt.plot([vertex_norm_tracker[k][u] for k in range(len(vertex_norm_tracker))],'r.')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('$||{X_{i}^{(t+1)}-X_{i}^{(t)}}||$')\n",
    "    plt.title('Window Length: {}'.format(window_length))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8cbe9-5ec0-4a64-972e-b31707ebf960",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_creation_UASE(14,vertex_mam_tracker_14,vertex_range_tracker_14,vertex_norm_tracker_14,23988)\n",
    "plot_creation_UASE(7,vertex_mam_tracker_7,vertex_range_tracker_7,vertex_norm_tracker_7,23988)\n",
    "plot_creation_UASE(5,vertex_mam_tracker_5,vertex_range_tracker_5,vertex_norm_tracker_5,23988)\n",
    "plot_creation_UASE(3,vertex_mam_tracker_3,vertex_range_tracker_3,vertex_norm_tracker_3,23988)\n",
    "plot_creation_UASE(2,vertex_mam_tracker_2,vertex_range_tracker_2,vertex_norm_tracker_2,23988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe036e19-ac33-4b05-bdd4-68422f5726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_count_rt(day_anomalies_14,14)\n",
    "anomaly_count_rt(day_anomalies_7,7)\n",
    "anomaly_count_rt(day_anomalies_5,5)\n",
    "anomaly_count_rt(day_anomalies_3,3)\n",
    "anomaly_count_rt(day_anomalies_2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43a491-2503-4f09-beb0-6381dbd8738b",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b916f-b5ea-4c2a-9938-9f6354627061",
   "metadata": {},
   "source": [
    "We make the assumption that traffic from the red team usernames during the 57-82 day period is always anomalous whereas the traffic between any other time periods and non-red teeam uesrnames is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d1d2c-b9a4-4acc-905d-e99c30b4cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation_supervised(user,n,anom):\n",
    "    \n",
    "    feat_dict = {}\n",
    "    # for reference (not used)\n",
    "    day_dict = {0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
    "    \n",
    "    for i in range(len(auth_start_days)-1):\n",
    "        # gives access to a day for the NN - assume start on a monday based on EDA\n",
    "        day = i%7\n",
    "        chunks = split_dataframe(authentication_data[auth_start_days[i]:auth_start_days[i+1]],n)\n",
    "        for j in range(n):\n",
    "            \n",
    "            hour = j\n",
    "            data = chunks[j]\n",
    "            \n",
    "            if 57 <= i+1 <= 82 and anom=True:\n",
    "                anombool = 1\n",
    "            else:\n",
    "                anombool = 0\n",
    "\n",
    "            authents = len(data[data['UserName'] == user])\n",
    "            failures = len(data[(data['UserName'] == user) & (data['Failure'] == 1)])\n",
    "            srcunique = len(data[data['UserName'] == user]['SrcDevice'].unique())\n",
    "            dstunique = len(data[data['UserName'] == user]['DstDevice'].unique())\n",
    "            uniquepairs = len(data.groupby(['SrcDevice','DstDevice']).size())\n",
    "            feat_dict[i*n + j] = [user,srcunique,dstunique,authents,failures,anombool]\n",
    "            #feat_dict[i*n + j] = [day,hour,srcunique,dstunique,uniquepairs,authents,failures]\n",
    "                \n",
    "    df = pd.DataFrame(data=feat_dict,index =  ['UserName','SrcUnique','DstUnique','Authentications','Failures','Anomaly'])\n",
    "    #df = pd.DataFrame(data=feat_dict,index =  ['Day','Hour','SrcUnique','DstUnique','UniquePairs','Authentications','Failures'])\n",
    "    df = df.transpose()\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38449b-4006-4b7e-9afb-ef0c91441b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_sample = list(pd.Series(rt_usernames).unique()) + random.sample(non_rt_users,400)\n",
    "data = []\n",
    "for user in tqdm(username_sample):\n",
    "    if user in rt_usernames:\n",
    "        anom = 1\n",
    "    else:\n",
    "        anom = 0\n",
    "    df = feature_generation_supervised(user,24,anom)\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00281f0-0d06-46cc-8836-a4578d77bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_sample = list(pd.Series(rt_usernames).unique()) + non_rt_sample\n",
    "data = []\n",
    "for user in tqdm(username_sample):\n",
    "    if user in rt_usernames:\n",
    "        anom = 1\n",
    "    else:\n",
    "        anom = 0\n",
    "    df = feature_generation_supervised(user,24,anom)\n",
    "    data.append(df)\n",
    "nn_data = pd.concat(data)\n",
    "nn_data.to_csv('nn_supervised data.gz', compression='gzip')\n",
    "X_train, X_test, y_train, y_test = train_test_split(nn_data[['UserName','SrcUnique','DstUnique','Authentications','Failures']], nn_data['Anomaly'], test_size=0.1, random_state=42)\n",
    "X_train_un_list = list(X_train['UserName'])\n",
    "X_test_un_list = list(X_test['UserName'])\n",
    "X_train = X_train.drop('UserName',axis=1)\n",
    "X_test = X_test.drop('UserName',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d67597-dfe5-4ef5-9151-01d7ce3ad4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_anomaly_finder(x_train,x_test,y_train,y_test,train_user,test_user,n,BATCH_SIZE=256, EPOCHS=1000):\n",
    "    \n",
    "    # create our scaling pipeline\n",
    "    pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                    ('scaler', MinMaxScaler())])\n",
    "    pipeline.fit(x_train)\n",
    "    x_train = pipeline.transform(x_train)\n",
    "    x_test = pipeline.transform(x_test)\n",
    "    \n",
    "    # define neural network parameters\n",
    "    input_dim = x_train.shape[1]\n",
    "    BATCH_SIZE = BATCH_SIZE\n",
    "    EPOCHS = EPOCHS\n",
    "\n",
    "    # define the nn\n",
    "    neural_network =Sequential([\n",
    "\n",
    "        tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(32, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(16, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(8, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(4, activation='elu'),\n",
    "        tf.keras.layers.Dropout(rate=0.1),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    ])\n",
    "\n",
    "    # compile the autoencoder\n",
    "    neural_network.compile(optimizer=\"adam\", \n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[\"acc\"])\n",
    "    \n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # allows for early stopping and better visualisation of progress\n",
    "    cb = [early_stop,TqdmCallback(verbose=1)]\n",
    "    \n",
    "    # fit the auto encoder\n",
    "    history = neural_network.fit(\n",
    "        x_train, y_train,\n",
    "        shuffle=True,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=cb   \n",
    "    )\n",
    "    \n",
    "    # make predictions on the remaining data\n",
    "    x_test_pred = neural_network.predict(x_test)\n",
    "    Y_pred = x_test_pred.argmax(axis=1)\n",
    "    test_loss, test_acc = neural_network.evaluate(X_test, y_test)\n",
    "    print(test_acc)\n",
    "    \n",
    "    anomalies = x_test[np.where(x_test_pred == 1)]\n",
    "    #anomaly_idx = anomalies[0]\n",
    "    \n",
    "    #frame = []\n",
    "    #for i in range(len(anomaly_idx)):\n",
    "    #    user = test_user[anomaly_idx[i]]\n",
    "    #    anomaly = nn_orig_finder(user,n,anomaly_idx[i])\n",
    "        \n",
    "        # indicates that an anomaly was found where we have no information i.e. the lack of an event was anomalous\n",
    "    #    if len(anomaly) == 0:\n",
    "    #        pass\n",
    "    #    else:\n",
    "    #        frame.append(anomaly)\n",
    "    #    \n",
    "    #if len(frame) != 0:\n",
    "    #    anomaly_df = pd.concat(frame)\n",
    "    #    return anomaly_df,1,history.history[\"val_loss\"]\n",
    "    #else:\n",
    "    #    print('No anomalies found.')\n",
    "    #    return pd.DataFrame(test_mae_loss).describe(),0,history.history[\"val_loss\"]\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4790614-d2ec-4c67-8ea2-6b6730873534",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nn_anomaly_finder(X_train,X_test,y_train,y_test,X_train_un_list,X_test_un_list,24,BATCH_SIZE=256, EPOCHS=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5732c-495b-4210-8490-2a326e871bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_pred).groupby(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe702c-c162-4f82-ad7e-a9566c4d22c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7348c6e1-c07e-4bca-ba1e-67904e9ae7ca",
   "metadata": {},
   "source": [
    "#### Bipartite Graph AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3dbcef5f-efa2-45c3-9c88-8ac49a1b8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = data_frame_list_uase[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cef9d20-80f1-421c-aba7-30cfcbe13c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a2d63aa74cfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mq_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mq_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "rs_scores = []\n",
    "for node in range(10):\n",
    "    c = 0.15\n",
    "    q_node = np.zeros(M.shape[0])\n",
    "    q_node[node] = 1\n",
    "    eps = 0.01\n",
    "    u_a = np.array([1/(M.shape[0]+M.shape[1])]*(M.shape[0]+M.shape[1]))\n",
    "    u_a_prev = np.zeros(u_a.shape)\n",
    "    \n",
    "    normalize = Normalizer()\n",
    "    M_normed = normalize.fit_transform(M)\n",
    "    \n",
    "\n",
    "    while np.linalg.norm(u_a_prev[:28815]-u_a[:28815]) > eps:\n",
    "        u_a_prev = u_a\n",
    "        u_a = np.dot(((1-c) * col_norm),u_a[:14000].transpose()) + c * q_node\n",
    "    rs_node = u_a[:28815]\n",
    "    rs_scores.append(rs_node)\n",
    "pd.DataFrame(rs_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c7625-45df-4105-8c41-793bdf42510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
